{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T18:06:03.987080Z",
     "start_time": "2020-02-23T18:06:03.889691Z"
    }
   },
   "source": [
    "<h1><center>HW1 Movie Review Sentiment Analysis</center></h1>\n",
    "<h2><center>Due: March 2020 6th, 23:59</center></h2>\n",
    "\n",
    "In this homework you will create different models to generate the positive/negative sentiment\n",
    "classification of movie reviews.\n",
    "\n",
    "This homework should be done individually without cooperation with others.\n",
    "\n",
    "You are given the following files:\n",
    "- `hw01.ipynb`: Notebook file with starter code\n",
    "- `train.txt`: Training set to train your model\n",
    "- `test.txt`: Test set to report your model’s performance\n",
    "- `sample_prediction.csv`: Sample file your prediction result should look like\n",
    "- `utils/`: folder containing all utility code for the series of homeworks\n",
    "\n",
    "Remember to leverage code in `utils/`, so you don't have to build everything from\n",
    "Scratch.\n",
    "- `load_data(filename)`: load the input data and return sklearn.Bunch object. For basic\n",
    "usage of Bunch object, please refer to sklearn documentation.\n",
    "- `save_prediction(arr, filename)`: save your prediction into the format required by the\n",
    "course\n",
    "\n",
    "<h3> 0. Install Anaconda </h3>\n",
    "\n",
    "If you do not yet have Python and Jupyter Notebook on your laptop, use this link\n",
    "(https://www.anaconda.com/) to install anaconda. Anaconda is a suite that provides one\n",
    "stop solution for all you need for Python development environment. This site contains\n",
    "installation document for Windows, Mac, and Linux, choose the one that suits your operating\n",
    "system.\n",
    "\n",
    "*Tips: You may want to consider installing Jupyter Extensions (link: https://github.com/ipython-contrib/jupyter_contrib_nbextensions), and turn on extensions such as `ExcuteTime` and \n",
    "`Table of Contents(2)`. You may find them very helpful to assist you finishing homework. However, this \n",
    "is definitely not a necessary requirement.*\n",
    "\n",
    "<h3> 1. Feature Dictionary Vectorization </h3> \n",
    "\n",
    "A quite unique step for NLP is to engineer the raw text input into numerical features. You will\n",
    "eventually implement several featurizers, just like `dummy_featurize`, that distinguish your\n",
    "models with others. However, we are not there yet. For convenience we allow you to\n",
    "represent the features using a dictionary, look at the `dummy_featurize` code. So each data\n",
    "point can be translated into a dictionary. However later we have to translate this list of\n",
    "dictionaries into a homogeneous data structure. Therefore, you need to first implement the\n",
    "pipeline method in the `SentimetnClassifier` class. Look into the code comment for more\n",
    "details. To ensure your implementation works, we also provided some test code in the cell\n",
    "below.\n",
    "\n",
    "<h3> 2. Better featurizers </h3>\n",
    "\n",
    "\n",
    "Have you finished the first step, you can run the model using the provided featurizer. See\n",
    "the performance is nearly as good as a donkey? No surprise! The `dummy_featurize` should\n",
    "have been named `really_dummy_featurize`. Now it is your turn to implement better\n",
    "featurizers. For this homework, you need to implement at least 3 distinguishable featurizer.\n",
    "Describe your features briefly in the write-up and include the accuracy of the model. No idea\n",
    "at all? Look at your lecture notes for inspiration. Still no clue? Why not start with Bag of\n",
    "words?\n",
    "\n",
    "*Note: Model performance is important but it’s not the only thing we care about, your work will\n",
    "also be rewarded by your creativity.*\n",
    "\n",
    "<h3> 3. Optional: Try different learning methods </h3> \n",
    "\n",
    "All the work you have done so far are related to feature engineering, and the featurized data\n",
    "is trained using Logistic Regression. Try to use different learning methods to train the model\n",
    "and see if you achieve any difference in the performance. Discuss your findings in the\n",
    "write-up.\n",
    "\n",
    "<h3> 4. Deliverables (zip them all) </h3>\n",
    "\n",
    "- pdf version of your final notebook\n",
    "- Use the best model you trained, generate the prediction for test.txt, name the\n",
    "output file prediction.csv (Be careful: the best model in your training set might not\n",
    "be the best model for the test set).\n",
    "- HW1_writeup.pdf: summarize the method you used and report their performance.\n",
    "If you worked on the optional task, add the discussion. Add a short essay\n",
    "discussing the biggest challenges you encounter during this assignment and\n",
    "what you have learnt.\n",
    "\n",
    "(**You are encouraged to add the writeup doc into your notebook\n",
    "using markdown/html langauge, just like how this notes is prepared**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center> HW1 Write up </center></h2>\n",
    "<h3>Data Pre-processing:</h3>\n",
    "\n",
    "  It is good to check/see that both the training and dev dataset has balanced categories.\n",
    "  \n",
    "  I used nltk tokenizer to tokenize the texts into list of tokens, and use WordNetLemmatizer to normalize the text.\n",
    "  \n",
    "  I implemented the vectorizer algorithm from scratch, however, it turns out to be slower\n",
    "  \n",
    "  Using the sklearn learn Pipeline and build-in method will speed up.\n",
    "\n",
    "<h3>Feature Engineering:</h3>\n",
    "   \n",
    "   I tried to use the following features:\n",
    "   \n",
    "   - bag of words(unigram) : around 7200 (with at least 10 occurance)\n",
    "   \n",
    "   - bigram :around 12300 (with at least 10 occurance)\n",
    "   \n",
    "   - some handcrafted features like number of unique words,number of punctions, etc., I also tried to use the nltk tagging to get the number of nouns, adjectives, and verbs : about 8\n",
    "   \n",
    "   - tf-idf (on the unigram or on the bigram), I implement the equation by hand:\n",
    "   \n",
    "   The training set has sparsity(number of non-zero fraction) = 0.9%, so we need to sparse format to store it for efficiency.\n",
    "   \n",
    "   It turns out that the handcrafted features and only bigram without tf-idf does not help to improve the accuracy of the model.\n",
    "      \n",
    "<h3>Learning method:</h3> \n",
    "    \n",
    "   I used the following methods:\n",
    "   \n",
    "   - Logistic regression with L2 regulazation (default)\n",
    "   \n",
    "   - SVM\n",
    "   \n",
    "   - RandomForestClassfier\n",
    "   \n",
    "   - XGBoost\n",
    "   \n",
    "   The Logistric regression is the \"baseline\" and actually it reaches good performance on training/dev set.\n",
    "   \n",
    "   RandomForestClassfier is easy to overfit, with large accuracy on training set but poor performance on dev set, and XGBoost encounters the same problem, even with a reasonable number of trees and max depth of the trees.\n",
    "   \n",
    "   SVM gives a good estimation on both training and dev set. However, they did not surpass the score of baseline model for the dev set.\n",
    "      \n",
    "   The caveat for all the models is that they all need finer tuning.\n",
    "   \n",
    "   To reach better accuracy, we might need more training data, or use other feature engineer techiques.\n",
    "\n",
    "<h3>Result</h3>\n",
    "\n",
    "My final feature selection with unigram and tf-idf applied improves the dev accuracy, and the prediction power mainly comes from unigram.\n",
    "\n",
    "Please see the second-to-last Cell to see the comparasions.\n",
    "\n",
    "It agrees with the paper(2015)\n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/S1877050915001520\n",
    "\n",
    "which says \"We found that unigram is the best method to extract sentiment from the review.\" And they got best accuracy around 82% - 84%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============== Coding Starts Here ==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T20:52:00.680659Z",
     "start_time": "2020-02-23T20:51:58.055407Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# add utils folder to path\n",
    "p = os.path.dirname(os.getcwd())\n",
    "if p not in sys.path:\n",
    "    sys.path = [p] + sys.path\n",
    "    \n",
    "from utils.hw1 import load_data, save_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T20:55:11.104653Z",
     "start_time": "2020-02-23T20:55:11.051932Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!! Do not modify !!\n",
    "\"\"\"\n",
    "def dumb_featurize(text):\n",
    "    feats = {}\n",
    "    words = text.split(\" \")\n",
    "\n",
    "    for word in words:\n",
    "        if word == \"love\" or word == \"like\" or word == \"best\":\n",
    "            feats[\"contains_positive_word\"] = 1\n",
    "        if word == \"hate\" or word == \"dislike\" or word == \"worst\" or word == \"awful\":\n",
    "            feats[\"contains_negative_word\"] = 1\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "import string\n",
    "\n",
    "tagger = PerceptronTagger()\n",
    "\n",
    "def better_featurize(text):\n",
    "    feats = {}\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    def n_gram(n):        \n",
    "        #n - gram\n",
    "        ngrams = []\n",
    "        for i in range(len(words)-n+1):\n",
    "            g = '_'.join(words[i:i+n])\n",
    "            ngrams.append(g)\n",
    "        return ngrams\n",
    "    \n",
    "    feats = Counter(words) + Counter(n_gram(2)) # merge uni-gram and bigram\n",
    "    #feats = Counter(words)\n",
    "    #feats = {k:v for k,v in feats.items() if k not in stop_words and k not in string.punctuation}\n",
    "    #feats = {k:v/len(words) for k,v in feats.items()}\n",
    "    #feats['num_unique_words'] = len(set(words))\n",
    "    #feats['num_chars'] = len(text)\n",
    "    #feats['num_stopwords'] =  len([word for word in words if word in stopwords.words('english')])\n",
    "    #feats['num_punctions'] = len([word for word in words if word in string.punctuation])\n",
    "    #feats['mean_word_len'] = np.mean([len(word) for word in words])\n",
    "    #pos_list = pos_tag(words)\n",
    "    #pos_list = tagger.tag(words)\n",
    "    #feats['num_nouns'] = len([w for w in pos_list if w[1] in ('NN','NNP','NNPS','NNS')])\n",
    "    #feats['num_adjs'] = len([w for w in pos_list if w[1] in ('JJ','JJR','JJS')])\n",
    "    #feats['num_verbs'] = len([w for w in pos_list if w[1] in ('VB','VBD','VBG','VBN','VBP','VBZ')])\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T20:55:14.685120Z",
     "start_time": "2020-02-23T20:55:14.618559Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy.sparse import dok_matrix\n",
    "from numpy import count_nonzero\n",
    "from math import log\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import normalize\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "class SentimentClassifier:\n",
    "    def __init__(self, feature_method=dumb_featurize, min_feature_ct=1, L2_reg=1.0):\n",
    "        \"\"\"\n",
    "        :param feature_method: featurize function\n",
    "        :param min_feature_count: int, ignore the features that appear less than this number to avoid overfitting\n",
    "        \"\"\"\n",
    "        self.feature_vocab = {}\n",
    "        self.feature_method = feature_method\n",
    "        self.min_feature_ct = min_feature_ct\n",
    "        self.L2_reg = L2_reg\n",
    "\n",
    "    def featurize(self, X):\n",
    "        \"\"\"\n",
    "        # Featurize input text\n",
    "\n",
    "        :param X: list of texts\n",
    "        :return: list of featurized vectors\n",
    "        \"\"\"\n",
    "        featurized_data = []\n",
    "        for text in X:\n",
    "            feats = self.feature_method(text)\n",
    "            featurized_data.append(feats)\n",
    "        return featurized_data\n",
    "\n",
    "    def pipeline(self, X, training=False):\n",
    "        \"\"\"\n",
    "        Data processing pipeline to translate raw data input into sparse vectors\n",
    "        :param X: featurized input\n",
    "        :return X2: 2d sparse vectors\n",
    "        \n",
    "        Implement the pipeline method that translate the dictionary like feature vectors into \n",
    "        homogeneous numerical vectors, for example:\n",
    "        [{\"fea1\": 1, \"fea2\": 2}, \n",
    "         {\"fea2\": 2, \"fea3\": 3}] \n",
    "         --> \n",
    "         [[1, 2, 0], \n",
    "          [0, 2, 3]]\n",
    "          \n",
    "        Hints:\n",
    "        1. How can you know the length of the feature vector?\n",
    "        2. When should you use sparse matrix?\n",
    "        3. Have you treated non-seen features properly?\n",
    "        4. Should you treat training and testing data differently?\n",
    "        \"\"\"\n",
    "        # Have to build feature_vocab during training\n",
    "        if training:\n",
    "            print(\"Now training...\")\n",
    "            # the commented out part is to implement the algorithm from scratch, however, it turns out to be slower\n",
    "            # Using the scikit learn Pipeline and build-in method will speed up\n",
    "            ''' \n",
    "            self.idf_ = {} # count the document which have the features\n",
    "            for feature_dic in X:\n",
    "                for feature, value in feature_dic.items():\n",
    "                    self.feature_vocab[feature] = self.feature_vocab.get(feature, 0) + value\n",
    "                    self.idf_[feature] = self.idf_.get(feature, 0) + 1\n",
    "            index = 0\n",
    "            \n",
    "            # sort the features by the occurance\n",
    "            for k, v in sorted(self.feature_vocab.items(), key=lambda item: item[1], reverse=True):\n",
    "                if v < self.min_feature_ct:\n",
    "                    del self.feature_vocab[k]\n",
    "                    del self.idf_[k]\n",
    "                else:\n",
    "                    index += 1\n",
    "                    idf = self.idf_[k]\n",
    "                    self.idf_[k] = log((1+len(X))/(1+idf))+1\n",
    "                    self.feature_vocab[k] = index\n",
    "            self.feature_vocab['__UNKNOWN__'] = 0\n",
    "        X2 = []\n",
    "        # Translate raw texts into vectors\n",
    "        for feature_dic in X:\n",
    "            feature_vector = []\n",
    "            for feature in self.feature_vocab.keys():\n",
    "                if '_' in feature: \n",
    "                    feature_vector.append(feature_dic.get(feature, 0))\n",
    "                else: # unigram apply the tf-idf formula\n",
    "                    feature_vector.append(bool(feature_dic.get(feature, 0))*self.idf_.get(feature, 0))\n",
    "\n",
    "            X2.append(feature_vector)\n",
    "            '''    \n",
    "            self.vec = DictVectorizer().fit(X)\n",
    "            X2 = self.vec.transform(X).toarray()\n",
    "            self.feature_sel = np.count_nonzero(X2,axis=0)>=self.min_feature_ct\n",
    "            self.pipe = TfidfTransformer().fit(X2[:,self.feature_sel])\n",
    "        #X2 = np.array(X2)\n",
    "        X2 = self.vec.transform(X).toarray()\n",
    "        X2 = X2[:,self.feature_sel]\n",
    "        X2 = self.pipe.transform(X2).toarray()\n",
    "        sparsity = count_nonzero(X2) / float(X2.size)\n",
    "        # for sparse matrix, store them use the sparse format\n",
    "        if (sparsity < 0.02):\n",
    "            X2 = dok_matrix(X2)\n",
    "        #X2 = normalize(X2, axis=1, norm='l1')\n",
    "        return X2\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = self.pipeline(self.featurize(X), training=True)\n",
    "        \n",
    "        D, F = X.shape\n",
    "        print(D,F)\n",
    "        self.model = LogisticRegression(C=self.L2_reg)\n",
    "        #self.model = LinearSVC(C=self.L2_reg)\n",
    "        #self.model = RandomForestClassifier(n_estimators=2000,\n",
    "        #                                    max_depth=5)\n",
    "        #self.model = xgb.XGBClassifier(n_estimators = 2500,\n",
    "        #                               random_state = 2009,\n",
    "        #                               max_depth = 5,\n",
    "        #                               learning_rate= 0.05,\n",
    "        #                               colsample_bytree = 0.2,\n",
    "        #                               subsample = 0.8,\n",
    "        #                               min_child_weight = 2.,\n",
    "        #                               n_jobs=4)\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.pipeline(self.featurize(X))\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        X = self.pipeline(self.featurize(X))\n",
    "        return self.model.score(X, y)\n",
    "\n",
    "    # Write learned parameters to file\n",
    "    def save_weights(self, filename='weights.csv'):\n",
    "        weights = [[\"__intercept__\", self.model.intercept_[0]]]\n",
    "        for feat, idx in self.feature_vocab.items():\n",
    "            weights.append([feat, self.model.coef_[0][idx]])\n",
    "        \n",
    "        weights = pd.DataFrame(weights)\n",
    "        weights.to_csv(filename, header=False, index=False)\n",
    "        \n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T19:58:51.448435Z",
     "start_time": "2020-02-23T19:58:51.341871Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training...\n",
      "Success!!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run this to test your model implementation\n",
    "\"\"\"\n",
    "\n",
    "cls = SentimentClassifier()\n",
    "X_train = [{\"fea1\": 1, \"fea2\": 2}, {\"fea2\": 2, \"fea3\": 3}]\n",
    "\n",
    "X = cls.pipeline(X_train, True)\n",
    "assert X.shape[0] == 2 and X.shape[1] >= 3, \"Fail to vectorize training features\"\n",
    "\n",
    "X_test = [{\"fea1\": 1, \"fea2\": 2}, {\"fea2\": 2, \"fea3\": 3}]\n",
    "X = cls.pipeline(X_test)\n",
    "assert X.shape[0] == 2 and X.shape[1] >= 3, \"Fail to vectorize testing features\"\n",
    "\n",
    "X_test = [{\"fea1\": 1, \"fea2\": 2}, {\"fea2\": 2, \"fea4\": 3}]\n",
    "try:\n",
    "    X = cls.pipeline(X_test)\n",
    "    assert X.shape[0] == 2 and X.shape[1] >= 3\n",
    "except:\n",
    "    raise Exception(\"Fail to treat un-seen features\")\n",
    "\n",
    "print(\"Success!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T20:55:17.432616Z",
     "start_time": "2020-02-23T20:55:17.086834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training...\n",
      "7000 2\n",
      "Training set accuracy:  0.526\n",
      "Dev set accuracy:  0.5176666666666667\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run this cell to test your model\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_data(\"train.txt\")\n",
    "X, y = data.text, data.target\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.3)\n",
    "cls = SentimentClassifier(feature_method=dumb_featurize)\n",
    "cls = cls.fit(X_train, y_train)\n",
    "print(\"Training set accuracy: \", cls.score(X_train, y_train))\n",
    "print(\"Dev set accuracy: \", cls.score(X_dev, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T19:59:41.183712Z",
     "start_time": "2020-02-23T19:59:40.752925Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run this cell to save weights and the prediction\n",
    "\"\"\"\n",
    "try:\n",
    "    weights = cls.save_weights()\n",
    "except:\n",
    "    pass\n",
    "X_test = load_data(\"test.txt\").text\n",
    "save_prediction(cls.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Example of better featurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-23T20:00:38.032607Z",
     "start_time": "2020-02-23T20:00:00.458431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training...\n",
      "7000 6324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:  0.9278571428571428\n",
      "Dev set accuracy:  0.7643333333333333\n"
     ]
    }
   ],
   "source": [
    "def bag_of_words(text):\n",
    "    word_bag = Counter(text.lower().split(\" \"))\n",
    "\n",
    "    return word_bag\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_data(\"train.txt\")\n",
    "X, y = data.text, data.target\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.3)\n",
    "cls = SentimentClassifier(feature_method=bag_of_words, min_feature_ct=10)\n",
    "cls = cls.fit(X_train, y_train)\n",
    "print(\"Training set accuracy: \", cls.score(X_train, y_train))\n",
    "print(\"Dev set accuracy: \", cls.score(X_dev, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training...\n",
      "7000 17344\n",
      "Training set accuracy:  0.9071428571428571\n",
      "Dev set accuracy:  0.793\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run this cell to test your model\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_data(\"train.txt\")\n",
    "X, y = data.text, data.target\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.3)\n",
    "cls = SentimentClassifier(feature_method=better_featurize, min_feature_ct=10,L2_reg=1.)\n",
    "cls = cls.fit(X_train, y_train)\n",
    "print(\"Training set accuracy: \", cls.score(X_train, y_train))\n",
    "print(\"Dev set accuracy: \", cls.score(X_dev, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#y_pred = cls.predict(X_test)\n",
    "#plt.hist([y_train,y_dev,y_pred],label=['train', 'dev', 'test'])\n",
    "#plt.legend()\n",
    "bag_of_words_method = {'Logistic Regression': (0.928,0.764),\"SVM\":(0.994,0.738),\"RandomForestClassifier\":(0.799,0.718),\"XGBoost\":(0.916,0.752)}\n",
    "own_method = {'Logistic Regression': (0.907,0.793),\"SVM\":(0.995,0.770),\"RandomForestClassifier\":(0.791,0.724),\"XGBoost\":(0.955,0.732)}\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "methods = bag_of_words_method.keys()\n",
    "train = [value[0] for value in bag_of_words_method.values()]\n",
    "dev = [value[1] for value in bag_of_words_method.values()]\n",
    "train1 = [value[0] for value in own_method.values()]\n",
    "dev1 = [value[1] for value in own_method.values()]\n",
    "X = np.arange(len(methods))*2\n",
    "ax.bar(X-0.375, train,width = 0.25)\n",
    "ax.bar(X-0.125, dev,width = 0.25)\n",
    "ax.bar(X+0.125, train1,width = 0.25)\n",
    "ax.bar(X+0.375, dev1,width = 0.25)\n",
    "ax.set_xticks(X)\n",
    "ax.set_xticklabels(methods)\n",
    "ax.legend(labels=['bow train', 'bow dev','own train','own dev'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([3485.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "         3515.]),\n",
       "  array([1526.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "         1474.]),\n",
       "  array([1861.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "         1853.])],\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <a list of 3 Lists of Patches objects>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([y_train, y_dev,cls.predict(X_test)]) # very balanced dataset! good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
