{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this homework you will be given a chance to explore the properties of word embedding using a pre-trained embedding. Then you will build and train your own embedding using the skip-gram method.\n",
    "\n",
    "## Files\n",
    "- `hw03.ipynb`: Notebook file with starter code\n",
    "- `plot_summaries_tokenized.txt`: training data for skipgram\n",
    "- `glove_6B_100d_top100k.csv`: pretrained glove embedding\n",
    "\n",
    "The training data and glove embedding \n",
    "\n",
    "\n",
    "## Reading material\n",
    "\n",
    "1. [Learning representations by back-propagating errors](https://www.nature.com/articles/323533a0)\n",
    "\n",
    "Recent Turing Reward winner Geoffrey Hinton and coworkers first introduced the concept of words embedding in their 1986 Nature paper.\n",
    "\n",
    "2. [word2vec](https://code.google.com/archive/p/word2vec/)\n",
    "\n",
    "Google's word2vec project built on skip-gram and google news data.\n",
    "\n",
    "3. [Efficient Estimation of Word Representations in\n",
    "Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "\n",
    "   [Distributed Representations of Words and Phrases\n",
    "and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf)\n",
    "\n",
    "Tomas Mikolov from Google published these two papers in 2013 proposing the skip-gram approach for word embedding which has become one of the most popular word embedding.\n",
    "\n",
    "4. [On word embeddings](http://ruder.io/word-embeddings-1/index.html)\n",
    "\n",
    "An online blog by DeepMind engineer Sebastian Ruder explaining skip-gram. I found it easier to understand than the original papers.\n",
    "\n",
    "## Deliverables:\n",
    "\n",
    "- pdf / html version of your final notebook\n",
    "- Discuss the questions in Section 3 (Play with pretrained)\n",
    "- If you have done any work to improve the model and model training, explain it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> HW3 Write Up </center> </h1>\n",
    "\n",
    "<h2>Find nearest words</h2>\n",
    "The closest word to '8' are '9' and '7' which is very interesting.\n",
    "\n",
    "The closest word to google are also technology companies.\n",
    "<h2>Find nearest words with vector</h2>\n",
    "glove['china']+glove['capital']\n",
    "'beijing' is the closest output!\n",
    "\n",
    "glove['beijing']-glove['china']+glove['italy']\n",
    "'rome' is the closest output!\n",
    "It means this embedding is able to reflect some deep connection/semantics of the words.\n",
    "\n",
    "<h2>skip-gram</h2>\n",
    "I tried to use more layers and regulazations, it tures out the losses for the skip-gram dummy classification problem is not stably going down and keep flat. It flucutate between 0.3 and 0.4, and the lowest losses is 0.29. Increasing the negative samples or enlarge the window size might make it better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =========================Coding starts here =================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required pacakges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T23:49:58.388977Z",
     "start_time": "2020-03-07T23:49:58.369081Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn, keras\n",
    "import numpy as np\n",
    "import os, sys\n",
    "\n",
    "# add utils folder to path\n",
    "p = os.path.dirname(os.getcwd())\n",
    "if p not in sys.path:\n",
    "    sys.path = [p] + sys.path\n",
    "\n",
    "from utils.hw3 import load_data\n",
    "from utils.general import show_keras_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play with pretrained embedding\n",
    "\n",
    "Before we start training our own words embedding, let's play with pretrained embeddings, so you know what you can expect from your own models. Here we use a very popular embedding called [GloVe](https://nlp.stanford.edu/projects/glove/) developed by standford university. The method used to produce this embedding is based on the factorization of word-word similarity matrix. Worth to notice, thi method is quite different to the skip-gram method we are going to implement later.\n",
    "\n",
    "First let's load the embedding as a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T23:44:59.746988Z",
     "start_time": "2020-03-07T23:44:23.376812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>,</th>\n",
       "      <th>.</th>\n",
       "      <th>of</th>\n",
       "      <th>to</th>\n",
       "      <th>and</th>\n",
       "      <th>in</th>\n",
       "      <th>a</th>\n",
       "      <th>\"</th>\n",
       "      <th>'s</th>\n",
       "      <th>...</th>\n",
       "      <th>antz</th>\n",
       "      <th>monnaie</th>\n",
       "      <th>copyist</th>\n",
       "      <th>toonami</th>\n",
       "      <th>bagration</th>\n",
       "      <th>divo</th>\n",
       "      <th>convulsive</th>\n",
       "      <th>unviable</th>\n",
       "      <th>madhava</th>\n",
       "      <th>autocephalous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.038194</td>\n",
       "      <td>-0.10767</td>\n",
       "      <td>-0.33979</td>\n",
       "      <td>-0.15290</td>\n",
       "      <td>-0.189700</td>\n",
       "      <td>-0.071953</td>\n",
       "      <td>0.085703</td>\n",
       "      <td>-0.270860</td>\n",
       "      <td>-0.30457</td>\n",
       "      <td>0.58854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175470</td>\n",
       "      <td>-0.032896</td>\n",
       "      <td>0.37886</td>\n",
       "      <td>0.339320</td>\n",
       "      <td>-0.52125</td>\n",
       "      <td>0.20375</td>\n",
       "      <td>0.42829</td>\n",
       "      <td>-0.560580</td>\n",
       "      <td>-0.26374</td>\n",
       "      <td>0.457840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.244870</td>\n",
       "      <td>0.11053</td>\n",
       "      <td>0.20941</td>\n",
       "      <td>-0.24279</td>\n",
       "      <td>0.050024</td>\n",
       "      <td>0.231270</td>\n",
       "      <td>-0.222010</td>\n",
       "      <td>0.044006</td>\n",
       "      <td>-0.23645</td>\n",
       "      <td>-0.20250</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.559680</td>\n",
       "      <td>-0.721960</td>\n",
       "      <td>-0.24468</td>\n",
       "      <td>-0.266210</td>\n",
       "      <td>-1.09030</td>\n",
       "      <td>-0.07108</td>\n",
       "      <td>0.29863</td>\n",
       "      <td>-0.486880</td>\n",
       "      <td>0.20904</td>\n",
       "      <td>0.154020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.728120</td>\n",
       "      <td>0.59812</td>\n",
       "      <td>0.46348</td>\n",
       "      <td>0.89837</td>\n",
       "      <td>0.190840</td>\n",
       "      <td>0.023731</td>\n",
       "      <td>0.165690</td>\n",
       "      <td>-0.020260</td>\n",
       "      <td>0.17576</td>\n",
       "      <td>0.73479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.480470</td>\n",
       "      <td>-0.129030</td>\n",
       "      <td>0.45714</td>\n",
       "      <td>-0.224130</td>\n",
       "      <td>-0.66645</td>\n",
       "      <td>-0.13259</td>\n",
       "      <td>-0.15156</td>\n",
       "      <td>-0.314340</td>\n",
       "      <td>-1.12440</td>\n",
       "      <td>0.321590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.399610</td>\n",
       "      <td>-0.54361</td>\n",
       "      <td>-0.64792</td>\n",
       "      <td>0.16996</td>\n",
       "      <td>-0.049184</td>\n",
       "      <td>-0.506380</td>\n",
       "      <td>0.133730</td>\n",
       "      <td>-0.173950</td>\n",
       "      <td>-0.72854</td>\n",
       "      <td>-0.68338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287520</td>\n",
       "      <td>1.169300</td>\n",
       "      <td>0.17411</td>\n",
       "      <td>-1.158300</td>\n",
       "      <td>0.23457</td>\n",
       "      <td>0.15806</td>\n",
       "      <td>0.21284</td>\n",
       "      <td>-0.174410</td>\n",
       "      <td>0.25690</td>\n",
       "      <td>0.807420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.083172</td>\n",
       "      <td>0.67396</td>\n",
       "      <td>-0.38377</td>\n",
       "      <td>0.53516</td>\n",
       "      <td>-0.089737</td>\n",
       "      <td>0.339230</td>\n",
       "      <td>0.382390</td>\n",
       "      <td>0.644400</td>\n",
       "      <td>-0.28343</td>\n",
       "      <td>-0.19675</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021609</td>\n",
       "      <td>0.335410</td>\n",
       "      <td>0.49281</td>\n",
       "      <td>-0.083679</td>\n",
       "      <td>0.36971</td>\n",
       "      <td>-0.48602</td>\n",
       "      <td>-0.13771</td>\n",
       "      <td>0.041936</td>\n",
       "      <td>-0.71330</td>\n",
       "      <td>-0.074811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        the        ,        .       of        to       and        in  \\\n",
       "0 -0.038194 -0.10767 -0.33979 -0.15290 -0.189700 -0.071953  0.085703   \n",
       "1 -0.244870  0.11053  0.20941 -0.24279  0.050024  0.231270 -0.222010   \n",
       "2  0.728120  0.59812  0.46348  0.89837  0.190840  0.023731  0.165690   \n",
       "3 -0.399610 -0.54361 -0.64792  0.16996 -0.049184 -0.506380  0.133730   \n",
       "4  0.083172  0.67396 -0.38377  0.53516 -0.089737  0.339230  0.382390   \n",
       "\n",
       "          a        \"       's      ...            antz   monnaie  copyist  \\\n",
       "0 -0.270860 -0.30457  0.58854      ...        0.175470 -0.032896  0.37886   \n",
       "1  0.044006 -0.23645 -0.20250      ...       -0.559680 -0.721960 -0.24468   \n",
       "2 -0.020260  0.17576  0.73479      ...        0.480470 -0.129030  0.45714   \n",
       "3 -0.173950 -0.72854 -0.68338      ...        0.287520  1.169300  0.17411   \n",
       "4  0.644400 -0.28343 -0.19675      ...       -0.021609  0.335410  0.49281   \n",
       "\n",
       "    toonami  bagration     divo  convulsive  unviable  madhava  autocephalous  \n",
       "0  0.339320   -0.52125  0.20375     0.42829 -0.560580 -0.26374       0.457840  \n",
       "1 -0.266210   -1.09030 -0.07108     0.29863 -0.486880  0.20904       0.154020  \n",
       "2 -0.224130   -0.66645 -0.13259    -0.15156 -0.314340 -1.12440       0.321590  \n",
       "3 -1.158300    0.23457  0.15806     0.21284 -0.174410  0.25690       0.807420  \n",
       "4 -0.083679    0.36971 -0.48602    -0.13771  0.041936 -0.71330      -0.074811  \n",
       "\n",
       "[5 rows x 100000 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove = pd.read_csv(\"glove_6B_100d_top100k.csv\"); glove.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find nearest words\n",
    "One of the many motivations that people are interested in words embedding is that it reveals similarities between words. Let's first check how this works with GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T23:58:51.519272Z",
     "start_time": "2020-03-07T23:58:49.105932Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances\n",
    "\n",
    "def find_nearest(embedding, word=None, n=5, distance=euclidean_distances):\n",
    "    \"\"\"\n",
    "    For given embedding matrix and a given word, find the n nearest words in the embedding space\n",
    "    \n",
    "    input:\n",
    "        embedding: DataFrame, look at `glove` \n",
    "        word: string, must be in the index of embedding dataframe\n",
    "        n: int, number of nearest words\n",
    "        distance: fucntion, it should at least support the euclidean_distances and cosine_distances\n",
    "        \n",
    "    return:\n",
    "        A series with word as index, distance as value, sorted from lower to high\n",
    "    \"\"\"\n",
    "\n",
    "    if word not in embedding:\n",
    "        print('word not in pre-trained model')\n",
    "        return\n",
    "    targetvec = embedding[word]\n",
    "    d = distance(embedding.T,[targetvec])\n",
    "    d = {embedding.columns[index]:value for (index, value) in enumerate(list(d))}\n",
    "    import heapq\n",
    "    k_keys_sorted = heapq.nsmallest(n+1, d,key=d.get)\n",
    "    return pd.Series({k:d[k][0] for k in k_keys_sorted[1:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T01:03:37.284629Z",
     "start_time": "2019-03-30T01:03:37.132508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using euclidean_distances, the closest words to frog are:\n",
      "dragon      4.210776\n",
      "elephant    4.273111\n",
      "wolf        4.667664\n",
      "beast       4.737944\n",
      "cat         4.739263\n",
      "dtype: float64\n",
      "Using cosine_distances, the closest words to frog are:\n",
      "dragon      0.301670\n",
      "elephant    0.329856\n",
      "leopard     0.386800\n",
      "bear        0.386803\n",
      "cat         0.404449\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Using euclidean_distances, the closest words to frog are:\")\n",
    "print(find_nearest(glove, 'lion'))\n",
    "print(\"Using cosine_distances, the closest words to frog are:\")\n",
    "print(find_nearest(glove, 'lion', distance=cosine_distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T23:45:44.318582Z",
     "start_time": "2020-03-07T23:45:44.126284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9    0.943478\n",
      "7    0.943649\n",
      "6    1.037347\n",
      "5    1.111455\n",
      "4    1.339804\n",
      "dtype: float64\n",
      "yahoo        0.136264\n",
      "microsoft    0.189590\n",
      "web          0.237019\n",
      "aol          0.242831\n",
      "facebook     0.248342\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(find_nearest(glove, '8'))\n",
    "\n",
    "print(find_nearest(glove, 'google', distance=cosine_distances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T02:05:13.828526Z",
     "start_time": "2019-03-29T02:05:13.732489Z"
    }
   },
   "source": [
    "What have you observed? Does the result make sense to you? Play with some other words, and see if you can find something interesting. Try countries and numebrs :). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find nearest words with vector\n",
    "Remember that at the beginning of the course we advertised the ability of word embedding being able to find relative relationship between words, such as king - male + female = queen. Let's test this with the embedding we have. But before that we need a method that's similar to find_nearest, but instead of taking a word, it takes an embedding vector as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T23:58:59.476431Z",
     "start_time": "2020-03-07T23:58:59.469863Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_nearest_with_vector(embedding, vector=None, n=5, distance=euclidean_distances):\n",
    "    \"\"\"\n",
    "    For given embedding matrix and a given vector, find the n nearest words in the embedding space\n",
    "    \n",
    "    input:\n",
    "        embedding: DataFrame, look at `glove` \n",
    "        vector: Series, looks like a coloumn vector of the embedding dataframe\n",
    "        n: int, number of nearest words\n",
    "        distance: fucntion, it should at least support the euclidean_distances and cosine_distances\n",
    "        \n",
    "    return:\n",
    "        A series with word as index, distance as value, sorted from lower to high\n",
    "    \"\"\"\n",
    "    \n",
    "    d = distance(embedding.T,[vector])\n",
    "    d = {embedding.columns[index]:value for (index, value) in enumerate(list(d))}\n",
    "    import heapq\n",
    "    k_keys_sorted = heapq.nsmallest(n+1, d,key=d.get)\n",
    "    return pd.Series({k:d[k][0] for k in k_keys_sorted[1:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T06:02:47.514735Z",
     "start_time": "2019-03-29T06:02:47.422576Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "queen      4.340831\n",
       "prince     4.542057\n",
       "monarch    4.704783\n",
       "brother    4.717307\n",
       "george     4.840019\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest_with_vector(glove, glove['king']-glove['male']+glove['female'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T23:46:03.833808Z",
     "start_time": "2020-03-07T23:46:03.754128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital       6.866541\n",
      "beijing       7.820086\n",
      "chinese       8.027392\n",
      "taiwan        8.451052\n",
      "government    8.454450\n",
      "dtype: float64\n",
      "rome       4.265470\n",
      "turin      4.500863\n",
      "milan      4.876893\n",
      "italian    4.959218\n",
      "naples     5.059326\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(find_nearest_with_vector(glove, glove['china']+glove['capital']))\n",
    "print(find_nearest_with_vector(glove, glove['beijing']-glove['china']+glove['italy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T06:04:08.834136Z",
     "start_time": "2019-03-29T06:04:08.755134Z"
    }
   },
   "source": [
    "What did you see? Can you explore some other interesting relations? Like countries vs cities, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word clustering\n",
    "\n",
    "Another feature of the word embedding is that it can cluster similar word in to the same cluster while keep semantic relationship with other clusters. Try the following dimention reduction code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T23:46:16.857248Z",
     "start_time": "2020-03-07T23:46:16.301587Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_2D(X, labels):\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = 0.1 + 0.8 * (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for x, lab in zip(X, labels):\n",
    "        plt.text(x[0], x[1], str(lab), fontdict={'size': 14})\n",
    "        \n",
    "def plot_words_embedding(embedding, words):\n",
    "    X = PCA(n_components=2).fit_transform(embedding[words].transpose())\n",
    "    plot_2D(X, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T23:46:51.609021Z",
     "start_time": "2020-03-07T23:46:50.969477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = ['china', 'beijing', 'russia', 'moscow', 'poland', 'warsaw', 'japan', 'tokyo',\n",
    "        'france', 'paris', 'germany', 'berlin', 'italy', 'rome', 'spain', 'madrid']\n",
    "\n",
    "plot_words_embedding(glove, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you spot something interesting? Try with some other words set and see what you can find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T23:50:30.676072Z",
     "start_time": "2020-03-07T23:50:22.675511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of summarys:  42306\n",
      "Number of words: 13065221\n",
      "Vocabulary size: 190786\n"
     ]
    }
   ],
   "source": [
    "#from tools import load_data, show_model\n",
    "\n",
    "text = load_data(\"plot_summaries_tokenized.txt\")\n",
    "\n",
    "print(\"Number of summarys: \", len(text))\n",
    "print(\"Number of words:\", len([w for s in text for w in s]))\n",
    "print(\"Vocabulary size:\", len({w for s in text for w in s}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T04:00:19.581151Z",
     "start_time": "2019-03-28T04:00:19.515978Z"
    }
   },
   "source": [
    "There are about 200K unique words in this corpus. To make it more computational feasible, let's reduce the size of the vocabulary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T23:59:22.225406Z",
     "start_time": "2020-03-07T23:59:22.216506Z"
    }
   },
   "outputs": [],
   "source": [
    "MIN_COUNT = 20\n",
    "def create_encoder(text, min_count=20):\n",
    "    \"\"\"\n",
    "    - Create a encoder which is a dictionary like {word: index}\n",
    "    - To reduce the total number of vocabularies, you can remove \n",
    "    the words that appear for less than min_count times in the entire\n",
    "    corpus\n",
    "    - Enfore {'_unknown_': 0}\n",
    "    \n",
    "    input:\n",
    "        text: list of token list, e.g. [['i', 'am', 'fine'], ['another', 'summary'], ...]\n",
    "    returns:\n",
    "        tokenmap:  encoder dictionary\n",
    "        tokenmap_reverse: reversed tokenmap {index: word} to faciliate inverse lookup\n",
    "    \"\"\"\n",
    "    tokencount = {}\n",
    "    tokenmap = {}\n",
    "    tokenmap_reverse = {}\n",
    "    for tokens in text:\n",
    "        for token in tokens:\n",
    "            tokencount[token] = tokencount.get(token, 0) + 1\n",
    "    index = 0\n",
    "    for tokens in text:\n",
    "        for token in tokens:\n",
    "            if tokencount[token] >= min_count:\n",
    "                if token not in tokenmap:\n",
    "                    tokenmap[token] = index\n",
    "                    tokenmap_reverse[index] = token\n",
    "                    index += 1\n",
    "    return (tokenmap, tokenmap_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T23:51:06.142814Z",
     "start_time": "2020-03-07T23:51:02.367188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the reduced vocabulary size is: 26343\n"
     ]
    }
   ],
   "source": [
    "tokenmap, tokenmap_reverse = create_encoder(text, MIN_COUNT)\n",
    "VOCAB_SIZE = len(tokenmap)\n",
    "print(\"the reduced vocabulary size is:\", VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T23:51:11.898244Z",
     "start_time": "2020-03-07T23:51:11.891922Z"
    }
   },
   "outputs": [],
   "source": [
    "# Encoder the text using the encoder you just created\n",
    "def encode(text, tokenmap, default=0):\n",
    "    return [[tokenmap.get(t, default) for t in s] for s in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T01:32:45.468046Z",
     "start_time": "2019-03-30T01:32:37.844437Z"
    }
   },
   "outputs": [],
   "source": [
    "text_encoded = encode(text, tokenmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct training context pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate training data, we need to find word-context pairs from the encoded text, \n",
    "we also want to generate some negative sample, so the input and output may look like:\n",
    "\n",
    "for input corpus: [[2, 3, 1, 2]] \n",
    "\n",
    "returns: [[word, context, label]]\n",
    "\n",
    "[[2, 3, 1], [2, 1, 1], [2, 2, 1], [3, 1, 1], ...., [4, 2, 0], [4, 3, 0], ...]\n",
    "\n",
    "Notice that in practice the sequence should be shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T23:59:37.507750Z",
     "start_time": "2020-03-07T23:59:29.496374Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "def training_data_generator(text_encoded, window_size=4, negative_samples=1.0, batch_docs=50):\n",
    "    \"\"\"\n",
    "    For given encoded text, return 3 np.array:\n",
    "    words, contexts, labels\n",
    "    Do not pair the w and its context cross different documents.\n",
    "    \n",
    "    input: \n",
    "        text_encoded: list of list of int, each list of int is the numerical encoding of the doc\n",
    "        window_size: int, define the context\n",
    "        negative_samples: float, how much negative sampling you need, normally 1.0\n",
    "        batch_docs: int, number of docs for which it generates one return\n",
    "        \n",
    "    return:\n",
    "        words: list of int, the numerical encoding of the central words\n",
    "        contexts: list of int, the numerical encoding of the context words\n",
    "        labels: list of int, 1 or 0\n",
    "        \n",
    "    hint: \n",
    "    1. You can use skipgrams method from keras\n",
    "    2. For training purpose, words and contexts needs to be 2D array, with shape (N, 1), \n",
    "       but labels is 1D array, with shape (N, )\n",
    "    3. The output can be very big, you SHOULD using generator\n",
    "    \"\"\"\n",
    "    for index, text in enumerate(text_encoded[::batch_docs]):\n",
    "        labels = []\n",
    "        words = []\n",
    "        contexts = []\n",
    "        for t in text_encoded[index:index+batch_docs]:\n",
    "            sg = skipgrams(sequence=t, vocabulary_size=VOCAB_SIZE, window_size=window_size, negative_samples=negative_samples, shuffle=True, categorical=False, sampling_table=None, seed=None)\n",
    "            labels.extend(sg[1])\n",
    "            words.extend([w[0] for w in sg[0]])\n",
    "            contexts.extend([w[1] for w in sg[0]])\n",
    "        N = len(labels)\n",
    "        yield np.array(words).reshape(N,1), np.array(contexts).reshape(N,1), np.array(labels)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create a network that looks like this:\n",
    "<img src=\"skip-gram-NN.png\" width=\"480\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T23:59:39.023522Z",
     "start_time": "2020-03-07T23:59:39.005764Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0318 16:06:57.453799 4315479488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0318 16:06:57.489341 4315479488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0318 16:06:57.513157 4315479488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 100)       2634300     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 100)          0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 100)          0           embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           dot_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 2,634,300\n",
      "Trainable params: 2,634,300\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Activation, Reshape, dot, Embedding, Permute\n",
    "from keras.models import Model\n",
    "\n",
    "# Define two input layers\n",
    "\n",
    "input_words = Input(shape=(1,))\n",
    "input_contexts = Input(shape=(1,))\n",
    "\n",
    "# Embedding for the text\n",
    "embed = Embedding(VOCAB_SIZE, 100, input_length=1)\n",
    "\n",
    "embed1 = embed(input_words)\n",
    "embed1 = Reshape((100,))(embed1)\n",
    "\n",
    "embed2 = embed(input_contexts)\n",
    "embed2 = Reshape((100,))(embed2)\n",
    "\n",
    "# Concatenate the convolutional features and the vector input\n",
    "dot_product = dot([embed1, embed2], axes=1, normalize=False)\n",
    "output = Activation('sigmoid')(dot_product)\n",
    "\n",
    "# define a model with a list of two inputs\n",
    "model = Model(inputs=[input_words, input_contexts], outputs=output)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a simple version of training on batch code. You do not need to use\n",
    "opochs more than 10 since it will soon start shaking around the minimum. If you want \n",
    "to further improve your training, consider gradually increase the batch size or reduce \n",
    "the learning rate, then you can try for more than 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T00:50:28.355452Z",
     "start_time": "2019-03-30T00:50:28.251027Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0318 16:07:15.124368 4315479488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0318 16:07:15.143607 4315479488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0318 16:07:15.151264 4315479488 deprecation.py:323] From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0318 16:07:19.196613 4315479488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trained pairs (M):       0.52 ; \t loss: 0.6930\n",
      "Total trained pairs (M):       1.04 ; \t loss: 0.6925\n",
      "Total trained pairs (M):       1.56 ; \t loss: 0.6919\n",
      "Total trained pairs (M):       2.09 ; \t loss: 0.6913\n",
      "Total trained pairs (M):       2.60 ; \t loss: 0.6905\n",
      "Total trained pairs (M):       3.11 ; \t loss: 0.6896\n",
      "Total trained pairs (M):       3.62 ; \t loss: 0.6885\n",
      "Total trained pairs (M):       4.12 ; \t loss: 0.6874\n",
      "Total trained pairs (M):       4.62 ; \t loss: 0.6861\n",
      "Total trained pairs (M):       5.12 ; \t loss: 0.6847\n",
      "Total trained pairs (M):       5.62 ; \t loss: 0.6832\n",
      "Total trained pairs (M):       6.12 ; \t loss: 0.6816\n",
      "Total trained pairs (M):       6.62 ; \t loss: 0.6798\n",
      "Total trained pairs (M):       7.12 ; \t loss: 0.6780\n",
      "Total trained pairs (M):       7.62 ; \t loss: 0.6760\n",
      "Total trained pairs (M):       8.10 ; \t loss: 0.6741\n",
      "Total trained pairs (M):       8.58 ; \t loss: 0.6720\n",
      "Total trained pairs (M):       9.06 ; \t loss: 0.6698\n",
      "Total trained pairs (M):       9.55 ; \t loss: 0.6675\n",
      "Total trained pairs (M):      10.02 ; \t loss: 0.6651\n",
      "Total trained pairs (M):      10.50 ; \t loss: 0.6627\n",
      "Total trained pairs (M):      10.98 ; \t loss: 0.6602\n",
      "Total trained pairs (M):      11.46 ; \t loss: 0.6577\n",
      "Total trained pairs (M):      11.93 ; \t loss: 0.6550\n",
      "Total trained pairs (M):      12.41 ; \t loss: 0.6524\n",
      "Total trained pairs (M):      12.90 ; \t loss: 0.6500\n",
      "Total trained pairs (M):      13.38 ; \t loss: 0.6473\n",
      "Total trained pairs (M):      13.85 ; \t loss: 0.6444\n",
      "Total trained pairs (M):      14.32 ; \t loss: 0.6417\n",
      "Total trained pairs (M):      14.79 ; \t loss: 0.6387\n",
      "Total trained pairs (M):      15.26 ; \t loss: 0.6360\n",
      "Total trained pairs (M):      15.74 ; \t loss: 0.6333\n",
      "Total trained pairs (M):      16.21 ; \t loss: 0.6301\n",
      "Total trained pairs (M):      16.70 ; \t loss: 0.6272\n",
      "Total trained pairs (M):      17.19 ; \t loss: 0.6244\n",
      "Total trained pairs (M):      17.67 ; \t loss: 0.6214\n",
      "Total trained pairs (M):      18.15 ; \t loss: 0.6181\n",
      "Total trained pairs (M):      18.63 ; \t loss: 0.6147\n",
      "Total trained pairs (M):      19.10 ; \t loss: 0.6114\n",
      "Total trained pairs (M):      19.58 ; \t loss: 0.6079\n",
      "Total trained pairs (M):      20.06 ; \t loss: 0.6051\n",
      "Total trained pairs (M):      20.54 ; \t loss: 0.6027\n",
      "Total trained pairs (M):      21.01 ; \t loss: 0.5996\n",
      "Total trained pairs (M):      21.49 ; \t loss: 0.5969\n",
      "Total trained pairs (M):      21.98 ; \t loss: 0.5937\n",
      "Total trained pairs (M):      22.46 ; \t loss: 0.5904\n",
      "Total trained pairs (M):      22.95 ; \t loss: 0.5867\n",
      "Total trained pairs (M):      23.43 ; \t loss: 0.5834\n",
      "Total trained pairs (M):      23.91 ; \t loss: 0.5798\n",
      "Total trained pairs (M):      24.39 ; \t loss: 0.5763\n",
      "Total trained pairs (M):      24.87 ; \t loss: 0.5740\n",
      "Total trained pairs (M):      25.36 ; \t loss: 0.5705\n",
      "Total trained pairs (M):      25.85 ; \t loss: 0.5671\n",
      "Total trained pairs (M):      26.34 ; \t loss: 0.5637\n",
      "Total trained pairs (M):      26.83 ; \t loss: 0.5598\n",
      "Total trained pairs (M):      27.32 ; \t loss: 0.5575\n",
      "Total trained pairs (M):      27.84 ; \t loss: 0.5564\n",
      "Total trained pairs (M):      28.37 ; \t loss: 0.5541\n",
      "Total trained pairs (M):      28.89 ; \t loss: 0.5512\n",
      "Total trained pairs (M):      29.42 ; \t loss: 0.5477\n",
      "Total trained pairs (M):      29.93 ; \t loss: 0.5450\n",
      "Total trained pairs (M):      30.44 ; \t loss: 0.5428\n",
      "Total trained pairs (M):      30.95 ; \t loss: 0.5389\n",
      "Total trained pairs (M):      31.46 ; \t loss: 0.5353\n",
      "Total trained pairs (M):      31.95 ; \t loss: 0.5328\n",
      "Total trained pairs (M):      32.44 ; \t loss: 0.5297\n",
      "Total trained pairs (M):      32.92 ; \t loss: 0.5261\n",
      "Total trained pairs (M):      33.41 ; \t loss: 0.5228\n",
      "Total trained pairs (M):      33.90 ; \t loss: 0.5199\n",
      "Total trained pairs (M):      34.39 ; \t loss: 0.5167\n",
      "Total trained pairs (M):      34.88 ; \t loss: 0.5134\n",
      "Total trained pairs (M):      35.38 ; \t loss: 0.5120\n",
      "Total trained pairs (M):      35.87 ; \t loss: 0.5088\n",
      "Total trained pairs (M):      36.37 ; \t loss: 0.5064\n",
      "Total trained pairs (M):      36.86 ; \t loss: 0.5029\n",
      "Total trained pairs (M):      37.36 ; \t loss: 0.5016\n",
      "Total trained pairs (M):      37.86 ; \t loss: 0.5009\n",
      "Total trained pairs (M):      38.35 ; \t loss: 0.4982\n",
      "Total trained pairs (M):      38.84 ; \t loss: 0.4963\n",
      "Total trained pairs (M):      39.35 ; \t loss: 0.4957\n",
      "Total trained pairs (M):      39.86 ; \t loss: 0.4925\n",
      "Total trained pairs (M):      40.38 ; \t loss: 0.4904\n",
      "Total trained pairs (M):      40.90 ; \t loss: 0.4892\n",
      "Total trained pairs (M):      41.42 ; \t loss: 0.4863\n",
      "Total trained pairs (M):      41.94 ; \t loss: 0.4833\n",
      "Total trained pairs (M):      42.44 ; \t loss: 0.4803\n",
      "Total trained pairs (M):      42.98 ; \t loss: 0.4811\n",
      "Total trained pairs (M):      43.51 ; \t loss: 0.4798\n",
      "Total trained pairs (M):      44.05 ; \t loss: 0.4773\n",
      "Total trained pairs (M):      44.59 ; \t loss: 0.4754\n",
      "Total trained pairs (M):      45.13 ; \t loss: 0.4730\n",
      "Total trained pairs (M):      45.69 ; \t loss: 0.4720\n",
      "Total trained pairs (M):      46.24 ; \t loss: 0.4696\n",
      "Total trained pairs (M):      46.80 ; \t loss: 0.4678\n",
      "Total trained pairs (M):      47.35 ; \t loss: 0.4657\n",
      "Total trained pairs (M):      47.91 ; \t loss: 0.4649\n",
      "Total trained pairs (M):      48.47 ; \t loss: 0.4630\n",
      "Total trained pairs (M):      49.02 ; \t loss: 0.4608\n",
      "Total trained pairs (M):      49.58 ; \t loss: 0.4584\n",
      "Total trained pairs (M):      50.10 ; \t loss: 0.4598\n",
      "Total trained pairs (M):      50.61 ; \t loss: 0.4574\n",
      "Total trained pairs (M):      51.13 ; \t loss: 0.4587\n",
      "Total trained pairs (M):      51.64 ; \t loss: 0.4572\n",
      "Total trained pairs (M):      52.15 ; \t loss: 0.4552\n",
      "Total trained pairs (M):      52.67 ; \t loss: 0.4529\n",
      "Total trained pairs (M):      53.18 ; \t loss: 0.4505\n",
      "Total trained pairs (M):      53.69 ; \t loss: 0.4490\n",
      "Total trained pairs (M):      54.20 ; \t loss: 0.4467\n",
      "Total trained pairs (M):      54.71 ; \t loss: 0.4460\n",
      "Total trained pairs (M):      55.21 ; \t loss: 0.4437\n",
      "Total trained pairs (M):      55.73 ; \t loss: 0.4438\n",
      "Total trained pairs (M):      56.23 ; \t loss: 0.4404\n",
      "Total trained pairs (M):      56.74 ; \t loss: 0.4385\n",
      "Total trained pairs (M):      57.26 ; \t loss: 0.4382\n",
      "Total trained pairs (M):      57.77 ; \t loss: 0.4367\n",
      "Total trained pairs (M):      58.29 ; \t loss: 0.4362\n",
      "Total trained pairs (M):      58.81 ; \t loss: 0.4359\n",
      "Total trained pairs (M):      59.33 ; \t loss: 0.4348\n",
      "Total trained pairs (M):      59.85 ; \t loss: 0.4332\n",
      "Total trained pairs (M):      60.38 ; \t loss: 0.4321\n",
      "Total trained pairs (M):      60.91 ; \t loss: 0.4299\n",
      "Total trained pairs (M):      61.44 ; \t loss: 0.4273\n",
      "Total trained pairs (M):      61.97 ; \t loss: 0.4242\n",
      "Total trained pairs (M):      62.50 ; \t loss: 0.4244\n",
      "Total trained pairs (M):      63.03 ; \t loss: 0.4225\n",
      "Total trained pairs (M):      63.55 ; \t loss: 0.4249\n",
      "Total trained pairs (M):      64.08 ; \t loss: 0.4226\n",
      "Total trained pairs (M):      64.60 ; \t loss: 0.4215\n",
      "Total trained pairs (M):      65.12 ; \t loss: 0.4191\n",
      "Total trained pairs (M):      65.64 ; \t loss: 0.4195\n",
      "Total trained pairs (M):      66.17 ; \t loss: 0.4194\n",
      "Total trained pairs (M):      66.69 ; \t loss: 0.4191\n",
      "Total trained pairs (M):      67.21 ; \t loss: 0.4178\n",
      "Total trained pairs (M):      67.73 ; \t loss: 0.4177\n",
      "Total trained pairs (M):      68.24 ; \t loss: 0.4157\n",
      "Total trained pairs (M):      68.75 ; \t loss: 0.4146\n",
      "Total trained pairs (M):      69.26 ; \t loss: 0.4129\n",
      "Total trained pairs (M):      69.79 ; \t loss: 0.4140\n",
      "Total trained pairs (M):      70.31 ; \t loss: 0.4110\n",
      "Total trained pairs (M):      70.84 ; \t loss: 0.4093\n",
      "Total trained pairs (M):      71.37 ; \t loss: 0.4116\n",
      "Total trained pairs (M):      71.89 ; \t loss: 0.4117\n",
      "Total trained pairs (M):      72.43 ; \t loss: 0.4162\n",
      "Total trained pairs (M):      72.96 ; \t loss: 0.4159\n",
      "Total trained pairs (M):      73.49 ; \t loss: 0.4164\n",
      "Total trained pairs (M):      74.03 ; \t loss: 0.4146\n",
      "Total trained pairs (M):      74.56 ; \t loss: 0.4126\n",
      "Total trained pairs (M):      75.10 ; \t loss: 0.4113\n",
      "Total trained pairs (M):      75.63 ; \t loss: 0.4100\n",
      "Total trained pairs (M):      76.17 ; \t loss: 0.4078\n",
      "Total trained pairs (M):      76.70 ; \t loss: 0.4087\n",
      "Total trained pairs (M):      77.22 ; \t loss: 0.4070\n",
      "Total trained pairs (M):      77.75 ; \t loss: 0.4071\n",
      "Total trained pairs (M):      78.29 ; \t loss: 0.4052\n",
      "Total trained pairs (M):      78.82 ; \t loss: 0.4033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trained pairs (M):      79.34 ; \t loss: 0.4058\n",
      "Total trained pairs (M):      79.84 ; \t loss: 0.4087\n",
      "Total trained pairs (M):      80.32 ; \t loss: 0.4085\n",
      "Total trained pairs (M):      80.81 ; \t loss: 0.4072\n",
      "Total trained pairs (M):      81.30 ; \t loss: 0.4048\n",
      "Total trained pairs (M):      81.79 ; \t loss: 0.4042\n",
      "Total trained pairs (M):      82.27 ; \t loss: 0.4019\n",
      "Total trained pairs (M):      82.77 ; \t loss: 0.4058\n",
      "Total trained pairs (M):      83.28 ; \t loss: 0.4081\n",
      "Total trained pairs (M):      83.79 ; \t loss: 0.4088\n",
      "Total trained pairs (M):      84.31 ; \t loss: 0.4069\n",
      "Total trained pairs (M):      84.83 ; \t loss: 0.4074\n",
      "Total trained pairs (M):      85.35 ; \t loss: 0.4075\n",
      "Total trained pairs (M):      85.86 ; \t loss: 0.4065\n",
      "Total trained pairs (M):      86.38 ; \t loss: 0.4066\n",
      "Total trained pairs (M):      86.89 ; \t loss: 0.4035\n",
      "Total trained pairs (M):      87.40 ; \t loss: 0.4073\n",
      "Total trained pairs (M):      87.91 ; \t loss: 0.4053\n",
      "Total trained pairs (M):      88.41 ; \t loss: 0.4060\n",
      "Total trained pairs (M):      88.91 ; \t loss: 0.4041\n",
      "Total trained pairs (M):      89.40 ; \t loss: 0.4051\n",
      "Total trained pairs (M):      89.88 ; \t loss: 0.4047\n",
      "Total trained pairs (M):      90.36 ; \t loss: 0.4053\n",
      "Total trained pairs (M):      90.84 ; \t loss: 0.4031\n",
      "Total trained pairs (M):      91.30 ; \t loss: 0.4079\n",
      "Total trained pairs (M):      91.76 ; \t loss: 0.4065\n",
      "Total trained pairs (M):      92.21 ; \t loss: 0.4055\n",
      "Total trained pairs (M):      92.66 ; \t loss: 0.4079\n",
      "Total trained pairs (M):      93.10 ; \t loss: 0.4065\n",
      "Total trained pairs (M):      93.54 ; \t loss: 0.4074\n",
      "Total trained pairs (M):      93.98 ; \t loss: 0.4063\n",
      "Total trained pairs (M):      94.41 ; \t loss: 0.4090\n",
      "Total trained pairs (M):      94.83 ; \t loss: 0.4119\n",
      "Total trained pairs (M):      95.26 ; \t loss: 0.4107\n",
      "Total trained pairs (M):      95.67 ; \t loss: 0.4126\n",
      "Total trained pairs (M):      96.10 ; \t loss: 0.4191\n",
      "Total trained pairs (M):      96.52 ; \t loss: 0.4192\n",
      "Total trained pairs (M):      96.94 ; \t loss: 0.4225\n",
      "Total trained pairs (M):      97.36 ; \t loss: 0.4223\n",
      "Total trained pairs (M):      97.78 ; \t loss: 0.4213\n",
      "Total trained pairs (M):      98.18 ; \t loss: 0.4264\n",
      "Total trained pairs (M):      98.59 ; \t loss: 0.4271\n",
      "Total trained pairs (M):      99.02 ; \t loss: 0.4289\n",
      "Total trained pairs (M):      99.46 ; \t loss: 0.4360\n",
      "Total trained pairs (M):      99.91 ; \t loss: 0.4432\n",
      "Total trained pairs (M):     100.37 ; \t loss: 0.4421\n",
      "Total trained pairs (M):     100.82 ; \t loss: 0.4434\n",
      "Total trained pairs (M):     101.27 ; \t loss: 0.4406\n",
      "Total trained pairs (M):     101.72 ; \t loss: 0.4407\n",
      "Total trained pairs (M):     102.16 ; \t loss: 0.4397\n",
      "Total trained pairs (M):     102.60 ; \t loss: 0.4393\n",
      "Total trained pairs (M):     103.05 ; \t loss: 0.4371\n",
      "Total trained pairs (M):     103.49 ; \t loss: 0.4382\n",
      "Total trained pairs (M):     103.94 ; \t loss: 0.4385\n",
      "Total trained pairs (M):     104.39 ; \t loss: 0.4389\n",
      "Total trained pairs (M):     104.84 ; \t loss: 0.4412\n",
      "Total trained pairs (M):     105.29 ; \t loss: 0.4408\n",
      "Total trained pairs (M):     105.74 ; \t loss: 0.4409\n",
      "Total trained pairs (M):     106.19 ; \t loss: 0.4435\n",
      "Total trained pairs (M):     106.64 ; \t loss: 0.4414\n",
      "Total trained pairs (M):     107.08 ; \t loss: 0.4411\n",
      "Total trained pairs (M):     107.52 ; \t loss: 0.4415\n",
      "Total trained pairs (M):     107.96 ; \t loss: 0.4421\n",
      "Total trained pairs (M):     108.40 ; \t loss: 0.4446\n",
      "Total trained pairs (M):     108.84 ; \t loss: 0.4444\n",
      "Total trained pairs (M):     109.28 ; \t loss: 0.4438\n",
      "Total trained pairs (M):     109.72 ; \t loss: 0.4409\n",
      "Total trained pairs (M):     110.16 ; \t loss: 0.4399\n",
      "Total trained pairs (M):     110.60 ; \t loss: 0.4398\n",
      "Total trained pairs (M):     111.04 ; \t loss: 0.4448\n",
      "Total trained pairs (M):     111.49 ; \t loss: 0.4446\n",
      "Total trained pairs (M):     111.93 ; \t loss: 0.4454\n",
      "Total trained pairs (M):     112.38 ; \t loss: 0.4427\n",
      "Total trained pairs (M):     112.82 ; \t loss: 0.4420\n",
      "Total trained pairs (M):     113.26 ; \t loss: 0.4465\n",
      "Total trained pairs (M):     113.71 ; \t loss: 0.4467\n",
      "Total trained pairs (M):     114.15 ; \t loss: 0.4457\n",
      "Total trained pairs (M):     114.61 ; \t loss: 0.4462\n",
      "Total trained pairs (M):     115.06 ; \t loss: 0.4489\n",
      "Total trained pairs (M):     115.51 ; \t loss: 0.4484\n",
      "Total trained pairs (M):     115.98 ; \t loss: 0.4491\n",
      "Total trained pairs (M):     116.45 ; \t loss: 0.4531\n",
      "Total trained pairs (M):     116.90 ; \t loss: 0.4534\n",
      "Total trained pairs (M):     117.36 ; \t loss: 0.4519\n",
      "Total trained pairs (M):     117.82 ; \t loss: 0.4519\n",
      "Total trained pairs (M):     118.27 ; \t loss: 0.4536\n",
      "Total trained pairs (M):     118.72 ; \t loss: 0.4508\n",
      "Total trained pairs (M):     119.16 ; \t loss: 0.4575\n",
      "Total trained pairs (M):     119.60 ; \t loss: 0.4587\n",
      "Total trained pairs (M):     120.03 ; \t loss: 0.4590\n",
      "Total trained pairs (M):     120.47 ; \t loss: 0.4590\n",
      "Total trained pairs (M):     120.90 ; \t loss: 0.4585\n",
      "Total trained pairs (M):     121.34 ; \t loss: 0.4593\n",
      "Total trained pairs (M):     121.78 ; \t loss: 0.4571\n",
      "Total trained pairs (M):     122.25 ; \t loss: 0.4747\n",
      "Total trained pairs (M):     122.73 ; \t loss: 0.4721\n",
      "Total trained pairs (M):     123.20 ; \t loss: 0.4706\n",
      "Total trained pairs (M):     123.67 ; \t loss: 0.4721\n",
      "Total trained pairs (M):     124.15 ; \t loss: 0.4746\n",
      "Total trained pairs (M):     124.63 ; \t loss: 0.4712\n",
      "Total trained pairs (M):     125.11 ; \t loss: 0.4708\n",
      "Total trained pairs (M):     125.58 ; \t loss: 0.4705\n",
      "Total trained pairs (M):     126.05 ; \t loss: 0.4680\n",
      "Total trained pairs (M):     126.53 ; \t loss: 0.4683\n",
      "Total trained pairs (M):     127.00 ; \t loss: 0.4673\n",
      "Total trained pairs (M):     127.48 ; \t loss: 0.4659\n",
      "Total trained pairs (M):     127.96 ; \t loss: 0.4646\n",
      "Total trained pairs (M):     128.43 ; \t loss: 0.4663\n",
      "Total trained pairs (M):     128.89 ; \t loss: 0.4680\n",
      "Total trained pairs (M):     129.34 ; \t loss: 0.4669\n",
      "Total trained pairs (M):     129.81 ; \t loss: 0.4636\n",
      "Total trained pairs (M):     130.27 ; \t loss: 0.4677\n",
      "Total trained pairs (M):     130.73 ; \t loss: 0.4669\n",
      "Total trained pairs (M):     131.20 ; \t loss: 0.4662\n",
      "Total trained pairs (M):     131.68 ; \t loss: 0.4647\n",
      "Total trained pairs (M):     132.16 ; \t loss: 0.4651\n",
      "Total trained pairs (M):     132.63 ; \t loss: 0.4647\n",
      "Total trained pairs (M):     133.13 ; \t loss: 0.4655\n",
      "Total trained pairs (M):     133.62 ; \t loss: 0.4649\n",
      "Total trained pairs (M):     134.13 ; \t loss: 0.4646\n",
      "Total trained pairs (M):     134.63 ; \t loss: 0.4626\n",
      "Total trained pairs (M):     135.14 ; \t loss: 0.4653\n",
      "Total trained pairs (M):     135.65 ; \t loss: 0.4646\n",
      "Total trained pairs (M):     136.17 ; \t loss: 0.4644\n",
      "Total trained pairs (M):     136.68 ; \t loss: 0.4625\n",
      "Total trained pairs (M):     137.19 ; \t loss: 0.4624\n",
      "Total trained pairs (M):     137.70 ; \t loss: 0.4603\n",
      "Total trained pairs (M):     138.21 ; \t loss: 0.4584\n",
      "Total trained pairs (M):     138.72 ; \t loss: 0.4580\n",
      "Total trained pairs (M):     139.24 ; \t loss: 0.4557\n",
      "Total trained pairs (M):     139.76 ; \t loss: 0.4542\n",
      "Total trained pairs (M):     140.28 ; \t loss: 0.4536\n",
      "Total trained pairs (M):     140.78 ; \t loss: 0.4545\n",
      "Total trained pairs (M):     141.29 ; \t loss: 0.4530\n",
      "Total trained pairs (M):     141.80 ; \t loss: 0.4539\n",
      "Total trained pairs (M):     142.30 ; \t loss: 0.4552\n",
      "Total trained pairs (M):     142.81 ; \t loss: 0.4555\n",
      "Total trained pairs (M):     143.30 ; \t loss: 0.4535\n",
      "Total trained pairs (M):     143.79 ; \t loss: 0.4529\n",
      "Total trained pairs (M):     144.29 ; \t loss: 0.4518\n",
      "Total trained pairs (M):     144.79 ; \t loss: 0.4518\n",
      "Total trained pairs (M):     145.26 ; \t loss: 0.4569\n",
      "Total trained pairs (M):     145.75 ; \t loss: 0.4604\n",
      "Total trained pairs (M):     146.22 ; \t loss: 0.4621\n",
      "Total trained pairs (M):     146.67 ; \t loss: 0.4627\n",
      "Total trained pairs (M):     147.11 ; \t loss: 0.4617\n",
      "Total trained pairs (M):     147.56 ; \t loss: 0.4609\n",
      "Total trained pairs (M):     148.01 ; \t loss: 0.4590\n",
      "Total trained pairs (M):     148.47 ; \t loss: 0.4589\n",
      "Total trained pairs (M):     148.92 ; \t loss: 0.4585\n",
      "Total trained pairs (M):     149.37 ; \t loss: 0.4570\n",
      "Total trained pairs (M):     149.84 ; \t loss: 0.4572\n",
      "Total trained pairs (M):     150.29 ; \t loss: 0.4565\n",
      "Total trained pairs (M):     150.76 ; \t loss: 0.4571\n",
      "Total trained pairs (M):     151.22 ; \t loss: 0.4578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trained pairs (M):     151.69 ; \t loss: 0.4563\n",
      "Total trained pairs (M):     152.15 ; \t loss: 0.4558\n",
      "Total trained pairs (M):     152.61 ; \t loss: 0.4559\n",
      "Total trained pairs (M):     153.08 ; \t loss: 0.4573\n",
      "Total trained pairs (M):     153.55 ; \t loss: 0.4591\n",
      "Total trained pairs (M):     154.02 ; \t loss: 0.4589\n",
      "Total trained pairs (M):     154.50 ; \t loss: 0.4578\n",
      "Total trained pairs (M):     154.97 ; \t loss: 0.4582\n",
      "Total trained pairs (M):     155.44 ; \t loss: 0.4560\n",
      "Total trained pairs (M):     155.92 ; \t loss: 0.4556\n",
      "Total trained pairs (M):     156.39 ; \t loss: 0.4558\n",
      "Total trained pairs (M):     156.87 ; \t loss: 0.4566\n",
      "Total trained pairs (M):     157.35 ; \t loss: 0.4534\n",
      "Total trained pairs (M):     157.82 ; \t loss: 0.4514\n",
      "Total trained pairs (M):     158.30 ; \t loss: 0.4552\n",
      "Total trained pairs (M):     158.77 ; \t loss: 0.4550\n",
      "Total trained pairs (M):     159.25 ; \t loss: 0.4527\n",
      "Total trained pairs (M):     159.72 ; \t loss: 0.4550\n",
      "Total trained pairs (M):     160.19 ; \t loss: 0.4526\n",
      "Total trained pairs (M):     160.67 ; \t loss: 0.4516\n",
      "Total trained pairs (M):     161.15 ; \t loss: 0.4516\n",
      "Total trained pairs (M):     161.63 ; \t loss: 0.4525\n",
      "Total trained pairs (M):     162.10 ; \t loss: 0.4529\n",
      "Total trained pairs (M):     162.56 ; \t loss: 0.4523\n",
      "Total trained pairs (M):     163.03 ; \t loss: 0.4517\n",
      "Total trained pairs (M):     163.50 ; \t loss: 0.4583\n",
      "Total trained pairs (M):     163.97 ; \t loss: 0.4589\n",
      "Total trained pairs (M):     164.44 ; \t loss: 0.4572\n",
      "Total trained pairs (M):     164.91 ; \t loss: 0.4555\n",
      "Total trained pairs (M):     165.38 ; \t loss: 0.4576\n",
      "Total trained pairs (M):     165.85 ; \t loss: 0.4579\n",
      "Total trained pairs (M):     166.32 ; \t loss: 0.4554\n",
      "Total trained pairs (M):     166.79 ; \t loss: 0.4555\n",
      "Total trained pairs (M):     167.26 ; \t loss: 0.4549\n",
      "Total trained pairs (M):     167.73 ; \t loss: 0.4548\n",
      "Total trained pairs (M):     168.20 ; \t loss: 0.4557\n",
      "Total trained pairs (M):     168.68 ; \t loss: 0.4548\n",
      "Total trained pairs (M):     169.16 ; \t loss: 0.4545\n",
      "Total trained pairs (M):     169.64 ; \t loss: 0.4538\n",
      "Total trained pairs (M):     170.08 ; \t loss: 0.4556\n",
      "Total trained pairs (M):     170.54 ; \t loss: 0.4562\n",
      "Total trained pairs (M):     171.00 ; \t loss: 0.4608\n",
      "Total trained pairs (M):     171.46 ; \t loss: 0.4588\n",
      "Total trained pairs (M):     171.91 ; \t loss: 0.4601\n",
      "Total trained pairs (M):     172.36 ; \t loss: 0.4579\n",
      "Total trained pairs (M):     172.81 ; \t loss: 0.4563\n",
      "Total trained pairs (M):     173.27 ; \t loss: 0.4531\n",
      "Total trained pairs (M):     173.72 ; \t loss: 0.4512\n",
      "Total trained pairs (M):     174.18 ; \t loss: 0.4496\n",
      "Total trained pairs (M):     174.64 ; \t loss: 0.4489\n",
      "Total trained pairs (M):     175.09 ; \t loss: 0.4488\n",
      "Total trained pairs (M):     175.55 ; \t loss: 0.4501\n",
      "Total trained pairs (M):     176.01 ; \t loss: 0.4504\n",
      "Total trained pairs (M):     176.47 ; \t loss: 0.4494\n",
      "Total trained pairs (M):     176.93 ; \t loss: 0.4482\n",
      "Total trained pairs (M):     177.38 ; \t loss: 0.4515\n",
      "Total trained pairs (M):     177.82 ; \t loss: 0.4514\n",
      "Total trained pairs (M):     178.27 ; \t loss: 0.4502\n",
      "Total trained pairs (M):     178.72 ; \t loss: 0.4521\n",
      "Total trained pairs (M):     179.16 ; \t loss: 0.4538\n",
      "Total trained pairs (M):     179.59 ; \t loss: 0.4532\n",
      "Total trained pairs (M):     180.02 ; \t loss: 0.4528\n",
      "Total trained pairs (M):     180.45 ; \t loss: 0.4548\n",
      "Total trained pairs (M):     180.87 ; \t loss: 0.4540\n",
      "Total trained pairs (M):     181.27 ; \t loss: 0.4562\n",
      "Total trained pairs (M):     181.68 ; \t loss: 0.4545\n",
      "Total trained pairs (M):     182.09 ; \t loss: 0.4536\n",
      "Total trained pairs (M):     182.49 ; \t loss: 0.4563\n",
      "Total trained pairs (M):     182.90 ; \t loss: 0.4533\n",
      "Total trained pairs (M):     183.30 ; \t loss: 0.4526\n",
      "Total trained pairs (M):     183.71 ; \t loss: 0.4502\n",
      "Total trained pairs (M):     184.11 ; \t loss: 0.4489\n",
      "Total trained pairs (M):     184.52 ; \t loss: 0.4454\n",
      "Total trained pairs (M):     184.92 ; \t loss: 0.4490\n",
      "Total trained pairs (M):     185.33 ; \t loss: 0.4496\n",
      "Total trained pairs (M):     185.74 ; \t loss: 0.4477\n",
      "Total trained pairs (M):     186.15 ; \t loss: 0.4464\n",
      "Total trained pairs (M):     186.56 ; \t loss: 0.4462\n",
      "Total trained pairs (M):     186.97 ; \t loss: 0.4450\n",
      "Total trained pairs (M):     187.38 ; \t loss: 0.4490\n",
      "Total trained pairs (M):     187.79 ; \t loss: 0.4488\n",
      "Total trained pairs (M):     188.21 ; \t loss: 0.4538\n",
      "Total trained pairs (M):     188.63 ; \t loss: 0.4530\n",
      "Total trained pairs (M):     189.05 ; \t loss: 0.4520\n",
      "Total trained pairs (M):     189.46 ; \t loss: 0.4498\n",
      "Total trained pairs (M):     189.88 ; \t loss: 0.4518\n",
      "Total trained pairs (M):     190.32 ; \t loss: 0.4535\n",
      "Total trained pairs (M):     190.73 ; \t loss: 0.4540\n",
      "Total trained pairs (M):     191.16 ; \t loss: 0.4526\n",
      "Total trained pairs (M):     191.59 ; \t loss: 0.4517\n",
      "Total trained pairs (M):     192.01 ; \t loss: 0.4506\n",
      "Total trained pairs (M):     192.44 ; \t loss: 0.4504\n",
      "Total trained pairs (M):     192.89 ; \t loss: 0.4499\n",
      "Total trained pairs (M):     193.34 ; \t loss: 0.4531\n",
      "Total trained pairs (M):     193.79 ; \t loss: 0.4513\n",
      "Total trained pairs (M):     194.24 ; \t loss: 0.4490\n",
      "Total trained pairs (M):     194.68 ; \t loss: 0.4512\n",
      "Total trained pairs (M):     195.11 ; \t loss: 0.4502\n",
      "Total trained pairs (M):     195.56 ; \t loss: 0.4572\n",
      "Total trained pairs (M):     196.01 ; \t loss: 0.4614\n",
      "Total trained pairs (M):     196.46 ; \t loss: 0.4598\n",
      "Total trained pairs (M):     196.91 ; \t loss: 0.4584\n",
      "Total trained pairs (M):     197.35 ; \t loss: 0.4585\n",
      "Total trained pairs (M):     197.79 ; \t loss: 0.4576\n",
      "Total trained pairs (M):     198.22 ; \t loss: 0.4589\n",
      "Total trained pairs (M):     198.65 ; \t loss: 0.4589\n",
      "Total trained pairs (M):     199.08 ; \t loss: 0.4586\n",
      "Total trained pairs (M):     199.50 ; \t loss: 0.4587\n",
      "Total trained pairs (M):     199.93 ; \t loss: 0.4585\n",
      "Total trained pairs (M):     200.36 ; \t loss: 0.4580\n",
      "Total trained pairs (M):     200.80 ; \t loss: 0.4633\n",
      "Total trained pairs (M):     201.24 ; \t loss: 0.4637\n",
      "Total trained pairs (M):     201.69 ; \t loss: 0.4633\n",
      "Total trained pairs (M):     202.14 ; \t loss: 0.4622\n",
      "Epoch 1 ======\n",
      "Total trained pairs (M):     202.66 ; \t loss: 0.5665\n",
      "Total trained pairs (M):     203.18 ; \t loss: 0.5592\n",
      "Total trained pairs (M):     203.71 ; \t loss: 0.5503\n",
      "Total trained pairs (M):     204.23 ; \t loss: 0.5425\n",
      "Total trained pairs (M):     204.74 ; \t loss: 0.5386\n",
      "Total trained pairs (M):     205.25 ; \t loss: 0.5337\n",
      "Total trained pairs (M):     205.76 ; \t loss: 0.5301\n",
      "Total trained pairs (M):     206.27 ; \t loss: 0.5277\n",
      "Total trained pairs (M):     206.77 ; \t loss: 0.5218\n",
      "Total trained pairs (M):     207.27 ; \t loss: 0.5174\n",
      "Total trained pairs (M):     207.77 ; \t loss: 0.5155\n",
      "Total trained pairs (M):     208.26 ; \t loss: 0.5106\n",
      "Total trained pairs (M):     208.76 ; \t loss: 0.5080\n",
      "Total trained pairs (M):     209.27 ; \t loss: 0.5058\n",
      "Total trained pairs (M):     209.76 ; \t loss: 0.5017\n",
      "Total trained pairs (M):     210.24 ; \t loss: 0.5016\n",
      "Total trained pairs (M):     210.72 ; \t loss: 0.4990\n",
      "Total trained pairs (M):     211.20 ; \t loss: 0.4964\n",
      "Total trained pairs (M):     211.69 ; \t loss: 0.4952\n",
      "Total trained pairs (M):     212.17 ; \t loss: 0.4923\n",
      "Total trained pairs (M):     212.64 ; \t loss: 0.4879\n",
      "Total trained pairs (M):     213.12 ; \t loss: 0.4871\n",
      "Total trained pairs (M):     213.60 ; \t loss: 0.4835\n",
      "Total trained pairs (M):     214.07 ; \t loss: 0.4827\n",
      "Total trained pairs (M):     214.55 ; \t loss: 0.4795\n",
      "Total trained pairs (M):     215.04 ; \t loss: 0.4757\n",
      "Total trained pairs (M):     215.52 ; \t loss: 0.4742\n",
      "Total trained pairs (M):     215.99 ; \t loss: 0.4663\n",
      "Total trained pairs (M):     216.46 ; \t loss: 0.4646\n",
      "Total trained pairs (M):     216.93 ; \t loss: 0.4641\n",
      "Total trained pairs (M):     217.40 ; \t loss: 0.4612\n",
      "Total trained pairs (M):     217.88 ; \t loss: 0.4617\n",
      "Total trained pairs (M):     218.35 ; \t loss: 0.4591\n",
      "Total trained pairs (M):     218.84 ; \t loss: 0.4595\n",
      "Total trained pairs (M):     219.33 ; \t loss: 0.4567\n",
      "Total trained pairs (M):     219.82 ; \t loss: 0.4566\n",
      "Total trained pairs (M):     220.29 ; \t loss: 0.4548\n",
      "Total trained pairs (M):     220.77 ; \t loss: 0.4532\n",
      "Total trained pairs (M):     221.24 ; \t loss: 0.4517\n",
      "Total trained pairs (M):     221.72 ; \t loss: 0.4490\n",
      "Total trained pairs (M):     222.20 ; \t loss: 0.4466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trained pairs (M):     222.68 ; \t loss: 0.4402\n",
      "Total trained pairs (M):     223.16 ; \t loss: 0.4395\n",
      "Total trained pairs (M):     223.64 ; \t loss: 0.4371\n",
      "Total trained pairs (M):     224.12 ; \t loss: 0.4356\n",
      "Total trained pairs (M):     224.61 ; \t loss: 0.4351\n",
      "Total trained pairs (M):     225.09 ; \t loss: 0.4332\n",
      "Total trained pairs (M):     225.57 ; \t loss: 0.4325\n",
      "Total trained pairs (M):     226.05 ; \t loss: 0.4309\n",
      "Total trained pairs (M):     226.53 ; \t loss: 0.4278\n",
      "Total trained pairs (M):     227.02 ; \t loss: 0.4282\n",
      "Total trained pairs (M):     227.50 ; \t loss: 0.4275\n",
      "Total trained pairs (M):     227.99 ; \t loss: 0.4247\n",
      "Total trained pairs (M):     228.48 ; \t loss: 0.4246\n",
      "Total trained pairs (M):     228.97 ; \t loss: 0.4204\n",
      "Total trained pairs (M):     229.46 ; \t loss: 0.4202\n",
      "Total trained pairs (M):     229.98 ; \t loss: 0.4226\n",
      "Total trained pairs (M):     230.51 ; \t loss: 0.4242\n",
      "Total trained pairs (M):     231.04 ; \t loss: 0.4237\n",
      "Total trained pairs (M):     231.56 ; \t loss: 0.4223\n",
      "Total trained pairs (M):     232.07 ; \t loss: 0.4218\n",
      "Total trained pairs (M):     232.58 ; \t loss: 0.4237\n",
      "Total trained pairs (M):     233.09 ; \t loss: 0.4198\n",
      "Total trained pairs (M):     233.60 ; \t loss: 0.4185\n",
      "Total trained pairs (M):     234.10 ; \t loss: 0.4155\n",
      "Total trained pairs (M):     234.58 ; \t loss: 0.4144\n",
      "Total trained pairs (M):     235.07 ; \t loss: 0.4115\n",
      "Total trained pairs (M):     235.55 ; \t loss: 0.4112\n",
      "Total trained pairs (M):     236.04 ; \t loss: 0.4102\n",
      "Total trained pairs (M):     236.53 ; \t loss: 0.4106\n",
      "Total trained pairs (M):     237.02 ; \t loss: 0.4085\n",
      "Total trained pairs (M):     237.52 ; \t loss: 0.4120\n",
      "Total trained pairs (M):     238.01 ; \t loss: 0.4113\n",
      "Total trained pairs (M):     238.51 ; \t loss: 0.4098\n",
      "Total trained pairs (M):     239.00 ; \t loss: 0.4089\n",
      "Total trained pairs (M):     239.50 ; \t loss: 0.4104\n",
      "Total trained pairs (M):     240.00 ; \t loss: 0.4115\n",
      "Total trained pairs (M):     240.49 ; \t loss: 0.4113\n",
      "Total trained pairs (M):     240.99 ; \t loss: 0.4136\n",
      "Total trained pairs (M):     241.49 ; \t loss: 0.4155\n",
      "Total trained pairs (M):     242.00 ; \t loss: 0.4148\n",
      "Total trained pairs (M):     242.52 ; \t loss: 0.4139\n",
      "Total trained pairs (M):     243.04 ; \t loss: 0.4152\n",
      "Total trained pairs (M):     243.56 ; \t loss: 0.4132\n",
      "Total trained pairs (M):     244.08 ; \t loss: 0.4136\n",
      "Total trained pairs (M):     244.59 ; \t loss: 0.4120\n",
      "Total trained pairs (M):     245.12 ; \t loss: 0.4183\n",
      "Total trained pairs (M):     245.66 ; \t loss: 0.4197\n",
      "Total trained pairs (M):     246.19 ; \t loss: 0.4187\n",
      "Total trained pairs (M):     246.73 ; \t loss: 0.4170\n",
      "Total trained pairs (M):     247.28 ; \t loss: 0.4157\n",
      "Total trained pairs (M):     247.83 ; \t loss: 0.4176\n",
      "Total trained pairs (M):     248.38 ; \t loss: 0.4169\n",
      "Total trained pairs (M):     248.94 ; \t loss: 0.4162\n",
      "Total trained pairs (M):     249.49 ; \t loss: 0.4167\n",
      "Total trained pairs (M):     250.05 ; \t loss: 0.4162\n",
      "Total trained pairs (M):     250.61 ; \t loss: 0.4166\n",
      "Total trained pairs (M):     251.17 ; \t loss: 0.4145\n",
      "Total trained pairs (M):     251.72 ; \t loss: 0.4148\n",
      "Total trained pairs (M):     252.24 ; \t loss: 0.4191\n",
      "Total trained pairs (M):     252.75 ; \t loss: 0.4171\n",
      "Total trained pairs (M):     253.27 ; \t loss: 0.4208\n",
      "Total trained pairs (M):     253.79 ; \t loss: 0.4214\n",
      "Total trained pairs (M):     254.30 ; \t loss: 0.4209\n",
      "Total trained pairs (M):     254.81 ; \t loss: 0.4194\n",
      "Total trained pairs (M):     255.33 ; \t loss: 0.4190\n",
      "Total trained pairs (M):     255.83 ; \t loss: 0.4206\n",
      "Total trained pairs (M):     256.34 ; \t loss: 0.4192\n",
      "Total trained pairs (M):     256.85 ; \t loss: 0.4189\n",
      "Total trained pairs (M):     257.36 ; \t loss: 0.4196\n",
      "Total trained pairs (M):     257.87 ; \t loss: 0.4213\n",
      "Total trained pairs (M):     258.38 ; \t loss: 0.4199\n",
      "Total trained pairs (M):     258.88 ; \t loss: 0.4185\n",
      "Total trained pairs (M):     259.40 ; \t loss: 0.4219\n",
      "Total trained pairs (M):     259.91 ; \t loss: 0.4198\n",
      "Total trained pairs (M):     260.43 ; \t loss: 0.4215\n",
      "Total trained pairs (M):     260.95 ; \t loss: 0.4226\n",
      "Total trained pairs (M):     261.47 ; \t loss: 0.4220\n",
      "Total trained pairs (M):     261.99 ; \t loss: 0.4219\n",
      "Total trained pairs (M):     262.53 ; \t loss: 0.4227\n",
      "Total trained pairs (M):     263.05 ; \t loss: 0.4221\n",
      "Total trained pairs (M):     263.58 ; \t loss: 0.4201\n",
      "Total trained pairs (M):     264.11 ; \t loss: 0.4182\n",
      "Total trained pairs (M):     264.64 ; \t loss: 0.4194\n",
      "Total trained pairs (M):     265.17 ; \t loss: 0.4180\n",
      "Total trained pairs (M):     265.70 ; \t loss: 0.4229\n",
      "Total trained pairs (M):     266.22 ; \t loss: 0.4213\n",
      "Total trained pairs (M):     266.74 ; \t loss: 0.4204\n",
      "Total trained pairs (M):     267.26 ; \t loss: 0.4189\n",
      "Total trained pairs (M):     267.79 ; \t loss: 0.4215\n",
      "Total trained pairs (M):     268.31 ; \t loss: 0.4211\n",
      "Total trained pairs (M):     268.83 ; \t loss: 0.4210\n",
      "Total trained pairs (M):     269.35 ; \t loss: 0.4210\n",
      "Total trained pairs (M):     269.87 ; \t loss: 0.4204\n",
      "Total trained pairs (M):     270.38 ; \t loss: 0.4216\n",
      "Total trained pairs (M):     270.89 ; \t loss: 0.4197\n",
      "Total trained pairs (M):     271.40 ; \t loss: 0.4194\n",
      "Total trained pairs (M):     271.93 ; \t loss: 0.4212\n",
      "Total trained pairs (M):     272.45 ; \t loss: 0.4190\n",
      "Total trained pairs (M):     272.98 ; \t loss: 0.4173\n",
      "Total trained pairs (M):     273.51 ; \t loss: 0.4217\n",
      "Total trained pairs (M):     274.03 ; \t loss: 0.4206\n",
      "Total trained pairs (M):     274.57 ; \t loss: 0.4258\n",
      "Total trained pairs (M):     275.10 ; \t loss: 0.4264\n",
      "Total trained pairs (M):     275.63 ; \t loss: 0.4285\n",
      "Total trained pairs (M):     276.17 ; \t loss: 0.4261\n",
      "Total trained pairs (M):     276.70 ; \t loss: 0.4258\n",
      "Total trained pairs (M):     277.24 ; \t loss: 0.4246\n",
      "Total trained pairs (M):     277.78 ; \t loss: 0.4219\n",
      "Total trained pairs (M):     278.31 ; \t loss: 0.4206\n",
      "Total trained pairs (M):     278.84 ; \t loss: 0.4229\n",
      "Total trained pairs (M):     279.36 ; \t loss: 0.4218\n",
      "Total trained pairs (M):     279.90 ; \t loss: 0.4209\n",
      "Total trained pairs (M):     280.43 ; \t loss: 0.4202\n",
      "Total trained pairs (M):     280.96 ; \t loss: 0.4197\n",
      "Total trained pairs (M):     281.48 ; \t loss: 0.4214\n",
      "Total trained pairs (M):     281.98 ; \t loss: 0.4251\n",
      "Total trained pairs (M):     282.47 ; \t loss: 0.4246\n",
      "Total trained pairs (M):     282.95 ; \t loss: 0.4232\n",
      "Total trained pairs (M):     283.45 ; \t loss: 0.4221\n",
      "Total trained pairs (M):     283.93 ; \t loss: 0.4216\n",
      "Total trained pairs (M):     284.42 ; \t loss: 0.4201\n",
      "Total trained pairs (M):     284.91 ; \t loss: 0.4201\n",
      "Total trained pairs (M):     285.42 ; \t loss: 0.4223\n",
      "Total trained pairs (M):     285.94 ; \t loss: 0.4218\n",
      "Total trained pairs (M):     286.45 ; \t loss: 0.4194\n",
      "Total trained pairs (M):     286.97 ; \t loss: 0.4216\n",
      "Total trained pairs (M):     287.49 ; \t loss: 0.4211\n",
      "Total trained pairs (M):     288.01 ; \t loss: 0.4192\n",
      "Total trained pairs (M):     288.52 ; \t loss: 0.4193\n",
      "Total trained pairs (M):     289.04 ; \t loss: 0.4179\n",
      "Total trained pairs (M):     289.54 ; \t loss: 0.4217\n",
      "Total trained pairs (M):     290.05 ; \t loss: 0.4192\n",
      "Total trained pairs (M):     290.55 ; \t loss: 0.4188\n",
      "Total trained pairs (M):     291.06 ; \t loss: 0.4170\n",
      "Total trained pairs (M):     291.55 ; \t loss: 0.4176\n",
      "Total trained pairs (M):     292.03 ; \t loss: 0.4187\n",
      "Total trained pairs (M):     292.51 ; \t loss: 0.4178\n",
      "Total trained pairs (M):     292.98 ; \t loss: 0.4171\n",
      "Total trained pairs (M):     293.44 ; \t loss: 0.4208\n",
      "Total trained pairs (M):     293.90 ; \t loss: 0.4194\n",
      "Total trained pairs (M):     294.36 ; \t loss: 0.4177\n",
      "Total trained pairs (M):     294.80 ; \t loss: 0.4190\n",
      "Total trained pairs (M):     295.24 ; \t loss: 0.4194\n",
      "Total trained pairs (M):     295.68 ; \t loss: 0.4192\n",
      "Total trained pairs (M):     296.13 ; \t loss: 0.4183\n",
      "Total trained pairs (M):     296.55 ; \t loss: 0.4199\n",
      "Total trained pairs (M):     296.97 ; \t loss: 0.4228\n",
      "Total trained pairs (M):     297.40 ; \t loss: 0.4198\n",
      "Total trained pairs (M):     297.82 ; \t loss: 0.4203\n",
      "Total trained pairs (M):     298.25 ; \t loss: 0.4268\n",
      "Total trained pairs (M):     298.66 ; \t loss: 0.4273\n",
      "Total trained pairs (M):     299.09 ; \t loss: 0.4297\n",
      "Total trained pairs (M):     299.50 ; \t loss: 0.4306\n",
      "Total trained pairs (M):     299.92 ; \t loss: 0.4304\n",
      "Total trained pairs (M):     300.32 ; \t loss: 0.4336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trained pairs (M):     300.74 ; \t loss: 0.4347\n",
      "Total trained pairs (M):     301.16 ; \t loss: 0.4351\n",
      "Total trained pairs (M):     301.60 ; \t loss: 0.4424\n",
      "Total trained pairs (M):     302.06 ; \t loss: 0.4478\n",
      "Total trained pairs (M):     302.52 ; \t loss: 0.4499\n",
      "Total trained pairs (M):     302.96 ; \t loss: 0.4482\n",
      "Total trained pairs (M):     303.41 ; \t loss: 0.4465\n",
      "Total trained pairs (M):     303.86 ; \t loss: 0.4458\n",
      "Total trained pairs (M):     304.30 ; \t loss: 0.4446\n",
      "Total trained pairs (M):     304.75 ; \t loss: 0.4450\n",
      "Total trained pairs (M):     305.19 ; \t loss: 0.4436\n",
      "Total trained pairs (M):     305.64 ; \t loss: 0.4435\n",
      "Total trained pairs (M):     306.08 ; \t loss: 0.4416\n",
      "Total trained pairs (M):     306.53 ; \t loss: 0.4429\n",
      "Total trained pairs (M):     306.98 ; \t loss: 0.4430\n",
      "Total trained pairs (M):     307.43 ; \t loss: 0.4423\n",
      "Total trained pairs (M):     307.88 ; \t loss: 0.4412\n",
      "Total trained pairs (M):     308.33 ; \t loss: 0.4436\n",
      "Total trained pairs (M):     308.78 ; \t loss: 0.4425\n",
      "Total trained pairs (M):     309.22 ; \t loss: 0.4422\n",
      "Total trained pairs (M):     309.66 ; \t loss: 0.4428\n",
      "Total trained pairs (M):     310.10 ; \t loss: 0.4412\n",
      "Total trained pairs (M):     310.54 ; \t loss: 0.4433\n",
      "Total trained pairs (M):     310.98 ; \t loss: 0.4447\n",
      "Total trained pairs (M):     311.42 ; \t loss: 0.4422\n",
      "Total trained pairs (M):     311.86 ; \t loss: 0.4393\n",
      "Total trained pairs (M):     312.30 ; \t loss: 0.4396\n",
      "Total trained pairs (M):     312.74 ; \t loss: 0.4381\n",
      "Total trained pairs (M):     313.18 ; \t loss: 0.4419\n",
      "Total trained pairs (M):     313.63 ; \t loss: 0.4403\n",
      "Total trained pairs (M):     314.07 ; \t loss: 0.4415\n",
      "Total trained pairs (M):     314.52 ; \t loss: 0.4403\n",
      "Total trained pairs (M):     314.96 ; \t loss: 0.4387\n",
      "Total trained pairs (M):     315.41 ; \t loss: 0.4432\n",
      "Total trained pairs (M):     315.85 ; \t loss: 0.4425\n",
      "Total trained pairs (M):     316.29 ; \t loss: 0.4405\n",
      "Total trained pairs (M):     316.75 ; \t loss: 0.4414\n",
      "Total trained pairs (M):     317.20 ; \t loss: 0.4443\n",
      "Total trained pairs (M):     317.66 ; \t loss: 0.4435\n",
      "Total trained pairs (M):     318.12 ; \t loss: 0.4440\n",
      "Total trained pairs (M):     318.59 ; \t loss: 0.4464\n",
      "Total trained pairs (M):     319.04 ; \t loss: 0.4478\n",
      "Total trained pairs (M):     319.50 ; \t loss: 0.4459\n",
      "Total trained pairs (M):     319.96 ; \t loss: 0.4443\n",
      "Total trained pairs (M):     320.41 ; \t loss: 0.4455\n",
      "Total trained pairs (M):     320.86 ; \t loss: 0.4434\n",
      "Total trained pairs (M):     321.30 ; \t loss: 0.4482\n",
      "Total trained pairs (M):     321.74 ; \t loss: 0.4502\n",
      "Total trained pairs (M):     322.17 ; \t loss: 0.4494\n",
      "Total trained pairs (M):     322.61 ; \t loss: 0.4486\n",
      "Total trained pairs (M):     323.04 ; \t loss: 0.4478\n",
      "Total trained pairs (M):     323.48 ; \t loss: 0.4486\n",
      "Total trained pairs (M):     323.92 ; \t loss: 0.4479\n",
      "Total trained pairs (M):     324.40 ; \t loss: 0.4616\n",
      "Total trained pairs (M):     324.87 ; \t loss: 0.4601\n",
      "Total trained pairs (M):     325.34 ; \t loss: 0.4577\n",
      "Total trained pairs (M):     325.81 ; \t loss: 0.4593\n",
      "Total trained pairs (M):     326.29 ; \t loss: 0.4604\n",
      "Total trained pairs (M):     326.77 ; \t loss: 0.4580\n",
      "Total trained pairs (M):     327.25 ; \t loss: 0.4572\n",
      "Total trained pairs (M):     327.72 ; \t loss: 0.4571\n",
      "Total trained pairs (M):     328.19 ; \t loss: 0.4560\n",
      "Total trained pairs (M):     328.67 ; \t loss: 0.4560\n",
      "Total trained pairs (M):     329.14 ; \t loss: 0.4532\n",
      "Total trained pairs (M):     329.62 ; \t loss: 0.4526\n",
      "Total trained pairs (M):     330.10 ; \t loss: 0.4514\n",
      "Total trained pairs (M):     330.57 ; \t loss: 0.4545\n",
      "Total trained pairs (M):     331.03 ; \t loss: 0.4560\n",
      "Total trained pairs (M):     331.48 ; \t loss: 0.4530\n",
      "Total trained pairs (M):     331.95 ; \t loss: 0.4496\n",
      "Total trained pairs (M):     332.41 ; \t loss: 0.4521\n",
      "Total trained pairs (M):     332.88 ; \t loss: 0.4533\n",
      "Total trained pairs (M):     333.35 ; \t loss: 0.4524\n",
      "Total trained pairs (M):     333.82 ; \t loss: 0.4512\n",
      "Total trained pairs (M):     334.30 ; \t loss: 0.4512\n",
      "Total trained pairs (M):     334.78 ; \t loss: 0.4511\n",
      "Total trained pairs (M):     335.27 ; \t loss: 0.4495\n",
      "Total trained pairs (M):     335.76 ; \t loss: 0.4493\n",
      "Total trained pairs (M):     336.27 ; \t loss: 0.4497\n",
      "Total trained pairs (M):     336.77 ; \t loss: 0.4480\n",
      "Total trained pairs (M):     337.28 ; \t loss: 0.4492\n",
      "Total trained pairs (M):     337.80 ; \t loss: 0.4482\n",
      "Total trained pairs (M):     338.31 ; \t loss: 0.4480\n",
      "Total trained pairs (M):     338.82 ; \t loss: 0.4465\n",
      "Total trained pairs (M):     339.33 ; \t loss: 0.4465\n",
      "Total trained pairs (M):     339.84 ; \t loss: 0.4442\n",
      "Total trained pairs (M):     340.35 ; \t loss: 0.4429\n",
      "Total trained pairs (M):     340.87 ; \t loss: 0.4428\n",
      "Total trained pairs (M):     341.38 ; \t loss: 0.4392\n",
      "Total trained pairs (M):     341.90 ; \t loss: 0.4374\n",
      "Total trained pairs (M):     342.42 ; \t loss: 0.4374\n",
      "Total trained pairs (M):     342.92 ; \t loss: 0.4378\n",
      "Total trained pairs (M):     343.43 ; \t loss: 0.4369\n",
      "Total trained pairs (M):     343.95 ; \t loss: 0.4374\n",
      "Total trained pairs (M):     344.45 ; \t loss: 0.4389\n",
      "Total trained pairs (M):     344.95 ; \t loss: 0.4372\n",
      "Total trained pairs (M):     345.44 ; \t loss: 0.4366\n",
      "Total trained pairs (M):     345.94 ; \t loss: 0.4354\n",
      "Total trained pairs (M):     346.43 ; \t loss: 0.4359\n",
      "Total trained pairs (M):     346.93 ; \t loss: 0.4332\n",
      "Total trained pairs (M):     347.41 ; \t loss: 0.4395\n",
      "Total trained pairs (M):     347.89 ; \t loss: 0.4421\n",
      "Total trained pairs (M):     348.36 ; \t loss: 0.4431\n",
      "Total trained pairs (M):     348.81 ; \t loss: 0.4428\n",
      "Total trained pairs (M):     349.26 ; \t loss: 0.4428\n",
      "Total trained pairs (M):     349.70 ; \t loss: 0.4404\n",
      "Total trained pairs (M):     350.15 ; \t loss: 0.4391\n",
      "Total trained pairs (M):     350.61 ; \t loss: 0.4388\n",
      "Total trained pairs (M):     351.06 ; \t loss: 0.4385\n",
      "Total trained pairs (M):     351.51 ; \t loss: 0.4368\n",
      "Total trained pairs (M):     351.98 ; \t loss: 0.4377\n",
      "Total trained pairs (M):     352.44 ; \t loss: 0.4377\n",
      "Total trained pairs (M):     352.90 ; \t loss: 0.4365\n",
      "Total trained pairs (M):     353.37 ; \t loss: 0.4369\n",
      "Total trained pairs (M):     353.83 ; \t loss: 0.4368\n",
      "Total trained pairs (M):     354.29 ; \t loss: 0.4352\n",
      "Total trained pairs (M):     354.76 ; \t loss: 0.4368\n",
      "Total trained pairs (M):     355.22 ; \t loss: 0.4351\n",
      "Total trained pairs (M):     355.69 ; \t loss: 0.4386\n",
      "Total trained pairs (M):     356.16 ; \t loss: 0.4370\n",
      "Total trained pairs (M):     356.64 ; \t loss: 0.4365\n",
      "Total trained pairs (M):     357.12 ; \t loss: 0.4364\n",
      "Total trained pairs (M):     357.59 ; \t loss: 0.4353\n",
      "Total trained pairs (M):     358.06 ; \t loss: 0.4346\n",
      "Total trained pairs (M):     358.53 ; \t loss: 0.4349\n",
      "Total trained pairs (M):     359.01 ; \t loss: 0.4344\n",
      "Total trained pairs (M):     359.49 ; \t loss: 0.4331\n",
      "Total trained pairs (M):     359.97 ; \t loss: 0.4305\n",
      "Total trained pairs (M):     360.44 ; \t loss: 0.4347\n",
      "Total trained pairs (M):     360.92 ; \t loss: 0.4337\n",
      "Total trained pairs (M):     361.39 ; \t loss: 0.4317\n",
      "Total trained pairs (M):     361.86 ; \t loss: 0.4337\n",
      "Total trained pairs (M):     362.33 ; \t loss: 0.4306\n",
      "Total trained pairs (M):     362.81 ; \t loss: 0.4296\n",
      "Total trained pairs (M):     363.29 ; \t loss: 0.4295\n",
      "Total trained pairs (M):     363.77 ; \t loss: 0.4309\n",
      "Total trained pairs (M):     364.24 ; \t loss: 0.4306\n",
      "Total trained pairs (M):     364.71 ; \t loss: 0.4302\n",
      "Total trained pairs (M):     365.17 ; \t loss: 0.4299\n",
      "Total trained pairs (M):     365.64 ; \t loss: 0.4352\n",
      "Total trained pairs (M):     366.11 ; \t loss: 0.4365\n",
      "Total trained pairs (M):     366.58 ; \t loss: 0.4335\n",
      "Total trained pairs (M):     367.05 ; \t loss: 0.4334\n",
      "Total trained pairs (M):     367.52 ; \t loss: 0.4343\n",
      "Total trained pairs (M):     367.99 ; \t loss: 0.4356\n",
      "Total trained pairs (M):     368.46 ; \t loss: 0.4330\n",
      "Total trained pairs (M):     368.93 ; \t loss: 0.4339\n",
      "Total trained pairs (M):     369.40 ; \t loss: 0.4331\n",
      "Total trained pairs (M):     369.87 ; \t loss: 0.4332\n",
      "Total trained pairs (M):     370.34 ; \t loss: 0.4348\n",
      "Total trained pairs (M):     370.82 ; \t loss: 0.4328\n",
      "Total trained pairs (M):     371.30 ; \t loss: 0.4333\n",
      "Total trained pairs (M):     371.78 ; \t loss: 0.4323\n",
      "Total trained pairs (M):     372.23 ; \t loss: 0.4334\n",
      "Total trained pairs (M):     372.68 ; \t loss: 0.4357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trained pairs (M):     373.14 ; \t loss: 0.4400\n",
      "Total trained pairs (M):     373.60 ; \t loss: 0.4384\n",
      "Total trained pairs (M):     374.05 ; \t loss: 0.4388\n",
      "Total trained pairs (M):     374.50 ; \t loss: 0.4356\n",
      "Total trained pairs (M):     374.95 ; \t loss: 0.4343\n",
      "Total trained pairs (M):     375.41 ; \t loss: 0.4328\n",
      "Total trained pairs (M):     375.87 ; \t loss: 0.4312\n",
      "Total trained pairs (M):     376.33 ; \t loss: 0.4289\n",
      "Total trained pairs (M):     376.78 ; \t loss: 0.4299\n",
      "Total trained pairs (M):     377.23 ; \t loss: 0.4295\n",
      "Total trained pairs (M):     377.69 ; \t loss: 0.4313\n",
      "Total trained pairs (M):     378.15 ; \t loss: 0.4303\n",
      "Total trained pairs (M):     378.61 ; \t loss: 0.4295\n",
      "Total trained pairs (M):     379.07 ; \t loss: 0.4281\n",
      "Total trained pairs (M):     379.52 ; \t loss: 0.4317\n",
      "Total trained pairs (M):     379.97 ; \t loss: 0.4336\n",
      "Total trained pairs (M):     380.41 ; \t loss: 0.4304\n",
      "Total trained pairs (M):     380.86 ; \t loss: 0.4342\n",
      "Total trained pairs (M):     381.30 ; \t loss: 0.4352\n",
      "Total trained pairs (M):     381.74 ; \t loss: 0.4342\n",
      "Total trained pairs (M):     382.17 ; \t loss: 0.4343\n",
      "Total trained pairs (M):     382.59 ; \t loss: 0.4363\n",
      "Total trained pairs (M):     383.01 ; \t loss: 0.4368\n",
      "Total trained pairs (M):     383.42 ; \t loss: 0.4371\n",
      "Total trained pairs (M):     383.83 ; \t loss: 0.4368\n",
      "Total trained pairs (M):     384.23 ; \t loss: 0.4354\n",
      "Total trained pairs (M):     384.63 ; \t loss: 0.4385\n",
      "Total trained pairs (M):     385.04 ; \t loss: 0.4370\n",
      "Total trained pairs (M):     385.45 ; \t loss: 0.4353\n",
      "Total trained pairs (M):     385.85 ; \t loss: 0.4328\n",
      "Total trained pairs (M):     386.25 ; \t loss: 0.4317\n",
      "Total trained pairs (M):     386.66 ; \t loss: 0.4285\n",
      "Total trained pairs (M):     387.06 ; \t loss: 0.4326\n",
      "Total trained pairs (M):     387.47 ; \t loss: 0.4328\n",
      "Total trained pairs (M):     387.88 ; \t loss: 0.4326\n",
      "Total trained pairs (M):     388.29 ; \t loss: 0.4295\n",
      "Total trained pairs (M):     388.70 ; \t loss: 0.4296\n",
      "Total trained pairs (M):     389.11 ; \t loss: 0.4283\n",
      "Total trained pairs (M):     389.52 ; \t loss: 0.4334\n",
      "Total trained pairs (M):     389.93 ; \t loss: 0.4330\n",
      "Total trained pairs (M):     390.35 ; \t loss: 0.4390\n",
      "Total trained pairs (M):     390.77 ; \t loss: 0.4383\n",
      "Total trained pairs (M):     391.19 ; \t loss: 0.4368\n",
      "Total trained pairs (M):     391.60 ; \t loss: 0.4364\n",
      "Total trained pairs (M):     392.03 ; \t loss: 0.4379\n",
      "Total trained pairs (M):     392.46 ; \t loss: 0.4411\n",
      "Total trained pairs (M):     392.88 ; \t loss: 0.4425\n",
      "Total trained pairs (M):     393.30 ; \t loss: 0.4409\n",
      "Total trained pairs (M):     393.73 ; \t loss: 0.4385\n",
      "Total trained pairs (M):     394.15 ; \t loss: 0.4380\n",
      "Total trained pairs (M):     394.58 ; \t loss: 0.4387\n",
      "Total trained pairs (M):     395.03 ; \t loss: 0.4388\n",
      "Total trained pairs (M):     395.48 ; \t loss: 0.4423\n",
      "Total trained pairs (M):     395.93 ; \t loss: 0.4395\n",
      "Total trained pairs (M):     396.38 ; \t loss: 0.4382\n",
      "Total trained pairs (M):     396.82 ; \t loss: 0.4398\n",
      "Total trained pairs (M):     397.26 ; \t loss: 0.4382\n",
      "Total trained pairs (M):     397.71 ; \t loss: 0.4474\n",
      "Total trained pairs (M):     398.15 ; \t loss: 0.4521\n",
      "Total trained pairs (M):     398.60 ; \t loss: 0.4509\n",
      "Total trained pairs (M):     399.05 ; \t loss: 0.4493\n",
      "Total trained pairs (M):     399.49 ; \t loss: 0.4499\n",
      "Total trained pairs (M):     399.93 ; \t loss: 0.4485\n",
      "Total trained pairs (M):     400.36 ; \t loss: 0.4510\n",
      "Total trained pairs (M):     400.79 ; \t loss: 0.4508\n",
      "Total trained pairs (M):     401.22 ; \t loss: 0.4503\n",
      "Total trained pairs (M):     401.64 ; \t loss: 0.4505\n",
      "Total trained pairs (M):     402.07 ; \t loss: 0.4512\n",
      "Total trained pairs (M):     402.50 ; \t loss: 0.4512\n",
      "Total trained pairs (M):     402.94 ; \t loss: 0.4571\n",
      "Total trained pairs (M):     403.38 ; \t loss: 0.4575\n",
      "Total trained pairs (M):     403.83 ; \t loss: 0.4567\n",
      "Total trained pairs (M):     404.28 ; \t loss: 0.4560\n",
      "Epoch 2 ======\n",
      "Total trained pairs (M):     404.80 ; \t loss: 0.5877\n",
      "Total trained pairs (M):     405.32 ; \t loss: 0.5803\n",
      "Total trained pairs (M):     405.85 ; \t loss: 0.5702\n",
      "Total trained pairs (M):     406.37 ; \t loss: 0.5642\n",
      "Total trained pairs (M):     406.88 ; \t loss: 0.5594\n",
      "Total trained pairs (M):     407.39 ; \t loss: 0.5529\n",
      "Total trained pairs (M):     407.90 ; \t loss: 0.5513\n",
      "Total trained pairs (M):     408.41 ; \t loss: 0.5461\n",
      "Total trained pairs (M):     408.91 ; \t loss: 0.5427\n",
      "Total trained pairs (M):     409.41 ; \t loss: 0.5363\n",
      "Total trained pairs (M):     409.91 ; \t loss: 0.5341\n",
      "Total trained pairs (M):     410.41 ; \t loss: 0.5294\n",
      "Total trained pairs (M):     410.91 ; \t loss: 0.5269\n",
      "Total trained pairs (M):     411.41 ; \t loss: 0.5233\n",
      "Total trained pairs (M):     411.90 ; \t loss: 0.5190\n",
      "Total trained pairs (M):     412.38 ; \t loss: 0.5191\n",
      "Total trained pairs (M):     412.87 ; \t loss: 0.5180\n",
      "Total trained pairs (M):     413.35 ; \t loss: 0.5148\n",
      "Total trained pairs (M):     413.83 ; \t loss: 0.5117\n",
      "Total trained pairs (M):     414.31 ; \t loss: 0.5089\n",
      "Total trained pairs (M):     414.79 ; \t loss: 0.5054\n",
      "Total trained pairs (M):     415.27 ; \t loss: 0.5037\n",
      "Total trained pairs (M):     415.74 ; \t loss: 0.5006\n",
      "Total trained pairs (M):     416.22 ; \t loss: 0.4996\n",
      "Total trained pairs (M):     416.69 ; \t loss: 0.4965\n",
      "Total trained pairs (M):     417.18 ; \t loss: 0.4927\n",
      "Total trained pairs (M):     417.67 ; \t loss: 0.4901\n",
      "Total trained pairs (M):     418.13 ; \t loss: 0.4830\n",
      "Total trained pairs (M):     418.60 ; \t loss: 0.4817\n",
      "Total trained pairs (M):     419.07 ; \t loss: 0.4789\n",
      "Total trained pairs (M):     419.54 ; \t loss: 0.4785\n",
      "Total trained pairs (M):     420.02 ; \t loss: 0.4784\n",
      "Total trained pairs (M):     420.50 ; \t loss: 0.4763\n",
      "Total trained pairs (M):     420.98 ; \t loss: 0.4754\n",
      "Total trained pairs (M):     421.47 ; \t loss: 0.4739\n",
      "Total trained pairs (M):     421.96 ; \t loss: 0.4727\n",
      "Total trained pairs (M):     422.44 ; \t loss: 0.4709\n",
      "Total trained pairs (M):     422.91 ; \t loss: 0.4688\n",
      "Total trained pairs (M):     423.39 ; \t loss: 0.4672\n",
      "Total trained pairs (M):     423.86 ; \t loss: 0.4643\n",
      "Total trained pairs (M):     424.34 ; \t loss: 0.4622\n",
      "Total trained pairs (M):     424.82 ; \t loss: 0.4543\n",
      "Total trained pairs (M):     425.30 ; \t loss: 0.4539\n",
      "Total trained pairs (M):     425.78 ; \t loss: 0.4531\n",
      "Total trained pairs (M):     426.26 ; \t loss: 0.4505\n",
      "Total trained pairs (M):     426.75 ; \t loss: 0.4486\n",
      "Total trained pairs (M):     427.23 ; \t loss: 0.4472\n",
      "Total trained pairs (M):     427.71 ; \t loss: 0.4455\n",
      "Total trained pairs (M):     428.19 ; \t loss: 0.4445\n",
      "Total trained pairs (M):     428.67 ; \t loss: 0.4427\n",
      "Total trained pairs (M):     429.16 ; \t loss: 0.4417\n",
      "Total trained pairs (M):     429.65 ; \t loss: 0.4399\n",
      "Total trained pairs (M):     430.13 ; \t loss: 0.4378\n",
      "Total trained pairs (M):     430.62 ; \t loss: 0.4367\n",
      "Total trained pairs (M):     431.11 ; \t loss: 0.4333\n",
      "Total trained pairs (M):     431.61 ; \t loss: 0.4333\n",
      "Total trained pairs (M):     432.12 ; \t loss: 0.4351\n",
      "Total trained pairs (M):     432.65 ; \t loss: 0.4351\n",
      "Total trained pairs (M):     433.18 ; \t loss: 0.4351\n",
      "Total trained pairs (M):     433.70 ; \t loss: 0.4336\n",
      "Total trained pairs (M):     434.21 ; \t loss: 0.4324\n",
      "Total trained pairs (M):     434.72 ; \t loss: 0.4331\n",
      "Total trained pairs (M):     435.23 ; \t loss: 0.4311\n",
      "Total trained pairs (M):     435.74 ; \t loss: 0.4283\n",
      "Total trained pairs (M):     436.24 ; \t loss: 0.4261\n",
      "Total trained pairs (M):     436.73 ; \t loss: 0.4240\n",
      "Total trained pairs (M):     437.21 ; \t loss: 0.4219\n",
      "Total trained pairs (M):     437.70 ; \t loss: 0.4212\n",
      "Total trained pairs (M):     438.18 ; \t loss: 0.4189\n",
      "Total trained pairs (M):     438.67 ; \t loss: 0.4197\n",
      "Total trained pairs (M):     439.16 ; \t loss: 0.4173\n",
      "Total trained pairs (M):     439.66 ; \t loss: 0.4220\n",
      "Total trained pairs (M):     440.16 ; \t loss: 0.4202\n",
      "Total trained pairs (M):     440.65 ; \t loss: 0.4183\n",
      "Total trained pairs (M):     441.14 ; \t loss: 0.4174\n",
      "Total trained pairs (M):     441.64 ; \t loss: 0.4190\n",
      "Total trained pairs (M):     442.14 ; \t loss: 0.4213\n",
      "Total trained pairs (M):     442.64 ; \t loss: 0.4205\n",
      "Total trained pairs (M):     443.13 ; \t loss: 0.4211\n",
      "Total trained pairs (M):     443.64 ; \t loss: 0.4228\n",
      "Total trained pairs (M):     444.15 ; \t loss: 0.4214\n",
      "Total trained pairs (M):     444.66 ; \t loss: 0.4206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trained pairs (M):     445.18 ; \t loss: 0.4207\n",
      "Total trained pairs (M):     445.70 ; \t loss: 0.4201\n",
      "Total trained pairs (M):     446.22 ; \t loss: 0.4185\n",
      "Total trained pairs (M):     446.73 ; \t loss: 0.4174\n",
      "Total trained pairs (M):     447.26 ; \t loss: 0.4226\n",
      "Total trained pairs (M):     447.80 ; \t loss: 0.4234\n",
      "Total trained pairs (M):     448.34 ; \t loss: 0.4220\n",
      "Total trained pairs (M):     448.87 ; \t loss: 0.4216\n",
      "Total trained pairs (M):     449.42 ; \t loss: 0.4203\n",
      "Total trained pairs (M):     449.97 ; \t loss: 0.4216\n",
      "Total trained pairs (M):     450.53 ; \t loss: 0.4197\n",
      "Total trained pairs (M):     451.09 ; \t loss: 0.4188\n",
      "Total trained pairs (M):     451.63 ; \t loss: 0.4195\n",
      "Total trained pairs (M):     452.20 ; \t loss: 0.4185\n",
      "Total trained pairs (M):     452.75 ; \t loss: 0.4182\n",
      "Total trained pairs (M):     453.31 ; \t loss: 0.4160\n",
      "Total trained pairs (M):     453.86 ; \t loss: 0.4177\n",
      "Total trained pairs (M):     454.38 ; \t loss: 0.4174\n",
      "Total trained pairs (M):     454.90 ; \t loss: 0.4165\n",
      "Total trained pairs (M):     455.42 ; \t loss: 0.4195\n",
      "Total trained pairs (M):     455.93 ; \t loss: 0.4199\n",
      "Total trained pairs (M):     456.44 ; \t loss: 0.4183\n",
      "Total trained pairs (M):     456.95 ; \t loss: 0.4170\n",
      "Total trained pairs (M):     457.47 ; \t loss: 0.4167\n",
      "Total trained pairs (M):     457.98 ; \t loss: 0.4163\n",
      "Total trained pairs (M):     458.48 ; \t loss: 0.4156\n",
      "Total trained pairs (M):     458.99 ; \t loss: 0.4167\n",
      "Total trained pairs (M):     459.50 ; \t loss: 0.4165\n",
      "Total trained pairs (M):     460.01 ; \t loss: 0.4170\n",
      "Total trained pairs (M):     460.52 ; \t loss: 0.4167\n",
      "Total trained pairs (M):     461.03 ; \t loss: 0.4150\n",
      "Total trained pairs (M):     461.54 ; \t loss: 0.4173\n",
      "Total trained pairs (M):     462.05 ; \t loss: 0.4151\n",
      "Total trained pairs (M):     462.57 ; \t loss: 0.4161\n",
      "Total trained pairs (M):     463.09 ; \t loss: 0.4170\n",
      "Total trained pairs (M):     463.61 ; \t loss: 0.4169\n",
      "Total trained pairs (M):     464.14 ; \t loss: 0.4154\n",
      "Total trained pairs (M):     464.67 ; \t loss: 0.4151\n",
      "Total trained pairs (M):     465.20 ; \t loss: 0.4146\n",
      "Total trained pairs (M):     465.72 ; \t loss: 0.4134\n",
      "Total trained pairs (M):     466.25 ; \t loss: 0.4118\n",
      "Total trained pairs (M):     466.78 ; \t loss: 0.4119\n",
      "Total trained pairs (M):     467.32 ; \t loss: 0.4118\n",
      "Total trained pairs (M):     467.84 ; \t loss: 0.4138\n",
      "Total trained pairs (M):     468.36 ; \t loss: 0.4141\n",
      "Total trained pairs (M):     468.88 ; \t loss: 0.4126\n",
      "Total trained pairs (M):     469.40 ; \t loss: 0.4116\n",
      "Total trained pairs (M):     469.93 ; \t loss: 0.4120\n",
      "Total trained pairs (M):     470.45 ; \t loss: 0.4128\n",
      "Total trained pairs (M):     470.97 ; \t loss: 0.4118\n",
      "Total trained pairs (M):     471.49 ; \t loss: 0.4114\n",
      "Total trained pairs (M):     472.01 ; \t loss: 0.4104\n",
      "Total trained pairs (M):     472.52 ; \t loss: 0.4103\n",
      "Total trained pairs (M):     473.03 ; \t loss: 0.4082\n",
      "Total trained pairs (M):     473.55 ; \t loss: 0.4076\n",
      "Total trained pairs (M):     474.07 ; \t loss: 0.4080\n",
      "Total trained pairs (M):     474.60 ; \t loss: 0.4068\n",
      "Total trained pairs (M):     475.12 ; \t loss: 0.4058\n",
      "Total trained pairs (M):     475.65 ; \t loss: 0.4080\n",
      "Total trained pairs (M):     476.17 ; \t loss: 0.4094\n",
      "Total trained pairs (M):     476.71 ; \t loss: 0.4127\n",
      "Total trained pairs (M):     477.24 ; \t loss: 0.4124\n",
      "Total trained pairs (M):     477.77 ; \t loss: 0.4132\n",
      "Total trained pairs (M):     478.31 ; \t loss: 0.4107\n",
      "Total trained pairs (M):     478.85 ; \t loss: 0.4110\n",
      "Total trained pairs (M):     479.38 ; \t loss: 0.4091\n",
      "Total trained pairs (M):     479.92 ; \t loss: 0.4079\n",
      "Total trained pairs (M):     480.45 ; \t loss: 0.4058\n",
      "Total trained pairs (M):     480.98 ; \t loss: 0.4063\n",
      "Total trained pairs (M):     481.51 ; \t loss: 0.4046\n",
      "Total trained pairs (M):     482.04 ; \t loss: 0.4049\n",
      "Total trained pairs (M):     482.57 ; \t loss: 0.4030\n",
      "Total trained pairs (M):     483.10 ; \t loss: 0.4028\n",
      "Total trained pairs (M):     483.62 ; \t loss: 0.4052\n",
      "Total trained pairs (M):     484.12 ; \t loss: 0.4066\n",
      "Total trained pairs (M):     484.61 ; \t loss: 0.4057\n",
      "Total trained pairs (M):     485.10 ; \t loss: 0.4041\n",
      "Total trained pairs (M):     485.59 ; \t loss: 0.4030\n",
      "Total trained pairs (M):     486.08 ; \t loss: 0.4019\n",
      "Total trained pairs (M):     486.56 ; \t loss: 0.4012\n",
      "Total trained pairs (M):     487.05 ; \t loss: 0.4010\n",
      "Total trained pairs (M):     487.57 ; \t loss: 0.4018\n",
      "Total trained pairs (M):     488.08 ; \t loss: 0.4015\n",
      "Total trained pairs (M):     488.59 ; \t loss: 0.4008\n",
      "Total trained pairs (M):     489.11 ; \t loss: 0.4000\n",
      "Total trained pairs (M):     489.63 ; \t loss: 0.4000\n",
      "Total trained pairs (M):     490.15 ; \t loss: 0.3983\n",
      "Total trained pairs (M):     490.66 ; \t loss: 0.3980\n",
      "Total trained pairs (M):     491.18 ; \t loss: 0.3958\n",
      "Total trained pairs (M):     491.68 ; \t loss: 0.3982\n",
      "Total trained pairs (M):     492.19 ; \t loss: 0.3966\n",
      "Total trained pairs (M):     492.69 ; \t loss: 0.3969\n",
      "Total trained pairs (M):     493.20 ; \t loss: 0.3942\n",
      "Total trained pairs (M):     493.69 ; \t loss: 0.3961\n",
      "Total trained pairs (M):     494.17 ; \t loss: 0.3946\n",
      "Total trained pairs (M):     494.65 ; \t loss: 0.3945\n",
      "Total trained pairs (M):     495.12 ; \t loss: 0.3935\n",
      "Total trained pairs (M):     495.58 ; \t loss: 0.3961\n",
      "Total trained pairs (M):     496.04 ; \t loss: 0.3958\n",
      "Total trained pairs (M):     496.50 ; \t loss: 0.3939\n",
      "Total trained pairs (M):     496.94 ; \t loss: 0.3954\n",
      "Total trained pairs (M):     497.38 ; \t loss: 0.3945\n",
      "Total trained pairs (M):     497.82 ; \t loss: 0.3931\n",
      "Total trained pairs (M):     498.27 ; \t loss: 0.3908\n",
      "Total trained pairs (M):     498.69 ; \t loss: 0.3937\n",
      "Total trained pairs (M):     499.12 ; \t loss: 0.3954\n",
      "Total trained pairs (M):     499.54 ; \t loss: 0.3930\n",
      "Total trained pairs (M):     499.96 ; \t loss: 0.3935\n",
      "Total trained pairs (M):     500.39 ; \t loss: 0.3989\n",
      "Total trained pairs (M):     500.80 ; \t loss: 0.4011\n",
      "Total trained pairs (M):     501.23 ; \t loss: 0.4026\n",
      "Total trained pairs (M):     501.64 ; \t loss: 0.4037\n",
      "Total trained pairs (M):     502.06 ; \t loss: 0.4019\n",
      "Total trained pairs (M):     502.46 ; \t loss: 0.4047\n",
      "Total trained pairs (M):     502.88 ; \t loss: 0.4055\n",
      "Total trained pairs (M):     503.30 ; \t loss: 0.4071\n",
      "Total trained pairs (M):     503.74 ; \t loss: 0.4121\n",
      "Total trained pairs (M):     504.20 ; \t loss: 0.4176\n",
      "Total trained pairs (M):     504.66 ; \t loss: 0.4177\n",
      "Total trained pairs (M):     505.11 ; \t loss: 0.4176\n",
      "Total trained pairs (M):     505.55 ; \t loss: 0.4163\n",
      "Total trained pairs (M):     506.00 ; \t loss: 0.4155\n",
      "Total trained pairs (M):     506.45 ; \t loss: 0.4144\n",
      "Total trained pairs (M):     506.89 ; \t loss: 0.4134\n",
      "Total trained pairs (M):     507.33 ; \t loss: 0.4132\n",
      "Total trained pairs (M):     507.78 ; \t loss: 0.4125\n",
      "Total trained pairs (M):     508.22 ; \t loss: 0.4118\n",
      "Total trained pairs (M):     508.67 ; \t loss: 0.4115\n",
      "Total trained pairs (M):     509.12 ; \t loss: 0.4121\n",
      "Total trained pairs (M):     509.57 ; \t loss: 0.4112\n",
      "Total trained pairs (M):     510.02 ; \t loss: 0.4104\n",
      "Total trained pairs (M):     510.47 ; \t loss: 0.4116\n",
      "Total trained pairs (M):     510.92 ; \t loss: 0.4105\n",
      "Total trained pairs (M):     511.36 ; \t loss: 0.4088\n",
      "Total trained pairs (M):     511.80 ; \t loss: 0.4101\n",
      "Total trained pairs (M):     512.24 ; \t loss: 0.4080\n",
      "Total trained pairs (M):     512.69 ; \t loss: 0.4115\n",
      "Total trained pairs (M):     513.12 ; \t loss: 0.4113\n",
      "Total trained pairs (M):     513.56 ; \t loss: 0.4090\n",
      "Total trained pairs (M):     514.00 ; \t loss: 0.4075\n",
      "Total trained pairs (M):     514.45 ; \t loss: 0.4070\n",
      "Total trained pairs (M):     514.88 ; \t loss: 0.4061\n",
      "Total trained pairs (M):     515.33 ; \t loss: 0.4089\n",
      "Total trained pairs (M):     515.77 ; \t loss: 0.4089\n",
      "Total trained pairs (M):     516.21 ; \t loss: 0.4101\n",
      "Total trained pairs (M):     516.66 ; \t loss: 0.4071\n",
      "Total trained pairs (M):     517.10 ; \t loss: 0.4069\n",
      "Total trained pairs (M):     517.55 ; \t loss: 0.4085\n",
      "Total trained pairs (M):     517.99 ; \t loss: 0.4087\n",
      "Total trained pairs (M):     518.44 ; \t loss: 0.4080\n",
      "Total trained pairs (M):     518.89 ; \t loss: 0.4095\n",
      "Total trained pairs (M):     519.34 ; \t loss: 0.4119\n",
      "Total trained pairs (M):     519.80 ; \t loss: 0.4117\n",
      "Total trained pairs (M):     520.26 ; \t loss: 0.4116\n",
      "Total trained pairs (M):     520.73 ; \t loss: 0.4137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trained pairs (M):     521.19 ; \t loss: 0.4146\n",
      "Total trained pairs (M):     521.64 ; \t loss: 0.4116\n",
      "Total trained pairs (M):     522.10 ; \t loss: 0.4119\n",
      "Total trained pairs (M):     522.55 ; \t loss: 0.4119\n",
      "Total trained pairs (M):     523.01 ; \t loss: 0.4105\n",
      "Total trained pairs (M):     523.44 ; \t loss: 0.4145\n",
      "Total trained pairs (M):     523.88 ; \t loss: 0.4174\n",
      "Total trained pairs (M):     524.31 ; \t loss: 0.4155\n",
      "Total trained pairs (M):     524.75 ; \t loss: 0.4173\n",
      "Total trained pairs (M):     525.18 ; \t loss: 0.4151\n",
      "Total trained pairs (M):     525.62 ; \t loss: 0.4152\n",
      "Total trained pairs (M):     526.06 ; \t loss: 0.4134\n",
      "Total trained pairs (M):     526.54 ; \t loss: 0.4274\n",
      "Total trained pairs (M):     527.01 ; \t loss: 0.4264\n",
      "Total trained pairs (M):     527.48 ; \t loss: 0.4241\n",
      "Total trained pairs (M):     527.95 ; \t loss: 0.4251\n",
      "Total trained pairs (M):     528.43 ; \t loss: 0.4254\n",
      "Total trained pairs (M):     528.91 ; \t loss: 0.4248\n",
      "Total trained pairs (M):     529.39 ; \t loss: 0.4241\n",
      "Total trained pairs (M):     529.86 ; \t loss: 0.4224\n",
      "Total trained pairs (M):     530.34 ; \t loss: 0.4210\n",
      "Total trained pairs (M):     530.81 ; \t loss: 0.4230\n",
      "Total trained pairs (M):     531.28 ; \t loss: 0.4196\n",
      "Total trained pairs (M):     531.76 ; \t loss: 0.4188\n",
      "Total trained pairs (M):     532.24 ; \t loss: 0.4190\n",
      "Total trained pairs (M):     532.71 ; \t loss: 0.4201\n",
      "Total trained pairs (M):     533.17 ; \t loss: 0.4222\n",
      "Total trained pairs (M):     533.62 ; \t loss: 0.4196\n",
      "Total trained pairs (M):     534.09 ; \t loss: 0.4185\n",
      "Total trained pairs (M):     534.56 ; \t loss: 0.4206\n",
      "Total trained pairs (M):     535.02 ; \t loss: 0.4212\n",
      "Total trained pairs (M):     535.49 ; \t loss: 0.4197\n",
      "Total trained pairs (M):     535.96 ; \t loss: 0.4190\n",
      "Total trained pairs (M):     536.44 ; \t loss: 0.4189\n",
      "Total trained pairs (M):     536.92 ; \t loss: 0.4176\n",
      "Total trained pairs (M):     537.41 ; \t loss: 0.4176\n",
      "Total trained pairs (M):     537.90 ; \t loss: 0.4189\n",
      "Total trained pairs (M):     538.41 ; \t loss: 0.4178\n",
      "Total trained pairs (M):     538.91 ; \t loss: 0.4166\n",
      "Total trained pairs (M):     539.43 ; \t loss: 0.4193\n",
      "Total trained pairs (M):     539.94 ; \t loss: 0.4164\n",
      "Total trained pairs (M):     540.45 ; \t loss: 0.4177\n",
      "Total trained pairs (M):     540.96 ; \t loss: 0.4160\n",
      "Total trained pairs (M):     541.47 ; \t loss: 0.4150\n",
      "Total trained pairs (M):     541.98 ; \t loss: 0.4139\n",
      "Total trained pairs (M):     542.49 ; \t loss: 0.4115\n",
      "Total trained pairs (M):     543.01 ; \t loss: 0.4120\n",
      "Total trained pairs (M):     543.53 ; \t loss: 0.4093\n",
      "Total trained pairs (M):     544.04 ; \t loss: 0.4083\n",
      "Total trained pairs (M):     544.56 ; \t loss: 0.4063\n",
      "Total trained pairs (M):     545.07 ; \t loss: 0.4081\n",
      "Total trained pairs (M):     545.57 ; \t loss: 0.4066\n",
      "Total trained pairs (M):     546.09 ; \t loss: 0.4067\n",
      "Total trained pairs (M):     546.59 ; \t loss: 0.4085\n",
      "Total trained pairs (M):     547.09 ; \t loss: 0.4068\n",
      "Total trained pairs (M):     547.59 ; \t loss: 0.4069\n",
      "Total trained pairs (M):     548.08 ; \t loss: 0.4061\n",
      "Total trained pairs (M):     548.57 ; \t loss: 0.4064\n",
      "Total trained pairs (M):     549.07 ; \t loss: 0.4041\n",
      "Total trained pairs (M):     549.55 ; \t loss: 0.4092\n",
      "Total trained pairs (M):     550.03 ; \t loss: 0.4118\n",
      "Total trained pairs (M):     550.50 ; \t loss: 0.4145\n",
      "Total trained pairs (M):     550.95 ; \t loss: 0.4138\n",
      "Total trained pairs (M):     551.40 ; \t loss: 0.4132\n",
      "Total trained pairs (M):     551.84 ; \t loss: 0.4120\n",
      "Total trained pairs (M):     552.29 ; \t loss: 0.4096\n",
      "Total trained pairs (M):     552.75 ; \t loss: 0.4107\n",
      "Total trained pairs (M):     553.20 ; \t loss: 0.4090\n",
      "Total trained pairs (M):     553.66 ; \t loss: 0.4083\n",
      "Total trained pairs (M):     554.12 ; \t loss: 0.4087\n",
      "Total trained pairs (M):     554.58 ; \t loss: 0.4078\n",
      "Total trained pairs (M):     555.04 ; \t loss: 0.4087\n",
      "Total trained pairs (M):     555.51 ; \t loss: 0.4095\n",
      "Total trained pairs (M):     555.97 ; \t loss: 0.4089\n",
      "Total trained pairs (M):     556.43 ; \t loss: 0.4070\n",
      "Total trained pairs (M):     556.90 ; \t loss: 0.4086\n",
      "Total trained pairs (M):     557.36 ; \t loss: 0.4092\n",
      "Total trained pairs (M):     557.83 ; \t loss: 0.4106\n",
      "Total trained pairs (M):     558.30 ; \t loss: 0.4106\n",
      "Total trained pairs (M):     558.78 ; \t loss: 0.4096\n",
      "Total trained pairs (M):     559.26 ; \t loss: 0.4097\n",
      "Total trained pairs (M):     559.73 ; \t loss: 0.4093\n",
      "Total trained pairs (M):     560.20 ; \t loss: 0.4086\n",
      "Total trained pairs (M):     560.68 ; \t loss: 0.4083\n",
      "Total trained pairs (M):     561.15 ; \t loss: 0.4081\n",
      "Total trained pairs (M):     561.63 ; \t loss: 0.4065\n",
      "Total trained pairs (M):     562.11 ; \t loss: 0.4035\n",
      "Total trained pairs (M):     562.58 ; \t loss: 0.4078\n",
      "Total trained pairs (M):     563.06 ; \t loss: 0.4067\n",
      "Total trained pairs (M):     563.53 ; \t loss: 0.4071\n",
      "Total trained pairs (M):     564.00 ; \t loss: 0.4059\n",
      "Total trained pairs (M):     564.48 ; \t loss: 0.4046\n",
      "Total trained pairs (M):     564.95 ; \t loss: 0.4041\n",
      "Total trained pairs (M):     565.43 ; \t loss: 0.4041\n",
      "Total trained pairs (M):     565.91 ; \t loss: 0.4053\n",
      "Total trained pairs (M):     566.38 ; \t loss: 0.4056\n",
      "Total trained pairs (M):     566.85 ; \t loss: 0.4052\n",
      "Total trained pairs (M):     567.31 ; \t loss: 0.4047\n",
      "Total trained pairs (M):     567.78 ; \t loss: 0.4118\n",
      "Total trained pairs (M):     568.25 ; \t loss: 0.4114\n",
      "Total trained pairs (M):     568.72 ; \t loss: 0.4111\n",
      "Total trained pairs (M):     569.19 ; \t loss: 0.4090\n",
      "Total trained pairs (M):     569.66 ; \t loss: 0.4104\n",
      "Total trained pairs (M):     570.13 ; \t loss: 0.4115\n",
      "Total trained pairs (M):     570.61 ; \t loss: 0.4096\n",
      "Total trained pairs (M):     571.08 ; \t loss: 0.4101\n",
      "Total trained pairs (M):     571.54 ; \t loss: 0.4085\n",
      "Total trained pairs (M):     572.01 ; \t loss: 0.4095\n",
      "Total trained pairs (M):     572.48 ; \t loss: 0.4110\n",
      "Total trained pairs (M):     572.96 ; \t loss: 0.4093\n",
      "Total trained pairs (M):     573.44 ; \t loss: 0.4098\n",
      "Total trained pairs (M):     573.92 ; \t loss: 0.4100\n",
      "Total trained pairs (M):     574.37 ; \t loss: 0.4118\n",
      "Total trained pairs (M):     574.82 ; \t loss: 0.4132\n",
      "Total trained pairs (M):     575.28 ; \t loss: 0.4176\n",
      "Total trained pairs (M):     575.74 ; \t loss: 0.4174\n",
      "Total trained pairs (M):     576.19 ; \t loss: 0.4179\n",
      "Total trained pairs (M):     576.64 ; \t loss: 0.4166\n",
      "Total trained pairs (M):     577.10 ; \t loss: 0.4136\n",
      "Total trained pairs (M):     577.55 ; \t loss: 0.4124\n",
      "Total trained pairs (M):     578.01 ; \t loss: 0.4100\n",
      "Total trained pairs (M):     578.47 ; \t loss: 0.4090\n",
      "Total trained pairs (M):     578.92 ; \t loss: 0.4083\n",
      "Total trained pairs (M):     579.38 ; \t loss: 0.4085\n",
      "Total trained pairs (M):     579.83 ; \t loss: 0.4113\n",
      "Total trained pairs (M):     580.29 ; \t loss: 0.4102\n",
      "Total trained pairs (M):     580.75 ; \t loss: 0.4089\n",
      "Total trained pairs (M):     581.21 ; \t loss: 0.4085\n",
      "Total trained pairs (M):     581.66 ; \t loss: 0.4107\n",
      "Total trained pairs (M):     582.11 ; \t loss: 0.4131\n",
      "Total trained pairs (M):     582.56 ; \t loss: 0.4111\n",
      "Total trained pairs (M):     583.00 ; \t loss: 0.4121\n",
      "Total trained pairs (M):     583.44 ; \t loss: 0.4152\n",
      "Total trained pairs (M):     583.88 ; \t loss: 0.4157\n",
      "Total trained pairs (M):     584.31 ; \t loss: 0.4142\n",
      "Total trained pairs (M):     584.73 ; \t loss: 0.4175\n",
      "Total trained pairs (M):     585.15 ; \t loss: 0.4186\n",
      "Total trained pairs (M):     585.56 ; \t loss: 0.4189\n",
      "Total trained pairs (M):     585.97 ; \t loss: 0.4178\n",
      "Total trained pairs (M):     586.37 ; \t loss: 0.4169\n",
      "Total trained pairs (M):     586.78 ; \t loss: 0.4187\n",
      "Total trained pairs (M):     587.18 ; \t loss: 0.4189\n",
      "Total trained pairs (M):     587.59 ; \t loss: 0.4166\n",
      "Total trained pairs (M):     587.99 ; \t loss: 0.4140\n",
      "Total trained pairs (M):     588.40 ; \t loss: 0.4145\n",
      "Total trained pairs (M):     588.80 ; \t loss: 0.4112\n",
      "Total trained pairs (M):     589.21 ; \t loss: 0.4151\n",
      "Total trained pairs (M):     589.61 ; \t loss: 0.4139\n",
      "Total trained pairs (M):     590.02 ; \t loss: 0.4133\n",
      "Total trained pairs (M):     590.43 ; \t loss: 0.4131\n",
      "Total trained pairs (M):     590.84 ; \t loss: 0.4133\n",
      "Total trained pairs (M):     591.25 ; \t loss: 0.4114\n",
      "Total trained pairs (M):     591.66 ; \t loss: 0.4169\n",
      "Total trained pairs (M):     592.08 ; \t loss: 0.4157\n",
      "Total trained pairs (M):     592.49 ; \t loss: 0.4227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trained pairs (M):     592.91 ; \t loss: 0.4227\n",
      "Total trained pairs (M):     593.33 ; \t loss: 0.4215\n",
      "Total trained pairs (M):     593.74 ; \t loss: 0.4192\n",
      "Total trained pairs (M):     594.17 ; \t loss: 0.4237\n",
      "Total trained pairs (M):     594.60 ; \t loss: 0.4247\n",
      "Total trained pairs (M):     595.02 ; \t loss: 0.4268\n",
      "Total trained pairs (M):     595.44 ; \t loss: 0.4248\n",
      "Total trained pairs (M):     595.87 ; \t loss: 0.4239\n",
      "Total trained pairs (M):     596.30 ; \t loss: 0.4241\n",
      "Total trained pairs (M):     596.72 ; \t loss: 0.4230\n",
      "Total trained pairs (M):     597.17 ; \t loss: 0.4260\n",
      "Total trained pairs (M):     597.62 ; \t loss: 0.4289\n",
      "Total trained pairs (M):     598.07 ; \t loss: 0.4273\n",
      "Total trained pairs (M):     598.52 ; \t loss: 0.4257\n",
      "Total trained pairs (M):     598.96 ; \t loss: 0.4263\n",
      "Total trained pairs (M):     599.40 ; \t loss: 0.4253\n",
      "Total trained pairs (M):     599.85 ; \t loss: 0.4354\n",
      "Total trained pairs (M):     600.30 ; \t loss: 0.4397\n",
      "Total trained pairs (M):     600.75 ; \t loss: 0.4393\n",
      "Total trained pairs (M):     601.19 ; \t loss: 0.4378\n",
      "Total trained pairs (M):     601.63 ; \t loss: 0.4378\n",
      "Total trained pairs (M):     602.07 ; \t loss: 0.4370\n",
      "Total trained pairs (M):     602.51 ; \t loss: 0.4409\n",
      "Total trained pairs (M):     602.94 ; \t loss: 0.4412\n",
      "Total trained pairs (M):     603.36 ; \t loss: 0.4397\n",
      "Total trained pairs (M):     603.79 ; \t loss: 0.4404\n",
      "Total trained pairs (M):     604.21 ; \t loss: 0.4422\n",
      "Total trained pairs (M):     604.65 ; \t loss: 0.4427\n",
      "Total trained pairs (M):     605.08 ; \t loss: 0.4482\n",
      "Total trained pairs (M):     605.52 ; \t loss: 0.4482\n",
      "Total trained pairs (M):     605.98 ; \t loss: 0.4480\n",
      "Total trained pairs (M):     606.43 ; \t loss: 0.4470\n",
      "Epoch 3 ======\n",
      "Total trained pairs (M):     606.94 ; \t loss: 0.5819\n",
      "Total trained pairs (M):     607.47 ; \t loss: 0.5740\n",
      "Total trained pairs (M):     607.99 ; \t loss: 0.5630\n",
      "Total trained pairs (M):     608.51 ; \t loss: 0.5568\n",
      "Total trained pairs (M):     609.02 ; \t loss: 0.5512\n",
      "Total trained pairs (M):     609.53 ; \t loss: 0.5455\n",
      "Total trained pairs (M):     610.05 ; \t loss: 0.5411\n",
      "Total trained pairs (M):     610.55 ; \t loss: 0.5366\n",
      "Total trained pairs (M):     611.05 ; \t loss: 0.5329\n",
      "Total trained pairs (M):     611.55 ; \t loss: 0.5259\n",
      "Total trained pairs (M):     612.05 ; \t loss: 0.5233\n",
      "Total trained pairs (M):     612.55 ; \t loss: 0.5186\n",
      "Total trained pairs (M):     613.05 ; \t loss: 0.5167\n",
      "Total trained pairs (M):     613.55 ; \t loss: 0.5126\n",
      "Total trained pairs (M):     614.04 ; \t loss: 0.5072\n",
      "Total trained pairs (M):     614.53 ; \t loss: 0.5068\n",
      "Total trained pairs (M):     615.01 ; \t loss: 0.5048\n",
      "Total trained pairs (M):     615.49 ; \t loss: 0.5005\n",
      "Total trained pairs (M):     615.97 ; \t loss: 0.5000\n",
      "Total trained pairs (M):     616.45 ; \t loss: 0.4970\n",
      "Total trained pairs (M):     616.93 ; \t loss: 0.4919\n",
      "Total trained pairs (M):     617.41 ; \t loss: 0.4888\n",
      "Total trained pairs (M):     617.88 ; \t loss: 0.4891\n",
      "Total trained pairs (M):     618.36 ; \t loss: 0.4855\n",
      "Total trained pairs (M):     618.83 ; \t loss: 0.4826\n",
      "Total trained pairs (M):     619.32 ; \t loss: 0.4784\n",
      "Total trained pairs (M):     619.81 ; \t loss: 0.4762\n",
      "Total trained pairs (M):     620.27 ; \t loss: 0.4672\n",
      "Total trained pairs (M):     620.74 ; \t loss: 0.4667\n",
      "Total trained pairs (M):     621.21 ; \t loss: 0.4638\n",
      "Total trained pairs (M):     621.68 ; \t loss: 0.4624\n",
      "Total trained pairs (M):     622.16 ; \t loss: 0.4624\n",
      "Total trained pairs (M):     622.64 ; \t loss: 0.4607\n",
      "Total trained pairs (M):     623.13 ; \t loss: 0.4575\n",
      "Total trained pairs (M):     623.62 ; \t loss: 0.4563\n",
      "Total trained pairs (M):     624.10 ; \t loss: 0.4550\n",
      "Total trained pairs (M):     624.58 ; \t loss: 0.4536\n",
      "Total trained pairs (M):     625.05 ; \t loss: 0.4505\n",
      "Total trained pairs (M):     625.53 ; \t loss: 0.4485\n",
      "Total trained pairs (M):     626.00 ; \t loss: 0.4469\n",
      "Total trained pairs (M):     626.48 ; \t loss: 0.4444\n",
      "Total trained pairs (M):     626.96 ; \t loss: 0.4367\n",
      "Total trained pairs (M):     627.44 ; \t loss: 0.4357\n",
      "Total trained pairs (M):     627.92 ; \t loss: 0.4334\n",
      "Total trained pairs (M):     628.40 ; \t loss: 0.4311\n",
      "Total trained pairs (M):     628.89 ; \t loss: 0.4299\n",
      "Total trained pairs (M):     629.37 ; \t loss: 0.4287\n",
      "Total trained pairs (M):     629.85 ; \t loss: 0.4266\n",
      "Total trained pairs (M):     630.33 ; \t loss: 0.4248\n",
      "Total trained pairs (M):     630.81 ; \t loss: 0.4219\n",
      "Total trained pairs (M):     631.30 ; \t loss: 0.4217\n",
      "Total trained pairs (M):     631.79 ; \t loss: 0.4193\n",
      "Total trained pairs (M):     632.28 ; \t loss: 0.4181\n",
      "Total trained pairs (M):     632.76 ; \t loss: 0.4159\n",
      "Total trained pairs (M):     633.25 ; \t loss: 0.4132\n",
      "Total trained pairs (M):     633.75 ; \t loss: 0.4126\n",
      "Total trained pairs (M):     634.27 ; \t loss: 0.4139\n",
      "Total trained pairs (M):     634.80 ; \t loss: 0.4155\n",
      "Total trained pairs (M):     635.32 ; \t loss: 0.4135\n",
      "Total trained pairs (M):     635.84 ; \t loss: 0.4127\n",
      "Total trained pairs (M):     636.35 ; \t loss: 0.4114\n",
      "Total trained pairs (M):     636.87 ; \t loss: 0.4108\n",
      "Total trained pairs (M):     637.38 ; \t loss: 0.4091\n",
      "Total trained pairs (M):     637.89 ; \t loss: 0.4070\n",
      "Total trained pairs (M):     638.38 ; \t loss: 0.4041\n",
      "Total trained pairs (M):     638.87 ; \t loss: 0.4011\n",
      "Total trained pairs (M):     639.35 ; \t loss: 0.3991\n",
      "Total trained pairs (M):     639.84 ; \t loss: 0.3985\n",
      "Total trained pairs (M):     640.33 ; \t loss: 0.3973\n",
      "Total trained pairs (M):     640.82 ; \t loss: 0.3964\n",
      "Total trained pairs (M):     641.30 ; \t loss: 0.3951\n",
      "Total trained pairs (M):     641.80 ; \t loss: 0.3990\n",
      "Total trained pairs (M):     642.30 ; \t loss: 0.3974\n",
      "Total trained pairs (M):     642.79 ; \t loss: 0.3956\n",
      "Total trained pairs (M):     643.28 ; \t loss: 0.3934\n",
      "Total trained pairs (M):     643.78 ; \t loss: 0.3957\n",
      "Total trained pairs (M):     644.28 ; \t loss: 0.3971\n",
      "Total trained pairs (M):     644.78 ; \t loss: 0.3948\n",
      "Total trained pairs (M):     645.27 ; \t loss: 0.3966\n",
      "Total trained pairs (M):     645.78 ; \t loss: 0.3981\n",
      "Total trained pairs (M):     646.29 ; \t loss: 0.3970\n",
      "Total trained pairs (M):     646.80 ; \t loss: 0.3963\n",
      "Total trained pairs (M):     647.33 ; \t loss: 0.3973\n",
      "Total trained pairs (M):     647.84 ; \t loss: 0.3978\n",
      "Total trained pairs (M):     648.36 ; \t loss: 0.3944\n",
      "Total trained pairs (M):     648.87 ; \t loss: 0.3922\n",
      "Total trained pairs (M):     649.40 ; \t loss: 0.3982\n",
      "Total trained pairs (M):     649.94 ; \t loss: 0.3987\n",
      "Total trained pairs (M):     650.48 ; \t loss: 0.3972\n",
      "Total trained pairs (M):     651.02 ; \t loss: 0.3985\n",
      "Total trained pairs (M):     651.56 ; \t loss: 0.3963\n",
      "Total trained pairs (M):     652.11 ; \t loss: 0.3975\n",
      "Total trained pairs (M):     652.67 ; \t loss: 0.3946\n",
      "Total trained pairs (M):     653.23 ; \t loss: 0.3949\n",
      "Total trained pairs (M):     653.78 ; \t loss: 0.3931\n",
      "Total trained pairs (M):     654.34 ; \t loss: 0.3931\n",
      "Total trained pairs (M):     654.90 ; \t loss: 0.3934\n",
      "Total trained pairs (M):     655.45 ; \t loss: 0.3914\n",
      "Total trained pairs (M):     656.00 ; \t loss: 0.3914\n",
      "Total trained pairs (M):     656.52 ; \t loss: 0.3925\n",
      "Total trained pairs (M):     657.04 ; \t loss: 0.3902\n",
      "Total trained pairs (M):     657.56 ; \t loss: 0.3942\n",
      "Total trained pairs (M):     658.07 ; \t loss: 0.3935\n",
      "Total trained pairs (M):     658.58 ; \t loss: 0.3931\n",
      "Total trained pairs (M):     659.09 ; \t loss: 0.3921\n",
      "Total trained pairs (M):     659.61 ; \t loss: 0.3906\n",
      "Total trained pairs (M):     660.12 ; \t loss: 0.3907\n",
      "Total trained pairs (M):     660.62 ; \t loss: 0.3890\n",
      "Total trained pairs (M):     661.13 ; \t loss: 0.3900\n",
      "Total trained pairs (M):     661.64 ; \t loss: 0.3894\n",
      "Total trained pairs (M):     662.15 ; \t loss: 0.3905\n",
      "Total trained pairs (M):     662.66 ; \t loss: 0.3905\n",
      "Total trained pairs (M):     663.17 ; \t loss: 0.3875\n",
      "Total trained pairs (M):     663.68 ; \t loss: 0.3895\n",
      "Total trained pairs (M):     664.20 ; \t loss: 0.3875\n",
      "Total trained pairs (M):     664.71 ; \t loss: 0.3885\n",
      "Total trained pairs (M):     665.23 ; \t loss: 0.3903\n",
      "Total trained pairs (M):     665.76 ; \t loss: 0.3887\n",
      "Total trained pairs (M):     666.28 ; \t loss: 0.3884\n",
      "Total trained pairs (M):     666.81 ; \t loss: 0.3891\n",
      "Total trained pairs (M):     667.34 ; \t loss: 0.3876\n",
      "Total trained pairs (M):     667.86 ; \t loss: 0.3869\n",
      "Total trained pairs (M):     668.39 ; \t loss: 0.3845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trained pairs (M):     668.92 ; \t loss: 0.3846\n",
      "Total trained pairs (M):     669.46 ; \t loss: 0.3833\n",
      "Total trained pairs (M):     669.98 ; \t loss: 0.3871\n",
      "Total trained pairs (M):     670.50 ; \t loss: 0.3865\n",
      "Total trained pairs (M):     671.03 ; \t loss: 0.3829\n",
      "Total trained pairs (M):     671.54 ; \t loss: 0.3830\n",
      "Total trained pairs (M):     672.07 ; \t loss: 0.3852\n",
      "Total trained pairs (M):     672.60 ; \t loss: 0.3846\n",
      "Total trained pairs (M):     673.11 ; \t loss: 0.3835\n",
      "Total trained pairs (M):     673.63 ; \t loss: 0.3826\n",
      "Total trained pairs (M):     674.15 ; \t loss: 0.3818\n",
      "Total trained pairs (M):     674.66 ; \t loss: 0.3811\n",
      "Total trained pairs (M):     675.17 ; \t loss: 0.3797\n",
      "Total trained pairs (M):     675.69 ; \t loss: 0.3784\n",
      "Total trained pairs (M):     676.21 ; \t loss: 0.3808\n",
      "Total trained pairs (M):     676.74 ; \t loss: 0.3785\n",
      "Total trained pairs (M):     677.27 ; \t loss: 0.3769\n",
      "Total trained pairs (M):     677.80 ; \t loss: 0.3801\n",
      "Total trained pairs (M):     678.31 ; \t loss: 0.3794\n",
      "Total trained pairs (M):     678.85 ; \t loss: 0.3848\n",
      "Total trained pairs (M):     679.38 ; \t loss: 0.3845\n",
      "Total trained pairs (M):     679.92 ; \t loss: 0.3842\n",
      "Total trained pairs (M):     680.45 ; \t loss: 0.3826\n",
      "Total trained pairs (M):     680.99 ; \t loss: 0.3827\n",
      "Total trained pairs (M):     681.52 ; \t loss: 0.3808\n",
      "Total trained pairs (M):     682.06 ; \t loss: 0.3791\n",
      "Total trained pairs (M):     682.60 ; \t loss: 0.3761\n",
      "Total trained pairs (M):     683.12 ; \t loss: 0.3786\n",
      "Total trained pairs (M):     683.65 ; \t loss: 0.3775\n",
      "Total trained pairs (M):     684.18 ; \t loss: 0.3766\n",
      "Total trained pairs (M):     684.71 ; \t loss: 0.3749\n",
      "Total trained pairs (M):     685.24 ; \t loss: 0.3735\n",
      "Total trained pairs (M):     685.76 ; \t loss: 0.3755\n",
      "Total trained pairs (M):     686.26 ; \t loss: 0.3772\n",
      "Total trained pairs (M):     686.75 ; \t loss: 0.3774\n",
      "Total trained pairs (M):     687.24 ; \t loss: 0.3747\n",
      "Total trained pairs (M):     687.73 ; \t loss: 0.3746\n",
      "Total trained pairs (M):     688.22 ; \t loss: 0.3730\n",
      "Total trained pairs (M):     688.70 ; \t loss: 0.3711\n",
      "Total trained pairs (M):     689.20 ; \t loss: 0.3726\n",
      "Total trained pairs (M):     689.71 ; \t loss: 0.3723\n",
      "Total trained pairs (M):     690.22 ; \t loss: 0.3723\n",
      "Total trained pairs (M):     690.73 ; \t loss: 0.3704\n",
      "Total trained pairs (M):     691.25 ; \t loss: 0.3710\n",
      "Total trained pairs (M):     691.77 ; \t loss: 0.3705\n",
      "Total trained pairs (M):     692.29 ; \t loss: 0.3701\n",
      "Total trained pairs (M):     692.80 ; \t loss: 0.3688\n",
      "Total trained pairs (M):     693.32 ; \t loss: 0.3667\n",
      "Total trained pairs (M):     693.83 ; \t loss: 0.3691\n",
      "Total trained pairs (M):     694.33 ; \t loss: 0.3688\n",
      "Total trained pairs (M):     694.84 ; \t loss: 0.3688\n",
      "Total trained pairs (M):     695.34 ; \t loss: 0.3654\n",
      "Total trained pairs (M):     695.83 ; \t loss: 0.3664\n",
      "Total trained pairs (M):     696.31 ; \t loss: 0.3666\n",
      "Total trained pairs (M):     696.79 ; \t loss: 0.3660\n",
      "Total trained pairs (M):     697.26 ; \t loss: 0.3644\n",
      "Total trained pairs (M):     697.72 ; \t loss: 0.3672\n",
      "Total trained pairs (M):     698.18 ; \t loss: 0.3662\n",
      "Total trained pairs (M):     698.64 ; \t loss: 0.3636\n",
      "Total trained pairs (M):     699.08 ; \t loss: 0.3645\n",
      "Total trained pairs (M):     699.52 ; \t loss: 0.3653\n",
      "Total trained pairs (M):     699.97 ; \t loss: 0.3635\n",
      "Total trained pairs (M):     700.41 ; \t loss: 0.3629\n",
      "Total trained pairs (M):     700.83 ; \t loss: 0.3643\n",
      "Total trained pairs (M):     701.26 ; \t loss: 0.3654\n",
      "Total trained pairs (M):     701.68 ; \t loss: 0.3648\n",
      "Total trained pairs (M):     702.10 ; \t loss: 0.3647\n",
      "Total trained pairs (M):     702.53 ; \t loss: 0.3708\n",
      "Total trained pairs (M):     702.95 ; \t loss: 0.3702\n",
      "Total trained pairs (M):     703.37 ; \t loss: 0.3725\n",
      "Total trained pairs (M):     703.79 ; \t loss: 0.3732\n",
      "Total trained pairs (M):     704.20 ; \t loss: 0.3732\n",
      "Total trained pairs (M):     704.60 ; \t loss: 0.3761\n",
      "Total trained pairs (M):     705.02 ; \t loss: 0.3766\n",
      "Total trained pairs (M):     705.44 ; \t loss: 0.3781\n",
      "Total trained pairs (M):     705.88 ; \t loss: 0.3836\n",
      "Total trained pairs (M):     706.34 ; \t loss: 0.3891\n",
      "Total trained pairs (M):     706.80 ; \t loss: 0.3884\n",
      "Total trained pairs (M):     707.25 ; \t loss: 0.3896\n",
      "Total trained pairs (M):     707.69 ; \t loss: 0.3883\n",
      "Total trained pairs (M):     708.14 ; \t loss: 0.3867\n",
      "Total trained pairs (M):     708.59 ; \t loss: 0.3855\n",
      "Total trained pairs (M):     709.03 ; \t loss: 0.3833\n",
      "Total trained pairs (M):     709.47 ; \t loss: 0.3828\n",
      "Total trained pairs (M):     709.92 ; \t loss: 0.3824\n",
      "Total trained pairs (M):     710.37 ; \t loss: 0.3820\n",
      "Total trained pairs (M):     710.82 ; \t loss: 0.3833\n",
      "Total trained pairs (M):     711.26 ; \t loss: 0.3818\n",
      "Total trained pairs (M):     711.71 ; \t loss: 0.3830\n",
      "Total trained pairs (M):     712.16 ; \t loss: 0.3807\n",
      "Total trained pairs (M):     712.61 ; \t loss: 0.3822\n",
      "Total trained pairs (M):     713.06 ; \t loss: 0.3814\n",
      "Total trained pairs (M):     713.51 ; \t loss: 0.3803\n",
      "Total trained pairs (M):     713.94 ; \t loss: 0.3817\n",
      "Total trained pairs (M):     714.38 ; \t loss: 0.3789\n",
      "Total trained pairs (M):     714.83 ; \t loss: 0.3818\n",
      "Total trained pairs (M):     715.27 ; \t loss: 0.3836\n",
      "Total trained pairs (M):     715.71 ; \t loss: 0.3804\n",
      "Total trained pairs (M):     716.15 ; \t loss: 0.3786\n",
      "Total trained pairs (M):     716.59 ; \t loss: 0.3774\n",
      "Total trained pairs (M):     717.02 ; \t loss: 0.3769\n",
      "Total trained pairs (M):     717.47 ; \t loss: 0.3801\n",
      "Total trained pairs (M):     717.91 ; \t loss: 0.3797\n",
      "Total trained pairs (M):     718.36 ; \t loss: 0.3801\n",
      "Total trained pairs (M):     718.80 ; \t loss: 0.3790\n",
      "Total trained pairs (M):     719.25 ; \t loss: 0.3765\n",
      "Total trained pairs (M):     719.69 ; \t loss: 0.3786\n",
      "Total trained pairs (M):     720.13 ; \t loss: 0.3800\n",
      "Total trained pairs (M):     720.58 ; \t loss: 0.3782\n",
      "Total trained pairs (M):     721.03 ; \t loss: 0.3803\n",
      "Total trained pairs (M):     721.48 ; \t loss: 0.3823\n",
      "Total trained pairs (M):     721.94 ; \t loss: 0.3823\n",
      "Total trained pairs (M):     722.40 ; \t loss: 0.3822\n",
      "Total trained pairs (M):     722.87 ; \t loss: 0.3848\n",
      "Total trained pairs (M):     723.33 ; \t loss: 0.3850\n",
      "Total trained pairs (M):     723.79 ; \t loss: 0.3816\n",
      "Total trained pairs (M):     724.24 ; \t loss: 0.3812\n",
      "Total trained pairs (M):     724.70 ; \t loss: 0.3833\n",
      "Total trained pairs (M):     725.15 ; \t loss: 0.3807\n",
      "Total trained pairs (M):     725.58 ; \t loss: 0.3843\n",
      "Total trained pairs (M):     726.02 ; \t loss: 0.3853\n",
      "Total trained pairs (M):     726.46 ; \t loss: 0.3866\n",
      "Total trained pairs (M):     726.89 ; \t loss: 0.3853\n",
      "Total trained pairs (M):     727.33 ; \t loss: 0.3846\n",
      "Total trained pairs (M):     727.76 ; \t loss: 0.3843\n",
      "Total trained pairs (M):     728.21 ; \t loss: 0.3832\n",
      "Total trained pairs (M):     728.68 ; \t loss: 0.3975\n",
      "Total trained pairs (M):     729.15 ; \t loss: 0.3963\n",
      "Total trained pairs (M):     729.62 ; \t loss: 0.3943\n",
      "Total trained pairs (M):     730.10 ; \t loss: 0.3951\n",
      "Total trained pairs (M):     730.58 ; \t loss: 0.3960\n",
      "Total trained pairs (M):     731.06 ; \t loss: 0.3948\n",
      "Total trained pairs (M):     731.53 ; \t loss: 0.3934\n",
      "Total trained pairs (M):     732.01 ; \t loss: 0.3929\n",
      "Total trained pairs (M):     732.48 ; \t loss: 0.3910\n",
      "Total trained pairs (M):     732.95 ; \t loss: 0.3916\n",
      "Total trained pairs (M):     733.43 ; \t loss: 0.3894\n",
      "Total trained pairs (M):     733.90 ; \t loss: 0.3886\n",
      "Total trained pairs (M):     734.38 ; \t loss: 0.3880\n",
      "Total trained pairs (M):     734.85 ; \t loss: 0.3899\n",
      "Total trained pairs (M):     735.31 ; \t loss: 0.3914\n",
      "Total trained pairs (M):     735.77 ; \t loss: 0.3891\n",
      "Total trained pairs (M):     736.23 ; \t loss: 0.3869\n",
      "Total trained pairs (M):     736.70 ; \t loss: 0.3895\n",
      "Total trained pairs (M):     737.16 ; \t loss: 0.3900\n",
      "Total trained pairs (M):     737.63 ; \t loss: 0.3900\n",
      "Total trained pairs (M):     738.10 ; \t loss: 0.3874\n",
      "Total trained pairs (M):     738.58 ; \t loss: 0.3891\n",
      "Total trained pairs (M):     739.06 ; \t loss: 0.3880\n",
      "Total trained pairs (M):     739.55 ; \t loss: 0.3871\n",
      "Total trained pairs (M):     740.05 ; \t loss: 0.3878\n",
      "Total trained pairs (M):     740.55 ; \t loss: 0.3876\n",
      "Total trained pairs (M):     741.06 ; \t loss: 0.3861\n",
      "Total trained pairs (M):     741.57 ; \t loss: 0.3871\n",
      "Total trained pairs (M):     742.08 ; \t loss: 0.3871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trained pairs (M):     742.59 ; \t loss: 0.3849\n",
      "Total trained pairs (M):     743.10 ; \t loss: 0.3862\n",
      "Total trained pairs (M):     743.61 ; \t loss: 0.3851\n",
      "Total trained pairs (M):     744.12 ; \t loss: 0.3830\n",
      "Total trained pairs (M):     744.63 ; \t loss: 0.3811\n",
      "Total trained pairs (M):     745.15 ; \t loss: 0.3811\n",
      "Total trained pairs (M):     745.67 ; \t loss: 0.3785\n",
      "Total trained pairs (M):     746.18 ; \t loss: 0.3772\n",
      "Total trained pairs (M):     746.70 ; \t loss: 0.3758\n",
      "Total trained pairs (M):     747.21 ; \t loss: 0.3778\n",
      "Total trained pairs (M):     747.71 ; \t loss: 0.3752\n",
      "Total trained pairs (M):     748.23 ; \t loss: 0.3771\n",
      "Total trained pairs (M):     748.73 ; \t loss: 0.3776\n",
      "Total trained pairs (M):     749.23 ; \t loss: 0.3767\n",
      "Total trained pairs (M):     749.73 ; \t loss: 0.3767\n",
      "Total trained pairs (M):     750.22 ; \t loss: 0.3750\n",
      "Total trained pairs (M):     750.72 ; \t loss: 0.3748\n",
      "Total trained pairs (M):     751.21 ; \t loss: 0.3742\n",
      "Total trained pairs (M):     751.69 ; \t loss: 0.3776\n",
      "Total trained pairs (M):     752.17 ; \t loss: 0.3822\n",
      "Total trained pairs (M):     752.64 ; \t loss: 0.3823\n",
      "Total trained pairs (M):     753.09 ; \t loss: 0.3832\n",
      "Total trained pairs (M):     753.54 ; \t loss: 0.3825\n",
      "Total trained pairs (M):     753.99 ; \t loss: 0.3796\n",
      "Total trained pairs (M):     754.44 ; \t loss: 0.3788\n",
      "Total trained pairs (M):     754.89 ; \t loss: 0.3792\n",
      "Total trained pairs (M):     755.34 ; \t loss: 0.3779\n",
      "Total trained pairs (M):     755.80 ; \t loss: 0.3778\n",
      "Total trained pairs (M):     756.26 ; \t loss: 0.3791\n",
      "Total trained pairs (M):     756.72 ; \t loss: 0.3776\n",
      "Total trained pairs (M):     757.18 ; \t loss: 0.3777\n",
      "Total trained pairs (M):     757.65 ; \t loss: 0.3799\n",
      "Total trained pairs (M):     758.11 ; \t loss: 0.3792\n",
      "Total trained pairs (M):     758.57 ; \t loss: 0.3771\n",
      "Total trained pairs (M):     759.04 ; \t loss: 0.3775\n",
      "Total trained pairs (M):     759.50 ; \t loss: 0.3782\n",
      "Total trained pairs (M):     759.97 ; \t loss: 0.3804\n",
      "Total trained pairs (M):     760.45 ; \t loss: 0.3807\n",
      "Total trained pairs (M):     760.92 ; \t loss: 0.3796\n",
      "Total trained pairs (M):     761.40 ; \t loss: 0.3786\n",
      "Total trained pairs (M):     761.87 ; \t loss: 0.3790\n",
      "Total trained pairs (M):     762.34 ; \t loss: 0.3767\n",
      "Total trained pairs (M):     762.82 ; \t loss: 0.3791\n",
      "Total trained pairs (M):     763.30 ; \t loss: 0.3778\n",
      "Total trained pairs (M):     763.77 ; \t loss: 0.3752\n",
      "Total trained pairs (M):     764.25 ; \t loss: 0.3739\n",
      "Total trained pairs (M):     764.73 ; \t loss: 0.3763\n",
      "Total trained pairs (M):     765.20 ; \t loss: 0.3769\n",
      "Total trained pairs (M):     765.67 ; \t loss: 0.3756\n",
      "Total trained pairs (M):     766.15 ; \t loss: 0.3767\n",
      "Total trained pairs (M):     766.62 ; \t loss: 0.3746\n",
      "Total trained pairs (M):     767.09 ; \t loss: 0.3739\n",
      "Total trained pairs (M):     767.57 ; \t loss: 0.3751\n",
      "Total trained pairs (M):     768.05 ; \t loss: 0.3752\n",
      "Total trained pairs (M):     768.52 ; \t loss: 0.3758\n",
      "Total trained pairs (M):     768.99 ; \t loss: 0.3760\n",
      "Total trained pairs (M):     769.45 ; \t loss: 0.3744\n",
      "Total trained pairs (M):     769.92 ; \t loss: 0.3817\n",
      "Total trained pairs (M):     770.39 ; \t loss: 0.3828\n",
      "Total trained pairs (M):     770.86 ; \t loss: 0.3803\n",
      "Total trained pairs (M):     771.33 ; \t loss: 0.3786\n",
      "Total trained pairs (M):     771.80 ; \t loss: 0.3812\n",
      "Total trained pairs (M):     772.27 ; \t loss: 0.3805\n",
      "Total trained pairs (M):     772.75 ; \t loss: 0.3807\n",
      "Total trained pairs (M):     773.22 ; \t loss: 0.3808\n",
      "Total trained pairs (M):     773.69 ; \t loss: 0.3792\n",
      "Total trained pairs (M):     774.16 ; \t loss: 0.3809\n",
      "Total trained pairs (M):     774.62 ; \t loss: 0.3807\n",
      "Total trained pairs (M):     775.10 ; \t loss: 0.3797\n",
      "Total trained pairs (M):     775.58 ; \t loss: 0.3816\n",
      "Total trained pairs (M):     776.06 ; \t loss: 0.3818\n",
      "Total trained pairs (M):     776.51 ; \t loss: 0.3836\n",
      "Total trained pairs (M):     776.96 ; \t loss: 0.3858\n",
      "Total trained pairs (M):     777.42 ; \t loss: 0.3916\n",
      "Total trained pairs (M):     777.88 ; \t loss: 0.3887\n",
      "Total trained pairs (M):     778.33 ; \t loss: 0.3901\n",
      "Total trained pairs (M):     778.78 ; \t loss: 0.3882\n",
      "Total trained pairs (M):     779.24 ; \t loss: 0.3868\n",
      "Total trained pairs (M):     779.69 ; \t loss: 0.3847\n",
      "Total trained pairs (M):     780.15 ; \t loss: 0.3829\n",
      "Total trained pairs (M):     780.61 ; \t loss: 0.3819\n",
      "Total trained pairs (M):     781.06 ; \t loss: 0.3818\n",
      "Total trained pairs (M):     781.52 ; \t loss: 0.3806\n",
      "Total trained pairs (M):     781.98 ; \t loss: 0.3842\n",
      "Total trained pairs (M):     782.44 ; \t loss: 0.3830\n",
      "Total trained pairs (M):     782.89 ; \t loss: 0.3814\n",
      "Total trained pairs (M):     783.35 ; \t loss: 0.3812\n",
      "Total trained pairs (M):     783.80 ; \t loss: 0.3843\n",
      "Total trained pairs (M):     784.25 ; \t loss: 0.3849\n",
      "Total trained pairs (M):     784.70 ; \t loss: 0.3846\n",
      "Total trained pairs (M):     785.14 ; \t loss: 0.3872\n",
      "Total trained pairs (M):     785.58 ; \t loss: 0.3889\n",
      "Total trained pairs (M):     786.02 ; \t loss: 0.3876\n",
      "Total trained pairs (M):     786.45 ; \t loss: 0.3884\n",
      "Total trained pairs (M):     786.87 ; \t loss: 0.3913\n",
      "Total trained pairs (M):     787.29 ; \t loss: 0.3918\n",
      "Total trained pairs (M):     787.70 ; \t loss: 0.3924\n",
      "Total trained pairs (M):     788.11 ; \t loss: 0.3910\n",
      "Total trained pairs (M):     788.51 ; \t loss: 0.3904\n",
      "Total trained pairs (M):     788.92 ; \t loss: 0.3938\n",
      "Total trained pairs (M):     789.32 ; \t loss: 0.3919\n",
      "Total trained pairs (M):     789.73 ; \t loss: 0.3912\n",
      "Total trained pairs (M):     790.13 ; \t loss: 0.3892\n",
      "Total trained pairs (M):     790.54 ; \t loss: 0.3898\n",
      "Total trained pairs (M):     790.94 ; \t loss: 0.3866\n",
      "Total trained pairs (M):     791.35 ; \t loss: 0.3892\n",
      "Total trained pairs (M):     791.75 ; \t loss: 0.3910\n",
      "Total trained pairs (M):     792.16 ; \t loss: 0.3901\n",
      "Total trained pairs (M):     792.57 ; \t loss: 0.3894\n",
      "Total trained pairs (M):     792.98 ; \t loss: 0.3881\n",
      "Total trained pairs (M):     793.40 ; \t loss: 0.3869\n",
      "Total trained pairs (M):     793.81 ; \t loss: 0.3934\n",
      "Total trained pairs (M):     794.22 ; \t loss: 0.3936\n",
      "Total trained pairs (M):     794.63 ; \t loss: 0.3993\n",
      "Total trained pairs (M):     795.05 ; \t loss: 0.4001\n",
      "Total trained pairs (M):     795.47 ; \t loss: 0.3982\n",
      "Total trained pairs (M):     795.89 ; \t loss: 0.3990\n",
      "Total trained pairs (M):     796.31 ; \t loss: 0.4014\n",
      "Total trained pairs (M):     796.74 ; \t loss: 0.4036\n",
      "Total trained pairs (M):     797.16 ; \t loss: 0.4054\n",
      "Total trained pairs (M):     797.58 ; \t loss: 0.4045\n",
      "Total trained pairs (M):     798.01 ; \t loss: 0.4033\n",
      "Total trained pairs (M):     798.44 ; \t loss: 0.4049\n",
      "Total trained pairs (M):     798.87 ; \t loss: 0.4041\n",
      "Total trained pairs (M):     799.31 ; \t loss: 0.4064\n",
      "Total trained pairs (M):     799.76 ; \t loss: 0.4105\n",
      "Total trained pairs (M):     800.21 ; \t loss: 0.4090\n",
      "Total trained pairs (M):     800.66 ; \t loss: 0.4065\n",
      "Total trained pairs (M):     801.10 ; \t loss: 0.4097\n",
      "Total trained pairs (M):     801.54 ; \t loss: 0.4078\n",
      "Total trained pairs (M):     801.99 ; \t loss: 0.4197\n",
      "Total trained pairs (M):     802.44 ; \t loss: 0.4236\n",
      "Total trained pairs (M):     802.89 ; \t loss: 0.4239\n",
      "Total trained pairs (M):     803.34 ; \t loss: 0.4222\n",
      "Total trained pairs (M):     803.78 ; \t loss: 0.4230\n",
      "Total trained pairs (M):     804.21 ; \t loss: 0.4228\n",
      "Total trained pairs (M):     804.65 ; \t loss: 0.4254\n",
      "Total trained pairs (M):     805.08 ; \t loss: 0.4254\n",
      "Total trained pairs (M):     805.51 ; \t loss: 0.4246\n",
      "Total trained pairs (M):     805.93 ; \t loss: 0.4250\n",
      "Total trained pairs (M):     806.35 ; \t loss: 0.4279\n",
      "Total trained pairs (M):     806.79 ; \t loss: 0.4277\n",
      "Total trained pairs (M):     807.23 ; \t loss: 0.4357\n",
      "Total trained pairs (M):     807.67 ; \t loss: 0.4365\n",
      "Total trained pairs (M):     808.12 ; \t loss: 0.4363\n",
      "Total trained pairs (M):     808.57 ; \t loss: 0.4352\n",
      "Epoch 4 ======\n",
      "Total trained pairs (M):     809.08 ; \t loss: 0.5742\n",
      "Total trained pairs (M):     809.61 ; \t loss: 0.5646\n",
      "Total trained pairs (M):     810.13 ; \t loss: 0.5525\n",
      "Total trained pairs (M):     810.65 ; \t loss: 0.5451\n",
      "Total trained pairs (M):     811.16 ; \t loss: 0.5381\n",
      "Total trained pairs (M):     811.67 ; \t loss: 0.5322\n",
      "Total trained pairs (M):     812.19 ; \t loss: 0.5281\n",
      "Total trained pairs (M):     812.69 ; \t loss: 0.5241\n",
      "Total trained pairs (M):     813.19 ; \t loss: 0.5179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trained pairs (M):     813.69 ; \t loss: 0.5109\n",
      "Total trained pairs (M):     814.19 ; \t loss: 0.5075\n",
      "Total trained pairs (M):     814.69 ; \t loss: 0.5041\n",
      "Total trained pairs (M):     815.19 ; \t loss: 0.5005\n",
      "Total trained pairs (M):     815.69 ; \t loss: 0.4979\n",
      "Total trained pairs (M):     816.19 ; \t loss: 0.4923\n",
      "Total trained pairs (M):     816.67 ; \t loss: 0.4906\n",
      "Total trained pairs (M):     817.15 ; \t loss: 0.4885\n",
      "Total trained pairs (M):     817.63 ; \t loss: 0.4846\n",
      "Total trained pairs (M):     818.11 ; \t loss: 0.4821\n",
      "Total trained pairs (M):     818.59 ; \t loss: 0.4780\n",
      "Total trained pairs (M):     819.07 ; \t loss: 0.4733\n",
      "Total trained pairs (M):     819.55 ; \t loss: 0.4714\n",
      "Total trained pairs (M):     820.02 ; \t loss: 0.4689\n",
      "Total trained pairs (M):     820.50 ; \t loss: 0.4674\n",
      "Total trained pairs (M):     820.98 ; \t loss: 0.4641\n",
      "Total trained pairs (M):     821.47 ; \t loss: 0.4598\n",
      "Total trained pairs (M):     821.95 ; \t loss: 0.4557\n",
      "Total trained pairs (M):     822.41 ; \t loss: 0.4472\n",
      "Total trained pairs (M):     822.88 ; \t loss: 0.4453\n",
      "Total trained pairs (M):     823.35 ; \t loss: 0.4432\n",
      "Total trained pairs (M):     823.83 ; \t loss: 0.4412\n",
      "Total trained pairs (M):     824.30 ; \t loss: 0.4396\n",
      "Total trained pairs (M):     824.78 ; \t loss: 0.4388\n",
      "Total trained pairs (M):     825.27 ; \t loss: 0.4360\n",
      "Total trained pairs (M):     825.76 ; \t loss: 0.4362\n",
      "Total trained pairs (M):     826.24 ; \t loss: 0.4328\n",
      "Total trained pairs (M):     826.72 ; \t loss: 0.4311\n",
      "Total trained pairs (M):     827.20 ; \t loss: 0.4284\n",
      "Total trained pairs (M):     827.67 ; \t loss: 0.4263\n",
      "Total trained pairs (M):     828.14 ; \t loss: 0.4242\n",
      "Total trained pairs (M):     828.63 ; \t loss: 0.4217\n",
      "Total trained pairs (M):     829.10 ; \t loss: 0.4142\n",
      "Total trained pairs (M):     829.58 ; \t loss: 0.4134\n",
      "Total trained pairs (M):     830.06 ; \t loss: 0.4113\n",
      "Total trained pairs (M):     830.55 ; \t loss: 0.4082\n",
      "Total trained pairs (M):     831.03 ; \t loss: 0.4076\n",
      "Total trained pairs (M):     831.51 ; \t loss: 0.4046\n",
      "Total trained pairs (M):     831.99 ; \t loss: 0.4038\n",
      "Total trained pairs (M):     832.47 ; \t loss: 0.4018\n",
      "Total trained pairs (M):     832.95 ; \t loss: 0.3988\n",
      "Total trained pairs (M):     833.44 ; \t loss: 0.3983\n",
      "Total trained pairs (M):     833.93 ; \t loss: 0.3966\n",
      "Total trained pairs (M):     834.42 ; \t loss: 0.3950\n",
      "Total trained pairs (M):     834.91 ; \t loss: 0.3924\n",
      "Total trained pairs (M):     835.40 ; \t loss: 0.3911\n",
      "Total trained pairs (M):     835.89 ; \t loss: 0.3878\n",
      "Total trained pairs (M):     836.41 ; \t loss: 0.3907\n",
      "Total trained pairs (M):     836.94 ; \t loss: 0.3908\n",
      "Total trained pairs (M):     837.46 ; \t loss: 0.3899\n",
      "Total trained pairs (M):     837.98 ; \t loss: 0.3881\n",
      "Total trained pairs (M):     838.50 ; \t loss: 0.3877\n",
      "Total trained pairs (M):     839.01 ; \t loss: 0.3878\n",
      "Total trained pairs (M):     839.52 ; \t loss: 0.3848\n",
      "Total trained pairs (M):     840.03 ; \t loss: 0.3827\n",
      "Total trained pairs (M):     840.52 ; \t loss: 0.3786\n",
      "Total trained pairs (M):     841.01 ; \t loss: 0.3775\n",
      "Total trained pairs (M):     841.49 ; \t loss: 0.3751\n",
      "Total trained pairs (M):     841.98 ; \t loss: 0.3734\n",
      "Total trained pairs (M):     842.47 ; \t loss: 0.3722\n",
      "Total trained pairs (M):     842.96 ; \t loss: 0.3726\n",
      "Total trained pairs (M):     843.45 ; \t loss: 0.3704\n",
      "Total trained pairs (M):     843.94 ; \t loss: 0.3744\n",
      "Total trained pairs (M):     844.44 ; \t loss: 0.3722\n",
      "Total trained pairs (M):     844.93 ; \t loss: 0.3704\n",
      "Total trained pairs (M):     845.42 ; \t loss: 0.3689\n",
      "Total trained pairs (M):     845.92 ; \t loss: 0.3699\n",
      "Total trained pairs (M):     846.43 ; \t loss: 0.3723\n",
      "Total trained pairs (M):     846.92 ; \t loss: 0.3721\n",
      "Total trained pairs (M):     847.41 ; \t loss: 0.3720\n",
      "Total trained pairs (M):     847.92 ; \t loss: 0.3743\n",
      "Total trained pairs (M):     848.43 ; \t loss: 0.3726\n",
      "Total trained pairs (M):     848.95 ; \t loss: 0.3731\n",
      "Total trained pairs (M):     849.47 ; \t loss: 0.3732\n",
      "Total trained pairs (M):     849.98 ; \t loss: 0.3714\n",
      "Total trained pairs (M):     850.50 ; \t loss: 0.3695\n",
      "Total trained pairs (M):     851.01 ; \t loss: 0.3689\n",
      "Total trained pairs (M):     851.54 ; \t loss: 0.3743\n",
      "Total trained pairs (M):     852.08 ; \t loss: 0.3746\n",
      "Total trained pairs (M):     852.62 ; \t loss: 0.3746\n",
      "Total trained pairs (M):     853.16 ; \t loss: 0.3737\n",
      "Total trained pairs (M):     853.70 ; \t loss: 0.3726\n",
      "Total trained pairs (M):     854.26 ; \t loss: 0.3732\n",
      "Total trained pairs (M):     854.81 ; \t loss: 0.3712\n",
      "Total trained pairs (M):     855.37 ; \t loss: 0.3706\n",
      "Total trained pairs (M):     855.92 ; \t loss: 0.3706\n",
      "Total trained pairs (M):     856.48 ; \t loss: 0.3701\n",
      "Total trained pairs (M):     857.04 ; \t loss: 0.3707\n",
      "Total trained pairs (M):     857.59 ; \t loss: 0.3682\n",
      "Total trained pairs (M):     858.15 ; \t loss: 0.3666\n",
      "Total trained pairs (M):     858.66 ; \t loss: 0.3687\n",
      "Total trained pairs (M):     859.18 ; \t loss: 0.3680\n",
      "Total trained pairs (M):     859.70 ; \t loss: 0.3704\n",
      "Total trained pairs (M):     860.21 ; \t loss: 0.3696\n",
      "Total trained pairs (M):     860.72 ; \t loss: 0.3683\n",
      "Total trained pairs (M):     861.24 ; \t loss: 0.3681\n",
      "Total trained pairs (M):     861.75 ; \t loss: 0.3673\n",
      "Total trained pairs (M):     862.26 ; \t loss: 0.3668\n",
      "Total trained pairs (M):     862.77 ; \t loss: 0.3647\n",
      "Total trained pairs (M):     863.28 ; \t loss: 0.3654\n",
      "Total trained pairs (M):     863.78 ; \t loss: 0.3655\n",
      "Total trained pairs (M):     864.29 ; \t loss: 0.3664\n",
      "Total trained pairs (M):     864.80 ; \t loss: 0.3650\n",
      "Total trained pairs (M):     865.31 ; \t loss: 0.3635\n",
      "Total trained pairs (M):     865.83 ; \t loss: 0.3656\n",
      "Total trained pairs (M):     866.34 ; \t loss: 0.3634\n",
      "Total trained pairs (M):     866.85 ; \t loss: 0.3648\n",
      "Total trained pairs (M):     867.37 ; \t loss: 0.3643\n",
      "Total trained pairs (M):     867.90 ; \t loss: 0.3643\n",
      "Total trained pairs (M):     868.42 ; \t loss: 0.3632\n",
      "Total trained pairs (M):     868.95 ; \t loss: 0.3654\n",
      "Total trained pairs (M):     869.48 ; \t loss: 0.3644\n",
      "Total trained pairs (M):     870.01 ; \t loss: 0.3624\n",
      "Total trained pairs (M):     870.53 ; \t loss: 0.3604\n",
      "Total trained pairs (M):     871.06 ; \t loss: 0.3595\n",
      "Total trained pairs (M):     871.60 ; \t loss: 0.3585\n",
      "Total trained pairs (M):     872.12 ; \t loss: 0.3612\n",
      "Total trained pairs (M):     872.64 ; \t loss: 0.3613\n",
      "Total trained pairs (M):     873.17 ; \t loss: 0.3595\n",
      "Total trained pairs (M):     873.69 ; \t loss: 0.3585\n",
      "Total trained pairs (M):     874.21 ; \t loss: 0.3597\n",
      "Total trained pairs (M):     874.74 ; \t loss: 0.3595\n",
      "Total trained pairs (M):     875.26 ; \t loss: 0.3588\n",
      "Total trained pairs (M):     875.77 ; \t loss: 0.3567\n",
      "Total trained pairs (M):     876.29 ; \t loss: 0.3571\n",
      "Total trained pairs (M):     876.80 ; \t loss: 0.3571\n",
      "Total trained pairs (M):     877.31 ; \t loss: 0.3555\n",
      "Total trained pairs (M):     877.83 ; \t loss: 0.3538\n",
      "Total trained pairs (M):     878.35 ; \t loss: 0.3558\n",
      "Total trained pairs (M):     878.88 ; \t loss: 0.3546\n",
      "Total trained pairs (M):     879.41 ; \t loss: 0.3528\n",
      "Total trained pairs (M):     879.94 ; \t loss: 0.3539\n",
      "Total trained pairs (M):     880.46 ; \t loss: 0.3549\n",
      "Total trained pairs (M):     880.99 ; \t loss: 0.3592\n",
      "Total trained pairs (M):     881.53 ; \t loss: 0.3582\n",
      "Total trained pairs (M):     882.06 ; \t loss: 0.3601\n",
      "Total trained pairs (M):     882.59 ; \t loss: 0.3584\n",
      "Total trained pairs (M):     883.13 ; \t loss: 0.3576\n",
      "Total trained pairs (M):     883.67 ; \t loss: 0.3558\n",
      "Total trained pairs (M):     884.20 ; \t loss: 0.3534\n",
      "Total trained pairs (M):     884.74 ; \t loss: 0.3513\n",
      "Total trained pairs (M):     885.26 ; \t loss: 0.3528\n",
      "Total trained pairs (M):     885.79 ; \t loss: 0.3514\n",
      "Total trained pairs (M):     886.32 ; \t loss: 0.3512\n",
      "Total trained pairs (M):     886.85 ; \t loss: 0.3492\n",
      "Total trained pairs (M):     887.38 ; \t loss: 0.3486\n",
      "Total trained pairs (M):     887.90 ; \t loss: 0.3509\n",
      "Total trained pairs (M):     888.40 ; \t loss: 0.3522\n",
      "Total trained pairs (M):     888.89 ; \t loss: 0.3512\n",
      "Total trained pairs (M):     889.38 ; \t loss: 0.3488\n",
      "Total trained pairs (M):     889.87 ; \t loss: 0.3486\n",
      "Total trained pairs (M):     890.36 ; \t loss: 0.3481\n",
      "Total trained pairs (M):     890.84 ; \t loss: 0.3467\n",
      "Total trained pairs (M):     891.34 ; \t loss: 0.3462\n",
      "Total trained pairs (M):     891.85 ; \t loss: 0.3488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trained pairs (M):     892.36 ; \t loss: 0.3468\n",
      "Total trained pairs (M):     892.88 ; \t loss: 0.3447\n",
      "Total trained pairs (M):     893.40 ; \t loss: 0.3470\n",
      "Total trained pairs (M):     893.91 ; \t loss: 0.3461\n",
      "Total trained pairs (M):     894.43 ; \t loss: 0.3449\n",
      "Total trained pairs (M):     894.95 ; \t loss: 0.3444\n",
      "Total trained pairs (M):     895.46 ; \t loss: 0.3431\n",
      "Total trained pairs (M):     895.97 ; \t loss: 0.3445\n",
      "Total trained pairs (M):     896.47 ; \t loss: 0.3424\n",
      "Total trained pairs (M):     896.98 ; \t loss: 0.3430\n",
      "Total trained pairs (M):     897.48 ; \t loss: 0.3404\n",
      "Total trained pairs (M):     897.97 ; \t loss: 0.3420\n",
      "Total trained pairs (M):     898.45 ; \t loss: 0.3412\n",
      "Total trained pairs (M):     898.93 ; \t loss: 0.3395\n",
      "Total trained pairs (M):     899.41 ; \t loss: 0.3386\n",
      "Total trained pairs (M):     899.87 ; \t loss: 0.3414\n",
      "Total trained pairs (M):     900.33 ; \t loss: 0.3406\n",
      "Total trained pairs (M):     900.78 ; \t loss: 0.3392\n",
      "Total trained pairs (M):     901.22 ; \t loss: 0.3395\n",
      "Total trained pairs (M):     901.67 ; \t loss: 0.3398\n",
      "Total trained pairs (M):     902.11 ; \t loss: 0.3390\n",
      "Total trained pairs (M):     902.55 ; \t loss: 0.3382\n",
      "Total trained pairs (M):     902.97 ; \t loss: 0.3383\n",
      "Total trained pairs (M):     903.40 ; \t loss: 0.3407\n",
      "Total trained pairs (M):     903.82 ; \t loss: 0.3383\n",
      "Total trained pairs (M):     904.24 ; \t loss: 0.3391\n",
      "Total trained pairs (M):     904.67 ; \t loss: 0.3439\n",
      "Total trained pairs (M):     905.09 ; \t loss: 0.3436\n",
      "Total trained pairs (M):     905.51 ; \t loss: 0.3473\n",
      "Total trained pairs (M):     905.93 ; \t loss: 0.3466\n",
      "Total trained pairs (M):     906.35 ; \t loss: 0.3466\n",
      "Total trained pairs (M):     906.74 ; \t loss: 0.3498\n",
      "Total trained pairs (M):     907.16 ; \t loss: 0.3509\n",
      "Total trained pairs (M):     907.59 ; \t loss: 0.3503\n",
      "Total trained pairs (M):     908.03 ; \t loss: 0.3578\n",
      "Total trained pairs (M):     908.48 ; \t loss: 0.3621\n",
      "Total trained pairs (M):     908.94 ; \t loss: 0.3627\n",
      "Total trained pairs (M):     909.39 ; \t loss: 0.3628\n",
      "Total trained pairs (M):     909.84 ; \t loss: 0.3614\n",
      "Total trained pairs (M):     910.28 ; \t loss: 0.3603\n",
      "Total trained pairs (M):     910.73 ; \t loss: 0.3583\n",
      "Total trained pairs (M):     911.17 ; \t loss: 0.3586\n",
      "Total trained pairs (M):     911.61 ; \t loss: 0.3576\n",
      "Total trained pairs (M):     912.06 ; \t loss: 0.3564\n",
      "Total trained pairs (M):     912.51 ; \t loss: 0.3567\n",
      "Total trained pairs (M):     912.96 ; \t loss: 0.3559\n",
      "Total trained pairs (M):     913.40 ; \t loss: 0.3567\n",
      "Total trained pairs (M):     913.86 ; \t loss: 0.3556\n",
      "Total trained pairs (M):     914.31 ; \t loss: 0.3539\n",
      "Total trained pairs (M):     914.75 ; \t loss: 0.3569\n",
      "Total trained pairs (M):     915.20 ; \t loss: 0.3547\n",
      "Total trained pairs (M):     915.65 ; \t loss: 0.3548\n",
      "Total trained pairs (M):     916.09 ; \t loss: 0.3541\n",
      "Total trained pairs (M):     916.52 ; \t loss: 0.3552\n",
      "Total trained pairs (M):     916.97 ; \t loss: 0.3554\n",
      "Total trained pairs (M):     917.41 ; \t loss: 0.3560\n",
      "Total trained pairs (M):     917.85 ; \t loss: 0.3538\n",
      "Total trained pairs (M):     918.29 ; \t loss: 0.3518\n",
      "Total trained pairs (M):     918.73 ; \t loss: 0.3508\n",
      "Total trained pairs (M):     919.17 ; \t loss: 0.3510\n",
      "Total trained pairs (M):     919.61 ; \t loss: 0.3543\n",
      "Total trained pairs (M):     920.05 ; \t loss: 0.3532\n",
      "Total trained pairs (M):     920.50 ; \t loss: 0.3534\n",
      "Total trained pairs (M):     920.94 ; \t loss: 0.3527\n",
      "Total trained pairs (M):     921.39 ; \t loss: 0.3509\n",
      "Total trained pairs (M):     921.83 ; \t loss: 0.3535\n",
      "Total trained pairs (M):     922.27 ; \t loss: 0.3518\n",
      "Total trained pairs (M):     922.72 ; \t loss: 0.3521\n",
      "Total trained pairs (M):     923.17 ; \t loss: 0.3532\n",
      "Total trained pairs (M):     923.63 ; \t loss: 0.3573\n",
      "Total trained pairs (M):     924.08 ; \t loss: 0.3561\n",
      "Total trained pairs (M):     924.55 ; \t loss: 0.3556\n",
      "Total trained pairs (M):     925.01 ; \t loss: 0.3582\n",
      "Total trained pairs (M):     925.47 ; \t loss: 0.3569\n",
      "Total trained pairs (M):     925.93 ; \t loss: 0.3571\n",
      "Total trained pairs (M):     926.39 ; \t loss: 0.3557\n",
      "Total trained pairs (M):     926.84 ; \t loss: 0.3553\n",
      "Total trained pairs (M):     927.29 ; \t loss: 0.3548\n",
      "Total trained pairs (M):     927.72 ; \t loss: 0.3581\n",
      "Total trained pairs (M):     928.16 ; \t loss: 0.3585\n",
      "Total trained pairs (M):     928.60 ; \t loss: 0.3594\n",
      "Total trained pairs (M):     929.03 ; \t loss: 0.3588\n",
      "Total trained pairs (M):     929.47 ; \t loss: 0.3568\n",
      "Total trained pairs (M):     929.91 ; \t loss: 0.3568\n",
      "Total trained pairs (M):     930.35 ; \t loss: 0.3557\n",
      "Total trained pairs (M):     930.82 ; \t loss: 0.3711\n",
      "Total trained pairs (M):     931.29 ; \t loss: 0.3696\n",
      "Total trained pairs (M):     931.77 ; \t loss: 0.3677\n",
      "Total trained pairs (M):     932.24 ; \t loss: 0.3682\n",
      "Total trained pairs (M):     932.72 ; \t loss: 0.3696\n",
      "Total trained pairs (M):     933.20 ; \t loss: 0.3681\n",
      "Total trained pairs (M):     933.67 ; \t loss: 0.3671\n",
      "Total trained pairs (M):     934.15 ; \t loss: 0.3657\n",
      "Total trained pairs (M):     934.62 ; \t loss: 0.3651\n",
      "Total trained pairs (M):     935.09 ; \t loss: 0.3624\n",
      "Total trained pairs (M):     935.57 ; \t loss: 0.3626\n",
      "Total trained pairs (M):     936.04 ; \t loss: 0.3611\n",
      "Total trained pairs (M):     936.53 ; \t loss: 0.3602\n",
      "Total trained pairs (M):     937.00 ; \t loss: 0.3623\n",
      "Total trained pairs (M):     937.45 ; \t loss: 0.3645\n",
      "Total trained pairs (M):     937.91 ; \t loss: 0.3627\n",
      "Total trained pairs (M):     938.37 ; \t loss: 0.3601\n",
      "Total trained pairs (M):     938.84 ; \t loss: 0.3637\n",
      "Total trained pairs (M):     939.30 ; \t loss: 0.3636\n",
      "Total trained pairs (M):     939.77 ; \t loss: 0.3629\n",
      "Total trained pairs (M):     940.24 ; \t loss: 0.3600\n",
      "Total trained pairs (M):     940.72 ; \t loss: 0.3607\n",
      "Total trained pairs (M):     941.20 ; \t loss: 0.3620\n",
      "Total trained pairs (M):     941.69 ; \t loss: 0.3603\n",
      "Total trained pairs (M):     942.19 ; \t loss: 0.3599\n",
      "Total trained pairs (M):     942.69 ; \t loss: 0.3617\n",
      "Total trained pairs (M):     943.20 ; \t loss: 0.3589\n",
      "Total trained pairs (M):     943.71 ; \t loss: 0.3622\n",
      "Total trained pairs (M):     944.22 ; \t loss: 0.3610\n",
      "Total trained pairs (M):     944.73 ; \t loss: 0.3590\n",
      "Total trained pairs (M):     945.25 ; \t loss: 0.3594\n",
      "Total trained pairs (M):     945.76 ; \t loss: 0.3569\n",
      "Total trained pairs (M):     946.27 ; \t loss: 0.3556\n",
      "Total trained pairs (M):     946.78 ; \t loss: 0.3546\n",
      "Total trained pairs (M):     947.29 ; \t loss: 0.3552\n",
      "Total trained pairs (M):     947.81 ; \t loss: 0.3537\n",
      "Total trained pairs (M):     948.33 ; \t loss: 0.3515\n",
      "Total trained pairs (M):     948.84 ; \t loss: 0.3495\n",
      "Total trained pairs (M):     949.35 ; \t loss: 0.3510\n",
      "Total trained pairs (M):     949.86 ; \t loss: 0.3496\n",
      "Total trained pairs (M):     950.37 ; \t loss: 0.3509\n",
      "Total trained pairs (M):     950.87 ; \t loss: 0.3518\n",
      "Total trained pairs (M):     951.37 ; \t loss: 0.3519\n",
      "Total trained pairs (M):     951.87 ; \t loss: 0.3500\n",
      "Total trained pairs (M):     952.36 ; \t loss: 0.3493\n",
      "Total trained pairs (M):     952.86 ; \t loss: 0.3484\n",
      "Total trained pairs (M):     953.36 ; \t loss: 0.3489\n",
      "Total trained pairs (M):     953.83 ; \t loss: 0.3524\n",
      "Total trained pairs (M):     954.32 ; \t loss: 0.3562\n",
      "Total trained pairs (M):     954.78 ; \t loss: 0.3565\n",
      "Total trained pairs (M):     955.23 ; \t loss: 0.3565\n",
      "Total trained pairs (M):     955.68 ; \t loss: 0.3568\n",
      "Total trained pairs (M):     956.13 ; \t loss: 0.3549\n",
      "Total trained pairs (M):     956.58 ; \t loss: 0.3540\n",
      "Total trained pairs (M):     957.03 ; \t loss: 0.3543\n",
      "Total trained pairs (M):     957.49 ; \t loss: 0.3533\n",
      "Total trained pairs (M):     957.94 ; \t loss: 0.3528\n",
      "Total trained pairs (M):     958.40 ; \t loss: 0.3528\n",
      "Total trained pairs (M):     958.86 ; \t loss: 0.3520\n",
      "Total trained pairs (M):     959.32 ; \t loss: 0.3531\n",
      "Total trained pairs (M):     959.79 ; \t loss: 0.3545\n",
      "Total trained pairs (M):     960.25 ; \t loss: 0.3539\n",
      "Total trained pairs (M):     960.71 ; \t loss: 0.3513\n",
      "Total trained pairs (M):     961.18 ; \t loss: 0.3529\n",
      "Total trained pairs (M):     961.64 ; \t loss: 0.3516\n",
      "Total trained pairs (M):     962.11 ; \t loss: 0.3557\n",
      "Total trained pairs (M):     962.59 ; \t loss: 0.3556\n",
      "Total trained pairs (M):     963.06 ; \t loss: 0.3553\n",
      "Total trained pairs (M):     963.54 ; \t loss: 0.3540\n",
      "Total trained pairs (M):     964.01 ; \t loss: 0.3528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trained pairs (M):     964.48 ; \t loss: 0.3536\n",
      "Total trained pairs (M):     964.96 ; \t loss: 0.3528\n",
      "Total trained pairs (M):     965.44 ; \t loss: 0.3532\n",
      "Total trained pairs (M):     965.91 ; \t loss: 0.3523\n",
      "Total trained pairs (M):     966.39 ; \t loss: 0.3493\n",
      "Total trained pairs (M):     966.87 ; \t loss: 0.3523\n",
      "Total trained pairs (M):     967.34 ; \t loss: 0.3523\n",
      "Total trained pairs (M):     967.81 ; \t loss: 0.3517\n",
      "Total trained pairs (M):     968.29 ; \t loss: 0.3511\n",
      "Total trained pairs (M):     968.76 ; \t loss: 0.3497\n",
      "Total trained pairs (M):     969.23 ; \t loss: 0.3491\n",
      "Total trained pairs (M):     969.72 ; \t loss: 0.3512\n",
      "Total trained pairs (M):     970.19 ; \t loss: 0.3516\n",
      "Total trained pairs (M):     970.67 ; \t loss: 0.3510\n",
      "Total trained pairs (M):     971.13 ; \t loss: 0.3504\n",
      "Total trained pairs (M):     971.59 ; \t loss: 0.3503\n",
      "Total trained pairs (M):     972.06 ; \t loss: 0.3586\n",
      "Total trained pairs (M):     972.53 ; \t loss: 0.3583\n",
      "Total trained pairs (M):     973.01 ; \t loss: 0.3579\n",
      "Total trained pairs (M):     973.48 ; \t loss: 0.3560\n",
      "Total trained pairs (M):     973.95 ; \t loss: 0.3582\n",
      "Total trained pairs (M):     974.42 ; \t loss: 0.3574\n",
      "Total trained pairs (M):     974.89 ; \t loss: 0.3567\n",
      "Total trained pairs (M):     975.36 ; \t loss: 0.3565\n",
      "Total trained pairs (M):     975.83 ; \t loss: 0.3567\n",
      "Total trained pairs (M):     976.30 ; \t loss: 0.3567\n",
      "Total trained pairs (M):     976.77 ; \t loss: 0.3584\n",
      "Total trained pairs (M):     977.24 ; \t loss: 0.3571\n",
      "Total trained pairs (M):     977.73 ; \t loss: 0.3586\n",
      "Total trained pairs (M):     978.21 ; \t loss: 0.3591\n",
      "Total trained pairs (M):     978.65 ; \t loss: 0.3617\n",
      "Total trained pairs (M):     979.10 ; \t loss: 0.3640\n",
      "Total trained pairs (M):     979.57 ; \t loss: 0.3690\n",
      "Total trained pairs (M):     980.02 ; \t loss: 0.3682\n",
      "Total trained pairs (M):     980.48 ; \t loss: 0.3688\n",
      "Total trained pairs (M):     980.93 ; \t loss: 0.3656\n",
      "Total trained pairs (M):     981.38 ; \t loss: 0.3657\n",
      "Total trained pairs (M):     981.84 ; \t loss: 0.3633\n",
      "Total trained pairs (M):     982.29 ; \t loss: 0.3624\n",
      "Total trained pairs (M):     982.75 ; \t loss: 0.3609\n",
      "Total trained pairs (M):     983.21 ; \t loss: 0.3609\n",
      "Total trained pairs (M):     983.66 ; \t loss: 0.3596\n",
      "Total trained pairs (M):     984.12 ; \t loss: 0.3637\n",
      "Total trained pairs (M):     984.58 ; \t loss: 0.3637\n",
      "Total trained pairs (M):     985.04 ; \t loss: 0.3616\n",
      "Total trained pairs (M):     985.50 ; \t loss: 0.3608\n",
      "Total trained pairs (M):     985.94 ; \t loss: 0.3640\n",
      "Total trained pairs (M):     986.39 ; \t loss: 0.3649\n",
      "Total trained pairs (M):     986.84 ; \t loss: 0.3638\n",
      "Total trained pairs (M):     987.28 ; \t loss: 0.3673\n",
      "Total trained pairs (M):     987.73 ; \t loss: 0.3683\n",
      "Total trained pairs (M):     988.16 ; \t loss: 0.3676\n",
      "Total trained pairs (M):     988.59 ; \t loss: 0.3686\n",
      "Total trained pairs (M):     989.01 ; \t loss: 0.3700\n",
      "Total trained pairs (M):     989.43 ; \t loss: 0.3724\n",
      "Total trained pairs (M):     989.84 ; \t loss: 0.3731\n",
      "Total trained pairs (M):     990.25 ; \t loss: 0.3715\n",
      "Total trained pairs (M):     990.65 ; \t loss: 0.3700\n",
      "Total trained pairs (M):     991.06 ; \t loss: 0.3747\n",
      "Total trained pairs (M):     991.47 ; \t loss: 0.3726\n",
      "Total trained pairs (M):     991.87 ; \t loss: 0.3703\n",
      "Total trained pairs (M):     992.28 ; \t loss: 0.3702\n",
      "Total trained pairs (M):     992.68 ; \t loss: 0.3692\n",
      "Total trained pairs (M):     993.08 ; \t loss: 0.3665\n",
      "Total trained pairs (M):     993.49 ; \t loss: 0.3709\n",
      "Total trained pairs (M):     993.89 ; \t loss: 0.3719\n",
      "Total trained pairs (M):     994.30 ; \t loss: 0.3721\n",
      "Total trained pairs (M):     994.71 ; \t loss: 0.3708\n",
      "Total trained pairs (M):     995.13 ; \t loss: 0.3701\n",
      "Total trained pairs (M):     995.54 ; \t loss: 0.3686\n",
      "Total trained pairs (M):     995.95 ; \t loss: 0.3754\n",
      "Total trained pairs (M):     996.36 ; \t loss: 0.3757\n",
      "Total trained pairs (M):     996.77 ; \t loss: 0.3818\n",
      "Total trained pairs (M):     997.19 ; \t loss: 0.3827\n",
      "Total trained pairs (M):     997.61 ; \t loss: 0.3835\n",
      "Total trained pairs (M):     998.03 ; \t loss: 0.3825\n",
      "Total trained pairs (M):     998.45 ; \t loss: 0.3844\n",
      "Total trained pairs (M):     998.88 ; \t loss: 0.3893\n",
      "Total trained pairs (M):     999.30 ; \t loss: 0.3907\n",
      "Total trained pairs (M):     999.73 ; \t loss: 0.3896\n",
      "Total trained pairs (M):    1000.15 ; \t loss: 0.3880\n",
      "Total trained pairs (M):    1000.58 ; \t loss: 0.3884\n",
      "Total trained pairs (M):    1001.01 ; \t loss: 0.3898\n",
      "Total trained pairs (M):    1001.45 ; \t loss: 0.3915\n",
      "Total trained pairs (M):    1001.90 ; \t loss: 0.3974\n",
      "Total trained pairs (M):    1002.35 ; \t loss: 0.3965\n",
      "Total trained pairs (M):    1002.81 ; \t loss: 0.3944\n",
      "Total trained pairs (M):    1003.24 ; \t loss: 0.3948\n",
      "Total trained pairs (M):    1003.68 ; \t loss: 0.3952\n",
      "Total trained pairs (M):    1004.13 ; \t loss: 0.4062\n",
      "Total trained pairs (M):    1004.58 ; \t loss: 0.4129\n",
      "Total trained pairs (M):    1005.03 ; \t loss: 0.4108\n",
      "Total trained pairs (M):    1005.48 ; \t loss: 0.4104\n",
      "Total trained pairs (M):    1005.92 ; \t loss: 0.4108\n",
      "Total trained pairs (M):    1006.35 ; \t loss: 0.4098\n",
      "Total trained pairs (M):    1006.79 ; \t loss: 0.4144\n",
      "Total trained pairs (M):    1007.22 ; \t loss: 0.4133\n",
      "Total trained pairs (M):    1007.65 ; \t loss: 0.4134\n",
      "Total trained pairs (M):    1008.07 ; \t loss: 0.4144\n",
      "Total trained pairs (M):    1008.50 ; \t loss: 0.4170\n",
      "Total trained pairs (M):    1008.93 ; \t loss: 0.4178\n",
      "Total trained pairs (M):    1009.37 ; \t loss: 0.4260\n",
      "Total trained pairs (M):    1009.81 ; \t loss: 0.4259\n",
      "Total trained pairs (M):    1010.26 ; \t loss: 0.4277\n",
      "Total trained pairs (M):    1010.71 ; \t loss: 0.4261\n",
      "Epoch 5 ======\n",
      "Total trained pairs (M):    1011.23 ; \t loss: 0.5651\n",
      "Total trained pairs (M):    1011.75 ; \t loss: 0.5563\n",
      "Total trained pairs (M):    1012.27 ; \t loss: 0.5442\n",
      "Total trained pairs (M):    1012.80 ; \t loss: 0.5358\n",
      "Total trained pairs (M):    1013.31 ; \t loss: 0.5285\n",
      "Total trained pairs (M):    1013.82 ; \t loss: 0.5218\n",
      "Total trained pairs (M):    1014.33 ; \t loss: 0.5178\n",
      "Total trained pairs (M):    1014.83 ; \t loss: 0.5129\n",
      "Total trained pairs (M):    1015.33 ; \t loss: 0.5071\n",
      "Total trained pairs (M):    1015.83 ; \t loss: 0.4996\n",
      "Total trained pairs (M):    1016.33 ; \t loss: 0.4958\n",
      "Total trained pairs (M):    1016.83 ; \t loss: 0.4919\n",
      "Total trained pairs (M):    1017.33 ; \t loss: 0.4878\n",
      "Total trained pairs (M):    1017.83 ; \t loss: 0.4833\n",
      "Total trained pairs (M):    1018.33 ; \t loss: 0.4781\n",
      "Total trained pairs (M):    1018.81 ; \t loss: 0.4768\n",
      "Total trained pairs (M):    1019.29 ; \t loss: 0.4740\n",
      "Total trained pairs (M):    1019.77 ; \t loss: 0.4696\n",
      "Total trained pairs (M):    1020.26 ; \t loss: 0.4667\n",
      "Total trained pairs (M):    1020.73 ; \t loss: 0.4625\n",
      "Total trained pairs (M):    1021.21 ; \t loss: 0.4595\n",
      "Total trained pairs (M):    1021.69 ; \t loss: 0.4573\n",
      "Total trained pairs (M):    1022.17 ; \t loss: 0.4542\n",
      "Total trained pairs (M):    1022.64 ; \t loss: 0.4500\n",
      "Total trained pairs (M):    1023.12 ; \t loss: 0.4497\n",
      "Total trained pairs (M):    1023.61 ; \t loss: 0.4445\n",
      "Total trained pairs (M):    1024.09 ; \t loss: 0.4406\n",
      "Total trained pairs (M):    1024.56 ; \t loss: 0.4305\n",
      "Total trained pairs (M):    1025.03 ; \t loss: 0.4297\n",
      "Total trained pairs (M):    1025.50 ; \t loss: 0.4259\n",
      "Total trained pairs (M):    1025.97 ; \t loss: 0.4239\n",
      "Total trained pairs (M):    1026.44 ; \t loss: 0.4233\n",
      "Total trained pairs (M):    1026.92 ; \t loss: 0.4201\n",
      "Total trained pairs (M):    1027.41 ; \t loss: 0.4184\n",
      "Total trained pairs (M):    1027.90 ; \t loss: 0.4188\n",
      "Total trained pairs (M):    1028.38 ; \t loss: 0.4160\n",
      "Total trained pairs (M):    1028.86 ; \t loss: 0.4135\n",
      "Total trained pairs (M):    1029.34 ; \t loss: 0.4109\n",
      "Total trained pairs (M):    1029.81 ; \t loss: 0.4087\n",
      "Total trained pairs (M):    1030.29 ; \t loss: 0.4071\n",
      "Total trained pairs (M):    1030.77 ; \t loss: 0.4046\n",
      "Total trained pairs (M):    1031.25 ; \t loss: 0.3966\n",
      "Total trained pairs (M):    1031.72 ; \t loss: 0.3947\n",
      "Total trained pairs (M):    1032.20 ; \t loss: 0.3921\n",
      "Total trained pairs (M):    1032.69 ; \t loss: 0.3894\n",
      "Total trained pairs (M):    1033.17 ; \t loss: 0.3881\n",
      "Total trained pairs (M):    1033.66 ; \t loss: 0.3860\n",
      "Total trained pairs (M):    1034.14 ; \t loss: 0.3843\n",
      "Total trained pairs (M):    1034.62 ; \t loss: 0.3824\n",
      "Total trained pairs (M):    1035.10 ; \t loss: 0.3807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trained pairs (M):    1035.58 ; \t loss: 0.3790\n",
      "Total trained pairs (M):    1036.07 ; \t loss: 0.3786\n",
      "Total trained pairs (M):    1036.56 ; \t loss: 0.3761\n",
      "Total trained pairs (M):    1037.05 ; \t loss: 0.3741\n",
      "Total trained pairs (M):    1037.54 ; \t loss: 0.3718\n",
      "Total trained pairs (M):    1038.03 ; \t loss: 0.3700\n",
      "Total trained pairs (M):    1038.55 ; \t loss: 0.3718\n",
      "Total trained pairs (M):    1039.08 ; \t loss: 0.3732\n",
      "Total trained pairs (M):    1039.60 ; \t loss: 0.3717\n",
      "Total trained pairs (M):    1040.13 ; \t loss: 0.3691\n",
      "Total trained pairs (M):    1040.64 ; \t loss: 0.3678\n",
      "Total trained pairs (M):    1041.15 ; \t loss: 0.3680\n",
      "Total trained pairs (M):    1041.66 ; \t loss: 0.3655\n",
      "Total trained pairs (M):    1042.17 ; \t loss: 0.3634\n",
      "Total trained pairs (M):    1042.66 ; \t loss: 0.3611\n",
      "Total trained pairs (M):    1043.15 ; \t loss: 0.3594\n",
      "Total trained pairs (M):    1043.63 ; \t loss: 0.3556\n",
      "Total trained pairs (M):    1044.12 ; \t loss: 0.3541\n",
      "Total trained pairs (M):    1044.61 ; \t loss: 0.3528\n",
      "Total trained pairs (M):    1045.10 ; \t loss: 0.3528\n",
      "Total trained pairs (M):    1045.59 ; \t loss: 0.3532\n",
      "Total trained pairs (M):    1046.09 ; \t loss: 0.3542\n",
      "Total trained pairs (M):    1046.58 ; \t loss: 0.3527\n",
      "Total trained pairs (M):    1047.07 ; \t loss: 0.3533\n",
      "Total trained pairs (M):    1047.57 ; \t loss: 0.3496\n",
      "Total trained pairs (M):    1048.07 ; \t loss: 0.3522\n",
      "Total trained pairs (M):    1048.57 ; \t loss: 0.3554\n",
      "Total trained pairs (M):    1049.06 ; \t loss: 0.3525\n",
      "Total trained pairs (M):    1049.55 ; \t loss: 0.3539\n",
      "Total trained pairs (M):    1050.06 ; \t loss: 0.3550\n",
      "Total trained pairs (M):    1050.57 ; \t loss: 0.3549\n",
      "Total trained pairs (M):    1051.09 ; \t loss: 0.3541\n",
      "Total trained pairs (M):    1051.61 ; \t loss: 0.3547\n",
      "Total trained pairs (M):    1052.13 ; \t loss: 0.3531\n",
      "Total trained pairs (M):    1052.64 ; \t loss: 0.3522\n",
      "Total trained pairs (M):    1053.15 ; \t loss: 0.3511\n",
      "Total trained pairs (M):    1053.69 ; \t loss: 0.3544\n",
      "Total trained pairs (M):    1054.22 ; \t loss: 0.3563\n",
      "Total trained pairs (M):    1054.76 ; \t loss: 0.3559\n",
      "Total trained pairs (M):    1055.30 ; \t loss: 0.3548\n",
      "Total trained pairs (M):    1055.84 ; \t loss: 0.3525\n",
      "Total trained pairs (M):    1056.40 ; \t loss: 0.3543\n",
      "Total trained pairs (M):    1056.95 ; \t loss: 0.3524\n",
      "Total trained pairs (M):    1057.51 ; \t loss: 0.3520\n",
      "Total trained pairs (M):    1058.06 ; \t loss: 0.3509\n",
      "Total trained pairs (M):    1058.62 ; \t loss: 0.3505\n",
      "Total trained pairs (M):    1059.18 ; \t loss: 0.3514\n",
      "Total trained pairs (M):    1059.73 ; \t loss: 0.3507\n",
      "Total trained pairs (M):    1060.29 ; \t loss: 0.3482\n",
      "Total trained pairs (M):    1060.81 ; \t loss: 0.3506\n",
      "Total trained pairs (M):    1061.32 ; \t loss: 0.3492\n",
      "Total trained pairs (M):    1061.84 ; \t loss: 0.3515\n",
      "Total trained pairs (M):    1062.35 ; \t loss: 0.3519\n",
      "Total trained pairs (M):    1062.86 ; \t loss: 0.3497\n",
      "Total trained pairs (M):    1063.38 ; \t loss: 0.3498\n",
      "Total trained pairs (M):    1063.89 ; \t loss: 0.3485\n",
      "Total trained pairs (M):    1064.40 ; \t loss: 0.3476\n",
      "Total trained pairs (M):    1064.91 ; \t loss: 0.3454\n",
      "Total trained pairs (M):    1065.42 ; \t loss: 0.3476\n",
      "Total trained pairs (M):    1065.92 ; \t loss: 0.3462\n",
      "Total trained pairs (M):    1066.44 ; \t loss: 0.3474\n",
      "Total trained pairs (M):    1066.94 ; \t loss: 0.3475\n",
      "Total trained pairs (M):    1067.45 ; \t loss: 0.3441\n",
      "Total trained pairs (M):    1067.97 ; \t loss: 0.3463\n",
      "Total trained pairs (M):    1068.48 ; \t loss: 0.3450\n",
      "Total trained pairs (M):    1068.99 ; \t loss: 0.3459\n",
      "Total trained pairs (M):    1069.52 ; \t loss: 0.3467\n",
      "Total trained pairs (M):    1070.04 ; \t loss: 0.3457\n",
      "Total trained pairs (M):    1070.56 ; \t loss: 0.3434\n",
      "Total trained pairs (M):    1071.09 ; \t loss: 0.3462\n",
      "Total trained pairs (M):    1071.62 ; \t loss: 0.3452\n",
      "Total trained pairs (M):    1072.15 ; \t loss: 0.3437\n",
      "Total trained pairs (M):    1072.68 ; \t loss: 0.3411\n",
      "Total trained pairs (M):    1073.21 ; \t loss: 0.3416\n",
      "Total trained pairs (M):    1073.74 ; \t loss: 0.3403\n",
      "Total trained pairs (M):    1074.26 ; \t loss: 0.3426\n",
      "Total trained pairs (M):    1074.79 ; \t loss: 0.3424\n",
      "Total trained pairs (M):    1075.31 ; \t loss: 0.3401\n",
      "Total trained pairs (M):    1075.83 ; \t loss: 0.3391\n",
      "Total trained pairs (M):    1076.35 ; \t loss: 0.3409\n",
      "Total trained pairs (M):    1076.88 ; \t loss: 0.3403\n",
      "Total trained pairs (M):    1077.40 ; \t loss: 0.3402\n",
      "Total trained pairs (M):    1077.92 ; \t loss: 0.3380\n",
      "Total trained pairs (M):    1078.43 ; \t loss: 0.3390\n",
      "Total trained pairs (M):    1078.95 ; \t loss: 0.3388\n",
      "Total trained pairs (M):    1079.46 ; \t loss: 0.3362\n",
      "Total trained pairs (M):    1079.97 ; \t loss: 0.3345\n",
      "Total trained pairs (M):    1080.50 ; \t loss: 0.3369\n",
      "Total trained pairs (M):    1081.02 ; \t loss: 0.3361\n",
      "Total trained pairs (M):    1081.55 ; \t loss: 0.3355\n",
      "Total trained pairs (M):    1082.08 ; \t loss: 0.3368\n",
      "Total trained pairs (M):    1082.60 ; \t loss: 0.3365\n",
      "Total trained pairs (M):    1083.14 ; \t loss: 0.3410\n",
      "Total trained pairs (M):    1083.67 ; \t loss: 0.3409\n",
      "Total trained pairs (M):    1084.20 ; \t loss: 0.3404\n",
      "Total trained pairs (M):    1084.74 ; \t loss: 0.3392\n",
      "Total trained pairs (M):    1085.27 ; \t loss: 0.3398\n",
      "Total trained pairs (M):    1085.81 ; \t loss: 0.3377\n",
      "Total trained pairs (M):    1086.34 ; \t loss: 0.3364\n",
      "Total trained pairs (M):    1086.88 ; \t loss: 0.3336\n",
      "Total trained pairs (M):    1087.41 ; \t loss: 0.3343\n",
      "Total trained pairs (M):    1087.93 ; \t loss: 0.3336\n",
      "Total trained pairs (M):    1088.46 ; \t loss: 0.3332\n",
      "Total trained pairs (M):    1088.99 ; \t loss: 0.3309\n",
      "Total trained pairs (M):    1089.53 ; \t loss: 0.3306\n",
      "Total trained pairs (M):    1090.05 ; \t loss: 0.3314\n",
      "Total trained pairs (M):    1090.55 ; \t loss: 0.3340\n",
      "Total trained pairs (M):    1091.03 ; \t loss: 0.3336\n",
      "Total trained pairs (M):    1091.52 ; \t loss: 0.3315\n",
      "Total trained pairs (M):    1092.01 ; \t loss: 0.3310\n",
      "Total trained pairs (M):    1092.50 ; \t loss: 0.3302\n",
      "Total trained pairs (M):    1092.98 ; \t loss: 0.3288\n",
      "Total trained pairs (M):    1093.48 ; \t loss: 0.3288\n",
      "Total trained pairs (M):    1093.99 ; \t loss: 0.3295\n",
      "Total trained pairs (M):    1094.50 ; \t loss: 0.3296\n",
      "Total trained pairs (M):    1095.02 ; \t loss: 0.3291\n",
      "Total trained pairs (M):    1095.54 ; \t loss: 0.3286\n",
      "Total trained pairs (M):    1096.06 ; \t loss: 0.3279\n",
      "Total trained pairs (M):    1096.57 ; \t loss: 0.3265\n",
      "Total trained pairs (M):    1097.09 ; \t loss: 0.3264\n",
      "Total trained pairs (M):    1097.60 ; \t loss: 0.3242\n",
      "Total trained pairs (M):    1098.11 ; \t loss: 0.3267\n",
      "Total trained pairs (M):    1098.61 ; \t loss: 0.3256\n",
      "Total trained pairs (M):    1099.12 ; \t loss: 0.3238\n",
      "Total trained pairs (M):    1099.62 ; \t loss: 0.3231\n",
      "Total trained pairs (M):    1100.11 ; \t loss: 0.3234\n",
      "Total trained pairs (M):    1100.59 ; \t loss: 0.3234\n",
      "Total trained pairs (M):    1101.07 ; \t loss: 0.3222\n",
      "Total trained pairs (M):    1101.55 ; \t loss: 0.3219\n",
      "Total trained pairs (M):    1102.01 ; \t loss: 0.3238\n",
      "Total trained pairs (M):    1102.47 ; \t loss: 0.3225\n",
      "Total trained pairs (M):    1102.92 ; \t loss: 0.3218\n",
      "Total trained pairs (M):    1103.37 ; \t loss: 0.3225\n",
      "Total trained pairs (M):    1103.81 ; \t loss: 0.3200\n",
      "Total trained pairs (M):    1104.25 ; \t loss: 0.3207\n",
      "Total trained pairs (M):    1104.69 ; \t loss: 0.3203\n",
      "Total trained pairs (M):    1105.12 ; \t loss: 0.3212\n",
      "Total trained pairs (M):    1105.54 ; \t loss: 0.3232\n",
      "Total trained pairs (M):    1105.97 ; \t loss: 0.3216\n",
      "Total trained pairs (M):    1106.38 ; \t loss: 0.3213\n",
      "Total trained pairs (M):    1106.81 ; \t loss: 0.3278\n",
      "Total trained pairs (M):    1107.23 ; \t loss: 0.3271\n",
      "Total trained pairs (M):    1107.65 ; \t loss: 0.3313\n",
      "Total trained pairs (M):    1108.07 ; \t loss: 0.3295\n",
      "Total trained pairs (M):    1108.49 ; \t loss: 0.3293\n",
      "Total trained pairs (M):    1108.89 ; \t loss: 0.3320\n",
      "Total trained pairs (M):    1109.30 ; \t loss: 0.3338\n",
      "Total trained pairs (M):    1109.73 ; \t loss: 0.3357\n",
      "Total trained pairs (M):    1110.17 ; \t loss: 0.3410\n",
      "Total trained pairs (M):    1110.62 ; \t loss: 0.3450\n",
      "Total trained pairs (M):    1111.08 ; \t loss: 0.3447\n",
      "Total trained pairs (M):    1111.53 ; \t loss: 0.3447\n",
      "Total trained pairs (M):    1111.98 ; \t loss: 0.3443\n",
      "Total trained pairs (M):    1112.43 ; \t loss: 0.3420\n",
      "Total trained pairs (M):    1112.87 ; \t loss: 0.3415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trained pairs (M):    1113.31 ; \t loss: 0.3418\n",
      "Total trained pairs (M):    1113.76 ; \t loss: 0.3401\n",
      "Total trained pairs (M):    1114.20 ; \t loss: 0.3394\n",
      "Total trained pairs (M):    1114.65 ; \t loss: 0.3394\n",
      "Total trained pairs (M):    1115.10 ; \t loss: 0.3397\n",
      "Total trained pairs (M):    1115.55 ; \t loss: 0.3401\n",
      "Total trained pairs (M):    1116.00 ; \t loss: 0.3395\n",
      "Total trained pairs (M):    1116.45 ; \t loss: 0.3381\n",
      "Total trained pairs (M):    1116.90 ; \t loss: 0.3400\n",
      "Total trained pairs (M):    1117.35 ; \t loss: 0.3395\n",
      "Total trained pairs (M):    1117.79 ; \t loss: 0.3361\n",
      "Total trained pairs (M):    1118.23 ; \t loss: 0.3382\n",
      "Total trained pairs (M):    1118.67 ; \t loss: 0.3376\n",
      "Total trained pairs (M):    1119.11 ; \t loss: 0.3404\n",
      "Total trained pairs (M):    1119.55 ; \t loss: 0.3396\n",
      "Total trained pairs (M):    1119.99 ; \t loss: 0.3391\n",
      "Total trained pairs (M):    1120.43 ; \t loss: 0.3360\n",
      "Total trained pairs (M):    1120.87 ; \t loss: 0.3332\n",
      "Total trained pairs (M):    1121.31 ; \t loss: 0.3343\n",
      "Total trained pairs (M):    1121.75 ; \t loss: 0.3384\n",
      "Total trained pairs (M):    1122.19 ; \t loss: 0.3371\n",
      "Total trained pairs (M):    1122.64 ; \t loss: 0.3380\n",
      "Total trained pairs (M):    1123.08 ; \t loss: 0.3343\n",
      "Total trained pairs (M):    1123.53 ; \t loss: 0.3335\n",
      "Total trained pairs (M):    1123.97 ; \t loss: 0.3376\n",
      "Total trained pairs (M):    1124.42 ; \t loss: 0.3362\n",
      "Total trained pairs (M):    1124.86 ; \t loss: 0.3364\n",
      "Total trained pairs (M):    1125.32 ; \t loss: 0.3370\n",
      "Total trained pairs (M):    1125.77 ; \t loss: 0.3404\n",
      "Total trained pairs (M):    1126.22 ; \t loss: 0.3399\n",
      "Total trained pairs (M):    1126.69 ; \t loss: 0.3406\n",
      "Total trained pairs (M):    1127.15 ; \t loss: 0.3406\n",
      "Total trained pairs (M):    1127.61 ; \t loss: 0.3414\n",
      "Total trained pairs (M):    1128.07 ; \t loss: 0.3400\n",
      "Total trained pairs (M):    1128.53 ; \t loss: 0.3394\n",
      "Total trained pairs (M):    1128.98 ; \t loss: 0.3402\n",
      "Total trained pairs (M):    1129.43 ; \t loss: 0.3379\n",
      "Total trained pairs (M):    1129.87 ; \t loss: 0.3417\n",
      "Total trained pairs (M):    1130.31 ; \t loss: 0.3434\n",
      "Total trained pairs (M):    1130.74 ; \t loss: 0.3425\n",
      "Total trained pairs (M):    1131.18 ; \t loss: 0.3403\n",
      "Total trained pairs (M):    1131.61 ; \t loss: 0.3403\n",
      "Total trained pairs (M):    1132.05 ; \t loss: 0.3411\n",
      "Total trained pairs (M):    1132.49 ; \t loss: 0.3399\n",
      "Total trained pairs (M):    1132.96 ; \t loss: 0.3543\n",
      "Total trained pairs (M):    1133.44 ; \t loss: 0.3524\n",
      "Total trained pairs (M):    1133.91 ; \t loss: 0.3517\n",
      "Total trained pairs (M):    1134.38 ; \t loss: 0.3512\n",
      "Total trained pairs (M):    1134.86 ; \t loss: 0.3522\n",
      "Total trained pairs (M):    1135.34 ; \t loss: 0.3505\n",
      "Total trained pairs (M):    1135.82 ; \t loss: 0.3504\n",
      "Total trained pairs (M):    1136.29 ; \t loss: 0.3490\n",
      "Total trained pairs (M):    1136.76 ; \t loss: 0.3471\n",
      "Total trained pairs (M):    1137.23 ; \t loss: 0.3474\n",
      "Total trained pairs (M):    1137.71 ; \t loss: 0.3452\n",
      "Total trained pairs (M):    1138.19 ; \t loss: 0.3444\n",
      "Total trained pairs (M):    1138.67 ; \t loss: 0.3447\n",
      "Total trained pairs (M):    1139.14 ; \t loss: 0.3458\n",
      "Total trained pairs (M):    1139.60 ; \t loss: 0.3479\n",
      "Total trained pairs (M):    1140.05 ; \t loss: 0.3446\n",
      "Total trained pairs (M):    1140.52 ; \t loss: 0.3430\n",
      "Total trained pairs (M):    1140.98 ; \t loss: 0.3459\n",
      "Total trained pairs (M):    1141.44 ; \t loss: 0.3464\n",
      "Total trained pairs (M):    1141.91 ; \t loss: 0.3454\n",
      "Total trained pairs (M):    1142.39 ; \t loss: 0.3438\n",
      "Total trained pairs (M):    1142.87 ; \t loss: 0.3440\n",
      "Total trained pairs (M):    1143.34 ; \t loss: 0.3430\n",
      "Total trained pairs (M):    1143.84 ; \t loss: 0.3430\n",
      "Total trained pairs (M):    1144.33 ; \t loss: 0.3438\n",
      "Total trained pairs (M):    1144.83 ; \t loss: 0.3445\n",
      "Total trained pairs (M):    1145.34 ; \t loss: 0.3416\n",
      "Total trained pairs (M):    1145.85 ; \t loss: 0.3442\n",
      "Total trained pairs (M):    1146.36 ; \t loss: 0.3438\n",
      "Total trained pairs (M):    1146.88 ; \t loss: 0.3428\n",
      "Total trained pairs (M):    1147.39 ; \t loss: 0.3412\n",
      "Total trained pairs (M):    1147.90 ; \t loss: 0.3409\n",
      "Total trained pairs (M):    1148.41 ; \t loss: 0.3397\n",
      "Total trained pairs (M):    1148.92 ; \t loss: 0.3375\n",
      "Total trained pairs (M):    1149.43 ; \t loss: 0.3373\n",
      "Total trained pairs (M):    1149.95 ; \t loss: 0.3362\n",
      "Total trained pairs (M):    1150.47 ; \t loss: 0.3343\n",
      "Total trained pairs (M):    1150.99 ; \t loss: 0.3335\n",
      "Total trained pairs (M):    1151.49 ; \t loss: 0.3334\n",
      "Total trained pairs (M):    1152.00 ; \t loss: 0.3332\n",
      "Total trained pairs (M):    1152.51 ; \t loss: 0.3342\n",
      "Total trained pairs (M):    1153.01 ; \t loss: 0.3348\n",
      "Total trained pairs (M):    1153.52 ; \t loss: 0.3326\n",
      "Total trained pairs (M):    1154.01 ; \t loss: 0.3339\n",
      "Total trained pairs (M):    1154.50 ; \t loss: 0.3329\n",
      "Total trained pairs (M):    1155.00 ; \t loss: 0.3318\n",
      "Total trained pairs (M):    1155.50 ; \t loss: 0.3318\n",
      "Total trained pairs (M):    1155.97 ; \t loss: 0.3346\n",
      "Total trained pairs (M):    1156.46 ; \t loss: 0.3396\n",
      "Total trained pairs (M):    1156.92 ; \t loss: 0.3406\n",
      "Total trained pairs (M):    1157.38 ; \t loss: 0.3410\n",
      "Total trained pairs (M):    1157.82 ; \t loss: 0.3394\n",
      "Total trained pairs (M):    1158.27 ; \t loss: 0.3383\n",
      "Total trained pairs (M):    1158.72 ; \t loss: 0.3354\n",
      "Total trained pairs (M):    1159.17 ; \t loss: 0.3364\n",
      "Total trained pairs (M):    1159.63 ; \t loss: 0.3361\n",
      "Total trained pairs (M):    1160.08 ; \t loss: 0.3338\n",
      "Total trained pairs (M):    1160.55 ; \t loss: 0.3359\n",
      "Total trained pairs (M):    1161.00 ; \t loss: 0.3347\n",
      "Total trained pairs (M):    1161.47 ; \t loss: 0.3349\n",
      "Total trained pairs (M):    1161.93 ; \t loss: 0.3361\n",
      "Total trained pairs (M):    1162.40 ; \t loss: 0.3358\n",
      "Total trained pairs (M):    1162.85 ; \t loss: 0.3345\n",
      "Total trained pairs (M):    1163.32 ; \t loss: 0.3354\n",
      "Total trained pairs (M):    1163.79 ; \t loss: 0.3358\n",
      "Total trained pairs (M):    1164.26 ; \t loss: 0.3383\n",
      "Total trained pairs (M):    1164.73 ; \t loss: 0.3375\n",
      "Total trained pairs (M):    1165.20 ; \t loss: 0.3380\n",
      "Total trained pairs (M):    1165.68 ; \t loss: 0.3370\n",
      "Total trained pairs (M):    1166.15 ; \t loss: 0.3361\n",
      "Total trained pairs (M):    1166.63 ; \t loss: 0.3365\n",
      "Total trained pairs (M):    1167.10 ; \t loss: 0.3367\n",
      "Total trained pairs (M):    1167.58 ; \t loss: 0.3352\n",
      "Total trained pairs (M):    1168.06 ; \t loss: 0.3338\n",
      "Total trained pairs (M):    1168.53 ; \t loss: 0.3325\n",
      "Total trained pairs (M):    1169.01 ; \t loss: 0.3363\n",
      "Total trained pairs (M):    1169.48 ; \t loss: 0.3355\n",
      "Total trained pairs (M):    1169.96 ; \t loss: 0.3338\n",
      "Total trained pairs (M):    1170.43 ; \t loss: 0.3347\n",
      "Total trained pairs (M):    1170.90 ; \t loss: 0.3337\n",
      "Total trained pairs (M):    1171.38 ; \t loss: 0.3335\n",
      "Total trained pairs (M):    1171.86 ; \t loss: 0.3346\n",
      "Total trained pairs (M):    1172.34 ; \t loss: 0.3345\n",
      "Total trained pairs (M):    1172.81 ; \t loss: 0.3336\n",
      "Total trained pairs (M):    1173.27 ; \t loss: 0.3346\n",
      "Total trained pairs (M):    1173.74 ; \t loss: 0.3341\n",
      "Total trained pairs (M):    1174.21 ; \t loss: 0.3420\n",
      "Total trained pairs (M):    1174.68 ; \t loss: 0.3428\n",
      "Total trained pairs (M):    1175.15 ; \t loss: 0.3409\n",
      "Total trained pairs (M):    1175.62 ; \t loss: 0.3398\n",
      "Total trained pairs (M):    1176.09 ; \t loss: 0.3400\n",
      "Total trained pairs (M):    1176.56 ; \t loss: 0.3409\n",
      "Total trained pairs (M):    1177.03 ; \t loss: 0.3411\n",
      "Total trained pairs (M):    1177.50 ; \t loss: 0.3414\n",
      "Total trained pairs (M):    1177.97 ; \t loss: 0.3399\n",
      "Total trained pairs (M):    1178.44 ; \t loss: 0.3402\n",
      "Total trained pairs (M):    1178.91 ; \t loss: 0.3419\n",
      "Total trained pairs (M):    1179.39 ; \t loss: 0.3423\n",
      "Total trained pairs (M):    1179.87 ; \t loss: 0.3440\n",
      "Total trained pairs (M):    1180.35 ; \t loss: 0.3425\n",
      "Total trained pairs (M):    1180.79 ; \t loss: 0.3465\n",
      "Total trained pairs (M):    1181.25 ; \t loss: 0.3473\n",
      "Total trained pairs (M):    1181.71 ; \t loss: 0.3529\n",
      "Total trained pairs (M):    1182.17 ; \t loss: 0.3523\n",
      "Total trained pairs (M):    1182.62 ; \t loss: 0.3523\n",
      "Total trained pairs (M):    1183.07 ; \t loss: 0.3520\n",
      "Total trained pairs (M):    1183.52 ; \t loss: 0.3506\n",
      "Total trained pairs (M):    1183.98 ; \t loss: 0.3484\n",
      "Total trained pairs (M):    1184.43 ; \t loss: 0.3463\n",
      "Total trained pairs (M):    1184.89 ; \t loss: 0.3463\n",
      "Total trained pairs (M):    1185.35 ; \t loss: 0.3461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trained pairs (M):    1185.80 ; \t loss: 0.3445\n",
      "Total trained pairs (M):    1186.26 ; \t loss: 0.3477\n",
      "Total trained pairs (M):    1186.72 ; \t loss: 0.3482\n",
      "Total trained pairs (M):    1187.18 ; \t loss: 0.3470\n",
      "Total trained pairs (M):    1187.64 ; \t loss: 0.3462\n",
      "Total trained pairs (M):    1188.09 ; \t loss: 0.3485\n",
      "Total trained pairs (M):    1188.53 ; \t loss: 0.3508\n",
      "Total trained pairs (M):    1188.98 ; \t loss: 0.3493\n",
      "Total trained pairs (M):    1189.43 ; \t loss: 0.3529\n",
      "Total trained pairs (M):    1189.87 ; \t loss: 0.3543\n",
      "Total trained pairs (M):    1190.30 ; \t loss: 0.3549\n",
      "Total trained pairs (M):    1190.73 ; \t loss: 0.3544\n",
      "Total trained pairs (M):    1191.16 ; \t loss: 0.3567\n",
      "Total trained pairs (M):    1191.58 ; \t loss: 0.3560\n",
      "Total trained pairs (M):    1191.98 ; \t loss: 0.3564\n",
      "Total trained pairs (M):    1192.39 ; \t loss: 0.3571\n",
      "Total trained pairs (M):    1192.80 ; \t loss: 0.3576\n",
      "Total trained pairs (M):    1193.20 ; \t loss: 0.3599\n",
      "Total trained pairs (M):    1193.61 ; \t loss: 0.3595\n",
      "Total trained pairs (M):    1194.01 ; \t loss: 0.3580\n",
      "Total trained pairs (M):    1194.42 ; \t loss: 0.3564\n",
      "Total trained pairs (M):    1194.82 ; \t loss: 0.3555\n",
      "Total trained pairs (M):    1195.23 ; \t loss: 0.3535\n",
      "Total trained pairs (M):    1195.63 ; \t loss: 0.3577\n",
      "Total trained pairs (M):    1196.04 ; \t loss: 0.3586\n",
      "Total trained pairs (M):    1196.45 ; \t loss: 0.3576\n",
      "Total trained pairs (M):    1196.86 ; \t loss: 0.3578\n",
      "Total trained pairs (M):    1197.27 ; \t loss: 0.3565\n",
      "Total trained pairs (M):    1197.68 ; \t loss: 0.3566\n",
      "Total trained pairs (M):    1198.09 ; \t loss: 0.3636\n",
      "Total trained pairs (M):    1198.50 ; \t loss: 0.3645\n",
      "Total trained pairs (M):    1198.92 ; \t loss: 0.3704\n",
      "Total trained pairs (M):    1199.33 ; \t loss: 0.3698\n",
      "Total trained pairs (M):    1199.75 ; \t loss: 0.3702\n",
      "Total trained pairs (M):    1200.17 ; \t loss: 0.3696\n",
      "Total trained pairs (M):    1200.59 ; \t loss: 0.3720\n",
      "Total trained pairs (M):    1201.03 ; \t loss: 0.3774\n",
      "Total trained pairs (M):    1201.44 ; \t loss: 0.3785\n",
      "Total trained pairs (M):    1201.87 ; \t loss: 0.3783\n",
      "Total trained pairs (M):    1202.30 ; \t loss: 0.3767\n",
      "Total trained pairs (M):    1202.72 ; \t loss: 0.3776\n",
      "Total trained pairs (M):    1203.15 ; \t loss: 0.3768\n",
      "Total trained pairs (M):    1203.59 ; \t loss: 0.3819\n",
      "Total trained pairs (M):    1204.05 ; \t loss: 0.3871\n",
      "Total trained pairs (M):    1204.50 ; \t loss: 0.3856\n",
      "Total trained pairs (M):    1204.95 ; \t loss: 0.3851\n",
      "Total trained pairs (M):    1205.39 ; \t loss: 0.3854\n",
      "Total trained pairs (M):    1205.82 ; \t loss: 0.3845\n",
      "Total trained pairs (M):    1206.27 ; \t loss: 0.3972\n",
      "Total trained pairs (M):    1206.72 ; \t loss: 0.4039\n",
      "Total trained pairs (M):    1207.17 ; \t loss: 0.4026\n",
      "Total trained pairs (M):    1207.62 ; \t loss: 0.4010\n",
      "Total trained pairs (M):    1208.06 ; \t loss: 0.4027\n",
      "Total trained pairs (M):    1208.50 ; \t loss: 0.4018\n",
      "Total trained pairs (M):    1208.93 ; \t loss: 0.4057\n",
      "Total trained pairs (M):    1209.36 ; \t loss: 0.4065\n",
      "Total trained pairs (M):    1209.79 ; \t loss: 0.4065\n",
      "Total trained pairs (M):    1210.21 ; \t loss: 0.4066\n",
      "Total trained pairs (M):    1210.64 ; \t loss: 0.4075\n",
      "Total trained pairs (M):    1211.07 ; \t loss: 0.4095\n",
      "Total trained pairs (M):    1211.51 ; \t loss: 0.4186\n",
      "Total trained pairs (M):    1211.95 ; \t loss: 0.4188\n",
      "Total trained pairs (M):    1212.40 ; \t loss: 0.4204\n",
      "Total trained pairs (M):    1212.85 ; \t loss: 0.4199\n",
      "Epoch 6 ======\n",
      "Total trained pairs (M):    1213.37 ; \t loss: 0.5615\n",
      "Total trained pairs (M):    1213.89 ; \t loss: 0.5527\n",
      "Total trained pairs (M):    1214.42 ; \t loss: 0.5388\n",
      "Total trained pairs (M):    1214.94 ; \t loss: 0.5305\n",
      "Total trained pairs (M):    1215.45 ; \t loss: 0.5229\n",
      "Total trained pairs (M):    1215.96 ; \t loss: 0.5167\n",
      "Total trained pairs (M):    1216.47 ; \t loss: 0.5102\n",
      "Total trained pairs (M):    1216.98 ; \t loss: 0.5049\n",
      "Total trained pairs (M):    1217.48 ; \t loss: 0.4993\n",
      "Total trained pairs (M):    1217.98 ; \t loss: 0.4909\n",
      "Total trained pairs (M):    1218.48 ; \t loss: 0.4877\n",
      "Total trained pairs (M):    1218.97 ; \t loss: 0.4823\n",
      "Total trained pairs (M):    1219.47 ; \t loss: 0.4782\n",
      "Total trained pairs (M):    1219.98 ; \t loss: 0.4745\n",
      "Total trained pairs (M):    1220.47 ; \t loss: 0.4698\n",
      "Total trained pairs (M):    1220.95 ; \t loss: 0.4663\n",
      "Total trained pairs (M):    1221.43 ; \t loss: 0.4626\n",
      "Total trained pairs (M):    1221.91 ; \t loss: 0.4594\n",
      "Total trained pairs (M):    1222.40 ; \t loss: 0.4563\n",
      "Total trained pairs (M):    1222.88 ; \t loss: 0.4532\n",
      "Total trained pairs (M):    1223.35 ; \t loss: 0.4485\n",
      "Total trained pairs (M):    1223.83 ; \t loss: 0.4466\n",
      "Total trained pairs (M):    1224.31 ; \t loss: 0.4430\n",
      "Total trained pairs (M):    1224.78 ; \t loss: 0.4413\n",
      "Total trained pairs (M):    1225.26 ; \t loss: 0.4370\n",
      "Total trained pairs (M):    1225.75 ; \t loss: 0.4328\n",
      "Total trained pairs (M):    1226.23 ; \t loss: 0.4280\n",
      "Total trained pairs (M):    1226.70 ; \t loss: 0.4177\n",
      "Total trained pairs (M):    1227.17 ; \t loss: 0.4180\n",
      "Total trained pairs (M):    1227.64 ; \t loss: 0.4136\n",
      "Total trained pairs (M):    1228.11 ; \t loss: 0.4118\n",
      "Total trained pairs (M):    1228.59 ; \t loss: 0.4096\n",
      "Total trained pairs (M):    1229.06 ; \t loss: 0.4084\n",
      "Total trained pairs (M):    1229.55 ; \t loss: 0.4072\n",
      "Total trained pairs (M):    1230.04 ; \t loss: 0.4051\n",
      "Total trained pairs (M):    1230.53 ; \t loss: 0.4015\n",
      "Total trained pairs (M):    1231.00 ; \t loss: 0.3985\n",
      "Total trained pairs (M):    1231.48 ; \t loss: 0.3973\n",
      "Total trained pairs (M):    1231.95 ; \t loss: 0.3950\n",
      "Total trained pairs (M):    1232.43 ; \t loss: 0.3933\n",
      "Total trained pairs (M):    1232.91 ; \t loss: 0.3906\n",
      "Total trained pairs (M):    1233.39 ; \t loss: 0.3830\n",
      "Total trained pairs (M):    1233.87 ; \t loss: 0.3813\n",
      "Total trained pairs (M):    1234.35 ; \t loss: 0.3783\n",
      "Total trained pairs (M):    1234.83 ; \t loss: 0.3768\n",
      "Total trained pairs (M):    1235.31 ; \t loss: 0.3743\n",
      "Total trained pairs (M):    1235.80 ; \t loss: 0.3716\n",
      "Total trained pairs (M):    1236.28 ; \t loss: 0.3708\n",
      "Total trained pairs (M):    1236.76 ; \t loss: 0.3689\n",
      "Total trained pairs (M):    1237.24 ; \t loss: 0.3664\n",
      "Total trained pairs (M):    1237.73 ; \t loss: 0.3652\n",
      "Total trained pairs (M):    1238.21 ; \t loss: 0.3630\n",
      "Total trained pairs (M):    1238.70 ; \t loss: 0.3621\n",
      "Total trained pairs (M):    1239.19 ; \t loss: 0.3603\n",
      "Total trained pairs (M):    1239.68 ; \t loss: 0.3573\n",
      "Total trained pairs (M):    1240.17 ; \t loss: 0.3555\n",
      "Total trained pairs (M):    1240.69 ; \t loss: 0.3583\n",
      "Total trained pairs (M):    1241.22 ; \t loss: 0.3592\n",
      "Total trained pairs (M):    1241.75 ; \t loss: 0.3572\n",
      "Total trained pairs (M):    1242.27 ; \t loss: 0.3550\n",
      "Total trained pairs (M):    1242.78 ; \t loss: 0.3538\n",
      "Total trained pairs (M):    1243.29 ; \t loss: 0.3534\n",
      "Total trained pairs (M):    1243.80 ; \t loss: 0.3517\n",
      "Total trained pairs (M):    1244.31 ; \t loss: 0.3493\n",
      "Total trained pairs (M):    1244.81 ; \t loss: 0.3472\n",
      "Total trained pairs (M):    1245.29 ; \t loss: 0.3441\n",
      "Total trained pairs (M):    1245.77 ; \t loss: 0.3422\n",
      "Total trained pairs (M):    1246.26 ; \t loss: 0.3399\n",
      "Total trained pairs (M):    1246.75 ; \t loss: 0.3399\n",
      "Total trained pairs (M):    1247.24 ; \t loss: 0.3402\n",
      "Total trained pairs (M):    1247.73 ; \t loss: 0.3380\n",
      "Total trained pairs (M):    1248.23 ; \t loss: 0.3420\n",
      "Total trained pairs (M):    1248.72 ; \t loss: 0.3394\n",
      "Total trained pairs (M):    1249.22 ; \t loss: 0.3381\n",
      "Total trained pairs (M):    1249.71 ; \t loss: 0.3372\n",
      "Total trained pairs (M):    1250.21 ; \t loss: 0.3371\n",
      "Total trained pairs (M):    1250.71 ; \t loss: 0.3403\n",
      "Total trained pairs (M):    1251.20 ; \t loss: 0.3385\n",
      "Total trained pairs (M):    1251.70 ; \t loss: 0.3395\n",
      "Total trained pairs (M):    1252.20 ; \t loss: 0.3402\n",
      "Total trained pairs (M):    1252.71 ; \t loss: 0.3399\n",
      "Total trained pairs (M):    1253.23 ; \t loss: 0.3408\n",
      "Total trained pairs (M):    1253.75 ; \t loss: 0.3396\n",
      "Total trained pairs (M):    1254.27 ; \t loss: 0.3391\n",
      "Total trained pairs (M):    1254.79 ; \t loss: 0.3376\n",
      "Total trained pairs (M):    1255.30 ; \t loss: 0.3360\n",
      "Total trained pairs (M):    1255.83 ; \t loss: 0.3418\n",
      "Total trained pairs (M):    1256.37 ; \t loss: 0.3419\n",
      "Total trained pairs (M):    1256.90 ; \t loss: 0.3412\n",
      "Total trained pairs (M):    1257.44 ; \t loss: 0.3407\n",
      "Total trained pairs (M):    1257.99 ; \t loss: 0.3402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trained pairs (M):    1258.54 ; \t loss: 0.3401\n",
      "Total trained pairs (M):    1259.09 ; \t loss: 0.3383\n",
      "Total trained pairs (M):    1259.65 ; \t loss: 0.3374\n",
      "Total trained pairs (M):    1260.20 ; \t loss: 0.3370\n",
      "Total trained pairs (M):    1260.76 ; \t loss: 0.3379\n",
      "Total trained pairs (M):    1261.32 ; \t loss: 0.3373\n",
      "Total trained pairs (M):    1261.87 ; \t loss: 0.3357\n",
      "Total trained pairs (M):    1262.43 ; \t loss: 0.3356\n",
      "Total trained pairs (M):    1262.95 ; \t loss: 0.3364\n",
      "Total trained pairs (M):    1263.46 ; \t loss: 0.3343\n",
      "Total trained pairs (M):    1263.98 ; \t loss: 0.3369\n",
      "Total trained pairs (M):    1264.49 ; \t loss: 0.3382\n",
      "Total trained pairs (M):    1265.01 ; \t loss: 0.3363\n",
      "Total trained pairs (M):    1265.52 ; \t loss: 0.3353\n",
      "Total trained pairs (M):    1266.04 ; \t loss: 0.3351\n",
      "Total trained pairs (M):    1266.54 ; \t loss: 0.3339\n",
      "Total trained pairs (M):    1267.05 ; \t loss: 0.3337\n",
      "Total trained pairs (M):    1267.56 ; \t loss: 0.3334\n",
      "Total trained pairs (M):    1268.07 ; \t loss: 0.3320\n",
      "Total trained pairs (M):    1268.58 ; \t loss: 0.3333\n",
      "Total trained pairs (M):    1269.09 ; \t loss: 0.3333\n",
      "Total trained pairs (M):    1269.59 ; \t loss: 0.3315\n",
      "Total trained pairs (M):    1270.11 ; \t loss: 0.3333\n",
      "Total trained pairs (M):    1270.62 ; \t loss: 0.3317\n",
      "Total trained pairs (M):    1271.14 ; \t loss: 0.3320\n",
      "Total trained pairs (M):    1271.66 ; \t loss: 0.3321\n",
      "Total trained pairs (M):    1272.18 ; \t loss: 0.3319\n",
      "Total trained pairs (M):    1272.70 ; \t loss: 0.3310\n",
      "Total trained pairs (M):    1273.23 ; \t loss: 0.3328\n",
      "Total trained pairs (M):    1273.76 ; \t loss: 0.3315\n",
      "Total trained pairs (M):    1274.29 ; \t loss: 0.3287\n",
      "Total trained pairs (M):    1274.82 ; \t loss: 0.3274\n",
      "Total trained pairs (M):    1275.35 ; \t loss: 0.3261\n",
      "Total trained pairs (M):    1275.88 ; \t loss: 0.3271\n",
      "Total trained pairs (M):    1276.41 ; \t loss: 0.3288\n",
      "Total trained pairs (M):    1276.93 ; \t loss: 0.3280\n",
      "Total trained pairs (M):    1277.45 ; \t loss: 0.3269\n",
      "Total trained pairs (M):    1277.97 ; \t loss: 0.3258\n",
      "Total trained pairs (M):    1278.50 ; \t loss: 0.3278\n",
      "Total trained pairs (M):    1279.02 ; \t loss: 0.3263\n",
      "Total trained pairs (M):    1279.54 ; \t loss: 0.3269\n",
      "Total trained pairs (M):    1280.06 ; \t loss: 0.3253\n",
      "Total trained pairs (M):    1280.58 ; \t loss: 0.3249\n",
      "Total trained pairs (M):    1281.09 ; \t loss: 0.3251\n",
      "Total trained pairs (M):    1281.60 ; \t loss: 0.3229\n",
      "Total trained pairs (M):    1282.11 ; \t loss: 0.3225\n",
      "Total trained pairs (M):    1282.64 ; \t loss: 0.3238\n",
      "Total trained pairs (M):    1283.16 ; \t loss: 0.3223\n",
      "Total trained pairs (M):    1283.69 ; \t loss: 0.3207\n",
      "Total trained pairs (M):    1284.22 ; \t loss: 0.3227\n",
      "Total trained pairs (M):    1284.74 ; \t loss: 0.3230\n",
      "Total trained pairs (M):    1285.28 ; \t loss: 0.3280\n",
      "Total trained pairs (M):    1285.81 ; \t loss: 0.3277\n",
      "Total trained pairs (M):    1286.34 ; \t loss: 0.3288\n",
      "Total trained pairs (M):    1286.88 ; \t loss: 0.3264\n",
      "Total trained pairs (M):    1287.41 ; \t loss: 0.3260\n",
      "Total trained pairs (M):    1287.95 ; \t loss: 0.3235\n",
      "Total trained pairs (M):    1288.49 ; \t loss: 0.3228\n",
      "Total trained pairs (M):    1289.02 ; \t loss: 0.3203\n",
      "Total trained pairs (M):    1289.55 ; \t loss: 0.3217\n",
      "Total trained pairs (M):    1290.07 ; \t loss: 0.3208\n",
      "Total trained pairs (M):    1290.61 ; \t loss: 0.3197\n",
      "Total trained pairs (M):    1291.14 ; \t loss: 0.3195\n",
      "Total trained pairs (M):    1291.67 ; \t loss: 0.3184\n",
      "Total trained pairs (M):    1292.19 ; \t loss: 0.3196\n",
      "Total trained pairs (M):    1292.69 ; \t loss: 0.3207\n",
      "Total trained pairs (M):    1293.18 ; \t loss: 0.3201\n",
      "Total trained pairs (M):    1293.66 ; \t loss: 0.3185\n",
      "Total trained pairs (M):    1294.16 ; \t loss: 0.3175\n",
      "Total trained pairs (M):    1294.64 ; \t loss: 0.3165\n",
      "Total trained pairs (M):    1295.13 ; \t loss: 0.3163\n",
      "Total trained pairs (M):    1295.62 ; \t loss: 0.3174\n",
      "Total trained pairs (M):    1296.13 ; \t loss: 0.3172\n",
      "Total trained pairs (M):    1296.65 ; \t loss: 0.3166\n",
      "Total trained pairs (M):    1297.16 ; \t loss: 0.3158\n",
      "Total trained pairs (M):    1297.68 ; \t loss: 0.3169\n",
      "Total trained pairs (M):    1298.20 ; \t loss: 0.3169\n",
      "Total trained pairs (M):    1298.72 ; \t loss: 0.3144\n",
      "Total trained pairs (M):    1299.23 ; \t loss: 0.3132\n",
      "Total trained pairs (M):    1299.75 ; \t loss: 0.3121\n",
      "Total trained pairs (M):    1300.25 ; \t loss: 0.3155\n",
      "Total trained pairs (M):    1300.76 ; \t loss: 0.3125\n",
      "Total trained pairs (M):    1301.26 ; \t loss: 0.3132\n",
      "Total trained pairs (M):    1301.77 ; \t loss: 0.3109\n",
      "Total trained pairs (M):    1302.26 ; \t loss: 0.3124\n",
      "Total trained pairs (M):    1302.73 ; \t loss: 0.3110\n",
      "Total trained pairs (M):    1303.22 ; \t loss: 0.3110\n",
      "Total trained pairs (M):    1303.69 ; \t loss: 0.3100\n",
      "Total trained pairs (M):    1304.15 ; \t loss: 0.3123\n",
      "Total trained pairs (M):    1304.61 ; \t loss: 0.3112\n",
      "Total trained pairs (M):    1305.07 ; \t loss: 0.3096\n",
      "Total trained pairs (M):    1305.51 ; \t loss: 0.3104\n",
      "Total trained pairs (M):    1305.95 ; \t loss: 0.3096\n",
      "Total trained pairs (M):    1306.39 ; \t loss: 0.3099\n",
      "Total trained pairs (M):    1306.84 ; \t loss: 0.3089\n",
      "Total trained pairs (M):    1307.26 ; \t loss: 0.3100\n",
      "Total trained pairs (M):    1307.68 ; \t loss: 0.3124\n",
      "Total trained pairs (M):    1308.11 ; \t loss: 0.3087\n",
      "Total trained pairs (M):    1308.53 ; \t loss: 0.3102\n",
      "Total trained pairs (M):    1308.95 ; \t loss: 0.3161\n",
      "Total trained pairs (M):    1309.37 ; \t loss: 0.3170\n",
      "Total trained pairs (M):    1309.80 ; \t loss: 0.3192\n",
      "Total trained pairs (M):    1310.21 ; \t loss: 0.3183\n",
      "Total trained pairs (M):    1310.63 ; \t loss: 0.3181\n",
      "Total trained pairs (M):    1311.03 ; \t loss: 0.3213\n",
      "Total trained pairs (M):    1311.45 ; \t loss: 0.3213\n",
      "Total trained pairs (M):    1311.87 ; \t loss: 0.3232\n",
      "Total trained pairs (M):    1312.31 ; \t loss: 0.3301\n",
      "Total trained pairs (M):    1312.77 ; \t loss: 0.3350\n",
      "Total trained pairs (M):    1313.22 ; \t loss: 0.3339\n",
      "Total trained pairs (M):    1313.67 ; \t loss: 0.3352\n",
      "Total trained pairs (M):    1314.12 ; \t loss: 0.3324\n",
      "Total trained pairs (M):    1314.57 ; \t loss: 0.3321\n",
      "Total trained pairs (M):    1315.01 ; \t loss: 0.3306\n",
      "Total trained pairs (M):    1315.46 ; \t loss: 0.3303\n",
      "Total trained pairs (M):    1315.90 ; \t loss: 0.3296\n",
      "Total trained pairs (M):    1316.35 ; \t loss: 0.3292\n",
      "Total trained pairs (M):    1316.79 ; \t loss: 0.3285\n",
      "Total trained pairs (M):    1317.24 ; \t loss: 0.3297\n",
      "Total trained pairs (M):    1317.69 ; \t loss: 0.3298\n",
      "Total trained pairs (M):    1318.14 ; \t loss: 0.3282\n",
      "Total trained pairs (M):    1318.59 ; \t loss: 0.3278\n",
      "Total trained pairs (M):    1319.04 ; \t loss: 0.3300\n",
      "Total trained pairs (M):    1319.49 ; \t loss: 0.3277\n",
      "Total trained pairs (M):    1319.93 ; \t loss: 0.3270\n",
      "Total trained pairs (M):    1320.37 ; \t loss: 0.3290\n",
      "Total trained pairs (M):    1320.81 ; \t loss: 0.3269\n",
      "Total trained pairs (M):    1321.25 ; \t loss: 0.3298\n",
      "Total trained pairs (M):    1321.69 ; \t loss: 0.3299\n",
      "Total trained pairs (M):    1322.13 ; \t loss: 0.3274\n",
      "Total trained pairs (M):    1322.57 ; \t loss: 0.3257\n",
      "Total trained pairs (M):    1323.01 ; \t loss: 0.3245\n",
      "Total trained pairs (M):    1323.45 ; \t loss: 0.3232\n",
      "Total trained pairs (M):    1323.89 ; \t loss: 0.3275\n",
      "Total trained pairs (M):    1324.34 ; \t loss: 0.3267\n",
      "Total trained pairs (M):    1324.78 ; \t loss: 0.3272\n",
      "Total trained pairs (M):    1325.23 ; \t loss: 0.3257\n",
      "Total trained pairs (M):    1325.67 ; \t loss: 0.3243\n",
      "Total trained pairs (M):    1326.12 ; \t loss: 0.3259\n",
      "Total trained pairs (M):    1326.56 ; \t loss: 0.3270\n",
      "Total trained pairs (M):    1327.00 ; \t loss: 0.3270\n",
      "Total trained pairs (M):    1327.46 ; \t loss: 0.3268\n",
      "Total trained pairs (M):    1327.91 ; \t loss: 0.3291\n",
      "Total trained pairs (M):    1328.37 ; \t loss: 0.3293\n",
      "Total trained pairs (M):    1328.83 ; \t loss: 0.3289\n",
      "Total trained pairs (M):    1329.30 ; \t loss: 0.3315\n",
      "Total trained pairs (M):    1329.75 ; \t loss: 0.3314\n",
      "Total trained pairs (M):    1330.21 ; \t loss: 0.3306\n",
      "Total trained pairs (M):    1330.67 ; \t loss: 0.3285\n",
      "Total trained pairs (M):    1331.12 ; \t loss: 0.3295\n",
      "Total trained pairs (M):    1331.57 ; \t loss: 0.3286\n",
      "Total trained pairs (M):    1332.01 ; \t loss: 0.3303\n",
      "Total trained pairs (M):    1332.45 ; \t loss: 0.3329\n",
      "Total trained pairs (M):    1332.88 ; \t loss: 0.3326\n",
      "Total trained pairs (M):    1333.32 ; \t loss: 0.3308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trained pairs (M):    1333.75 ; \t loss: 0.3301\n",
      "Total trained pairs (M):    1334.19 ; \t loss: 0.3301\n",
      "Total trained pairs (M):    1334.63 ; \t loss: 0.3285\n",
      "Total trained pairs (M):    1335.11 ; \t loss: 0.3451\n",
      "Total trained pairs (M):    1335.58 ; \t loss: 0.3427\n",
      "Total trained pairs (M):    1336.05 ; \t loss: 0.3406\n",
      "Total trained pairs (M):    1336.52 ; \t loss: 0.3408\n",
      "Total trained pairs (M):    1337.00 ; \t loss: 0.3413\n",
      "Total trained pairs (M):    1337.48 ; \t loss: 0.3406\n",
      "Total trained pairs (M):    1337.96 ; \t loss: 0.3377\n",
      "Total trained pairs (M):    1338.43 ; \t loss: 0.3389\n",
      "Total trained pairs (M):    1338.90 ; \t loss: 0.3368\n",
      "Total trained pairs (M):    1339.38 ; \t loss: 0.3365\n",
      "Total trained pairs (M):    1339.85 ; \t loss: 0.3365\n",
      "Total trained pairs (M):    1340.33 ; \t loss: 0.3341\n",
      "Total trained pairs (M):    1340.81 ; \t loss: 0.3331\n",
      "Total trained pairs (M):    1341.28 ; \t loss: 0.3338\n",
      "Total trained pairs (M):    1341.74 ; \t loss: 0.3355\n",
      "Total trained pairs (M):    1342.19 ; \t loss: 0.3349\n",
      "Total trained pairs (M):    1342.66 ; \t loss: 0.3325\n",
      "Total trained pairs (M):    1343.12 ; \t loss: 0.3356\n",
      "Total trained pairs (M):    1343.59 ; \t loss: 0.3366\n",
      "Total trained pairs (M):    1344.06 ; \t loss: 0.3351\n",
      "Total trained pairs (M):    1344.53 ; \t loss: 0.3342\n",
      "Total trained pairs (M):    1345.01 ; \t loss: 0.3342\n",
      "Total trained pairs (M):    1345.49 ; \t loss: 0.3338\n",
      "Total trained pairs (M):    1345.98 ; \t loss: 0.3329\n",
      "Total trained pairs (M):    1346.47 ; \t loss: 0.3330\n",
      "Total trained pairs (M):    1346.98 ; \t loss: 0.3333\n",
      "Total trained pairs (M):    1347.48 ; \t loss: 0.3327\n",
      "Total trained pairs (M):    1347.99 ; \t loss: 0.3326\n",
      "Total trained pairs (M):    1348.50 ; \t loss: 0.3337\n",
      "Total trained pairs (M):    1349.02 ; \t loss: 0.3323\n",
      "Total trained pairs (M):    1349.53 ; \t loss: 0.3314\n",
      "Total trained pairs (M):    1350.04 ; \t loss: 0.3313\n",
      "Total trained pairs (M):    1350.55 ; \t loss: 0.3290\n",
      "Total trained pairs (M):    1351.06 ; \t loss: 0.3264\n",
      "Total trained pairs (M):    1351.57 ; \t loss: 0.3264\n",
      "Total trained pairs (M):    1352.09 ; \t loss: 0.3254\n",
      "Total trained pairs (M):    1352.61 ; \t loss: 0.3233\n",
      "Total trained pairs (M):    1353.13 ; \t loss: 0.3233\n",
      "Total trained pairs (M):    1353.63 ; \t loss: 0.3237\n",
      "Total trained pairs (M):    1354.14 ; \t loss: 0.3204\n",
      "Total trained pairs (M):    1354.66 ; \t loss: 0.3239\n",
      "Total trained pairs (M):    1355.16 ; \t loss: 0.3235\n",
      "Total trained pairs (M):    1355.66 ; \t loss: 0.3224\n",
      "Total trained pairs (M):    1356.15 ; \t loss: 0.3226\n",
      "Total trained pairs (M):    1356.65 ; \t loss: 0.3219\n",
      "Total trained pairs (M):    1357.14 ; \t loss: 0.3228\n",
      "Total trained pairs (M):    1357.64 ; \t loss: 0.3206\n",
      "Total trained pairs (M):    1358.11 ; \t loss: 0.3232\n",
      "Total trained pairs (M):    1358.60 ; \t loss: 0.3288\n",
      "Total trained pairs (M):    1359.07 ; \t loss: 0.3278\n",
      "Total trained pairs (M):    1359.52 ; \t loss: 0.3290\n",
      "Total trained pairs (M):    1359.97 ; \t loss: 0.3279\n",
      "Total trained pairs (M):    1360.41 ; \t loss: 0.3255\n",
      "Total trained pairs (M):    1360.86 ; \t loss: 0.3262\n",
      "Total trained pairs (M):    1361.32 ; \t loss: 0.3265\n",
      "Total trained pairs (M):    1361.77 ; \t loss: 0.3243\n",
      "Total trained pairs (M):    1362.22 ; \t loss: 0.3243\n",
      "Total trained pairs (M):    1362.69 ; \t loss: 0.3239\n",
      "Total trained pairs (M):    1363.15 ; \t loss: 0.3247\n",
      "Total trained pairs (M):    1363.61 ; \t loss: 0.3239\n",
      "Total trained pairs (M):    1364.07 ; \t loss: 0.3256\n",
      "Total trained pairs (M):    1364.54 ; \t loss: 0.3250\n",
      "Total trained pairs (M):    1365.00 ; \t loss: 0.3234\n",
      "Total trained pairs (M):    1365.46 ; \t loss: 0.3242\n",
      "Total trained pairs (M):    1365.93 ; \t loss: 0.3250\n",
      "Total trained pairs (M):    1366.40 ; \t loss: 0.3277\n",
      "Total trained pairs (M):    1366.87 ; \t loss: 0.3276\n",
      "Total trained pairs (M):    1367.35 ; \t loss: 0.3260\n",
      "Total trained pairs (M):    1367.82 ; \t loss: 0.3268\n",
      "Total trained pairs (M):    1368.30 ; \t loss: 0.3263\n",
      "Total trained pairs (M):    1368.77 ; \t loss: 0.3252\n",
      "Total trained pairs (M):    1369.24 ; \t loss: 0.3263\n",
      "Total trained pairs (M):    1369.72 ; \t loss: 0.3248\n",
      "Total trained pairs (M):    1370.20 ; \t loss: 0.3245\n",
      "Total trained pairs (M):    1370.68 ; \t loss: 0.3217\n",
      "Total trained pairs (M):    1371.15 ; \t loss: 0.3234\n",
      "Total trained pairs (M):    1371.63 ; \t loss: 0.3223\n",
      "Total trained pairs (M):    1372.10 ; \t loss: 0.3228\n",
      "Total trained pairs (M):    1372.57 ; \t loss: 0.3226\n",
      "Total trained pairs (M):    1373.04 ; \t loss: 0.3222\n",
      "Total trained pairs (M):    1373.52 ; \t loss: 0.3222\n",
      "Total trained pairs (M):    1374.00 ; \t loss: 0.3210\n",
      "Total trained pairs (M):    1374.48 ; \t loss: 0.3242\n",
      "Total trained pairs (M):    1374.95 ; \t loss: 0.3225\n",
      "Total trained pairs (M):    1375.42 ; \t loss: 0.3229\n",
      "Total trained pairs (M):    1375.88 ; \t loss: 0.3230\n",
      "Total trained pairs (M):    1376.35 ; \t loss: 0.3298\n",
      "Total trained pairs (M):    1376.82 ; \t loss: 0.3299\n",
      "Total trained pairs (M):    1377.29 ; \t loss: 0.3299\n",
      "Total trained pairs (M):    1377.76 ; \t loss: 0.3281\n",
      "Total trained pairs (M):    1378.23 ; \t loss: 0.3287\n",
      "Total trained pairs (M):    1378.70 ; \t loss: 0.3299\n",
      "Total trained pairs (M):    1379.17 ; \t loss: 0.3307\n",
      "Total trained pairs (M):    1379.64 ; \t loss: 0.3301\n",
      "Total trained pairs (M):    1380.11 ; \t loss: 0.3289\n",
      "Total trained pairs (M):    1380.58 ; \t loss: 0.3294\n",
      "Total trained pairs (M):    1381.05 ; \t loss: 0.3322\n",
      "Total trained pairs (M):    1381.53 ; \t loss: 0.3321\n",
      "Total trained pairs (M):    1382.01 ; \t loss: 0.3330\n",
      "Total trained pairs (M):    1382.49 ; \t loss: 0.3321\n",
      "Total trained pairs (M):    1382.94 ; \t loss: 0.3343\n",
      "Total trained pairs (M):    1383.39 ; \t loss: 0.3374\n",
      "Total trained pairs (M):    1383.85 ; \t loss: 0.3434\n",
      "Total trained pairs (M):    1384.31 ; \t loss: 0.3415\n",
      "Total trained pairs (M):    1384.76 ; \t loss: 0.3430\n",
      "Total trained pairs (M):    1385.21 ; \t loss: 0.3391\n",
      "Total trained pairs (M):    1385.66 ; \t loss: 0.3393\n",
      "Total trained pairs (M):    1386.12 ; \t loss: 0.3379\n",
      "Total trained pairs (M):    1386.58 ; \t loss: 0.3370\n",
      "Total trained pairs (M):    1387.04 ; \t loss: 0.3358\n",
      "Total trained pairs (M):    1387.49 ; \t loss: 0.3350\n",
      "Total trained pairs (M):    1387.94 ; \t loss: 0.3345\n",
      "Total trained pairs (M):    1388.40 ; \t loss: 0.3374\n",
      "Total trained pairs (M):    1388.86 ; \t loss: 0.3383\n",
      "Total trained pairs (M):    1389.32 ; \t loss: 0.3383\n",
      "Total trained pairs (M):    1389.78 ; \t loss: 0.3363\n",
      "Total trained pairs (M):    1390.23 ; \t loss: 0.3379\n",
      "Total trained pairs (M):    1390.68 ; \t loss: 0.3398\n",
      "Total trained pairs (M):    1391.12 ; \t loss: 0.3397\n",
      "Total trained pairs (M):    1391.57 ; \t loss: 0.3422\n",
      "Total trained pairs (M):    1392.01 ; \t loss: 0.3421\n",
      "Total trained pairs (M):    1392.45 ; \t loss: 0.3439\n",
      "Total trained pairs (M):    1392.88 ; \t loss: 0.3436\n",
      "Total trained pairs (M):    1393.30 ; \t loss: 0.3474\n",
      "Total trained pairs (M):    1393.72 ; \t loss: 0.3470\n",
      "Total trained pairs (M):    1394.13 ; \t loss: 0.3460\n",
      "Total trained pairs (M):    1394.54 ; \t loss: 0.3474\n",
      "Total trained pairs (M):    1394.94 ; \t loss: 0.3462\n",
      "Total trained pairs (M):    1395.34 ; \t loss: 0.3511\n",
      "Total trained pairs (M):    1395.75 ; \t loss: 0.3502\n",
      "Total trained pairs (M):    1396.16 ; \t loss: 0.3484\n",
      "Total trained pairs (M):    1396.56 ; \t loss: 0.3480\n",
      "Total trained pairs (M):    1396.96 ; \t loss: 0.3462\n",
      "Total trained pairs (M):    1397.37 ; \t loss: 0.3449\n",
      "Total trained pairs (M):    1397.77 ; \t loss: 0.3488\n",
      "Total trained pairs (M):    1398.18 ; \t loss: 0.3482\n",
      "Total trained pairs (M):    1398.59 ; \t loss: 0.3491\n",
      "Total trained pairs (M):    1399.00 ; \t loss: 0.3480\n",
      "Total trained pairs (M):    1399.41 ; \t loss: 0.3491\n",
      "Total trained pairs (M):    1399.82 ; \t loss: 0.3488\n",
      "Total trained pairs (M):    1400.23 ; \t loss: 0.3553\n",
      "Total trained pairs (M):    1400.64 ; \t loss: 0.3538\n",
      "Total trained pairs (M):    1401.06 ; \t loss: 0.3611\n",
      "Total trained pairs (M):    1401.48 ; \t loss: 0.3624\n",
      "Total trained pairs (M):    1401.90 ; \t loss: 0.3615\n",
      "Total trained pairs (M):    1402.31 ; \t loss: 0.3606\n",
      "Total trained pairs (M):    1402.74 ; \t loss: 0.3649\n",
      "Total trained pairs (M):    1403.17 ; \t loss: 0.3693\n",
      "Total trained pairs (M):    1403.59 ; \t loss: 0.3709\n",
      "Total trained pairs (M):    1404.01 ; \t loss: 0.3694\n",
      "Total trained pairs (M):    1404.44 ; \t loss: 0.3702\n",
      "Total trained pairs (M):    1404.86 ; \t loss: 0.3704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trained pairs (M):    1405.29 ; \t loss: 0.3694\n",
      "Total trained pairs (M):    1405.74 ; \t loss: 0.3759\n",
      "Total trained pairs (M):    1406.19 ; \t loss: 0.3799\n",
      "Total trained pairs (M):    1406.64 ; \t loss: 0.3792\n",
      "Total trained pairs (M):    1407.09 ; \t loss: 0.3769\n",
      "Total trained pairs (M):    1407.53 ; \t loss: 0.3789\n",
      "Total trained pairs (M):    1407.97 ; \t loss: 0.3771\n",
      "Total trained pairs (M):    1408.42 ; \t loss: 0.3910\n",
      "Total trained pairs (M):    1408.86 ; \t loss: 0.3973\n",
      "Total trained pairs (M):    1409.31 ; \t loss: 0.3962\n",
      "Total trained pairs (M):    1409.76 ; \t loss: 0.3939\n",
      "Total trained pairs (M):    1410.20 ; \t loss: 0.3960\n",
      "Total trained pairs (M):    1410.64 ; \t loss: 0.3947\n",
      "Total trained pairs (M):    1411.07 ; \t loss: 0.4002\n",
      "Total trained pairs (M):    1411.50 ; \t loss: 0.3985\n",
      "Total trained pairs (M):    1411.93 ; \t loss: 0.4004\n",
      "Total trained pairs (M):    1412.35 ; \t loss: 0.4017\n",
      "Total trained pairs (M):    1412.78 ; \t loss: 0.4017\n",
      "Total trained pairs (M):    1413.21 ; \t loss: 0.4055\n",
      "Total trained pairs (M):    1413.65 ; \t loss: 0.4149\n",
      "Total trained pairs (M):    1414.09 ; \t loss: 0.4149\n",
      "Total trained pairs (M):    1414.54 ; \t loss: 0.4169\n",
      "Total trained pairs (M):    1414.99 ; \t loss: 0.4155\n",
      "Epoch 7 ======\n",
      "Total trained pairs (M):    1415.51 ; \t loss: 0.5570\n",
      "Total trained pairs (M):    1416.03 ; \t loss: 0.5485\n",
      "Total trained pairs (M):    1416.56 ; \t loss: 0.5338\n",
      "Total trained pairs (M):    1417.08 ; \t loss: 0.5243\n",
      "Total trained pairs (M):    1417.59 ; \t loss: 0.5178\n",
      "Total trained pairs (M):    1418.10 ; \t loss: 0.5085\n",
      "Total trained pairs (M):    1418.61 ; \t loss: 0.5042\n",
      "Total trained pairs (M):    1419.12 ; \t loss: 0.4976\n",
      "Total trained pairs (M):    1419.62 ; \t loss: 0.4920\n",
      "Total trained pairs (M):    1420.12 ; \t loss: 0.4822\n",
      "Total trained pairs (M):    1420.62 ; \t loss: 0.4793\n",
      "Total trained pairs (M):    1421.12 ; \t loss: 0.4754\n",
      "Total trained pairs (M):    1421.62 ; \t loss: 0.4713\n",
      "Total trained pairs (M):    1422.12 ; \t loss: 0.4667\n",
      "Total trained pairs (M):    1422.61 ; \t loss: 0.4617\n",
      "Total trained pairs (M):    1423.09 ; \t loss: 0.4570\n",
      "Total trained pairs (M):    1423.58 ; \t loss: 0.4545\n",
      "Total trained pairs (M):    1424.06 ; \t loss: 0.4499\n",
      "Total trained pairs (M):    1424.54 ; \t loss: 0.4477\n",
      "Total trained pairs (M):    1425.02 ; \t loss: 0.4425\n",
      "Total trained pairs (M):    1425.50 ; \t loss: 0.4398\n",
      "Total trained pairs (M):    1425.98 ; \t loss: 0.4356\n",
      "Total trained pairs (M):    1426.45 ; \t loss: 0.4335\n",
      "Total trained pairs (M):    1426.93 ; \t loss: 0.4313\n",
      "Total trained pairs (M):    1427.40 ; \t loss: 0.4289\n",
      "Total trained pairs (M):    1427.89 ; \t loss: 0.4236\n",
      "Total trained pairs (M):    1428.38 ; \t loss: 0.4178\n",
      "Total trained pairs (M):    1428.84 ; \t loss: 0.4084\n",
      "Total trained pairs (M):    1429.31 ; \t loss: 0.4066\n",
      "Total trained pairs (M):    1429.78 ; \t loss: 0.4030\n",
      "Total trained pairs (M):    1430.25 ; \t loss: 0.4011\n",
      "Total trained pairs (M):    1430.73 ; \t loss: 0.3996\n",
      "Total trained pairs (M):    1431.21 ; \t loss: 0.3979\n",
      "Total trained pairs (M):    1431.69 ; \t loss: 0.3960\n",
      "Total trained pairs (M):    1432.18 ; \t loss: 0.3941\n",
      "Total trained pairs (M):    1432.67 ; \t loss: 0.3912\n",
      "Total trained pairs (M):    1433.15 ; \t loss: 0.3892\n",
      "Total trained pairs (M):    1433.62 ; \t loss: 0.3871\n",
      "Total trained pairs (M):    1434.10 ; \t loss: 0.3846\n",
      "Total trained pairs (M):    1434.57 ; \t loss: 0.3826\n",
      "Total trained pairs (M):    1435.05 ; \t loss: 0.3804\n",
      "Total trained pairs (M):    1435.53 ; \t loss: 0.3718\n",
      "Total trained pairs (M):    1436.01 ; \t loss: 0.3697\n",
      "Total trained pairs (M):    1436.49 ; \t loss: 0.3676\n",
      "Total trained pairs (M):    1436.97 ; \t loss: 0.3654\n",
      "Total trained pairs (M):    1437.46 ; \t loss: 0.3633\n",
      "Total trained pairs (M):    1437.94 ; \t loss: 0.3619\n",
      "Total trained pairs (M):    1438.42 ; \t loss: 0.3612\n",
      "Total trained pairs (M):    1438.90 ; \t loss: 0.3588\n",
      "Total trained pairs (M):    1439.38 ; \t loss: 0.3557\n",
      "Total trained pairs (M):    1439.87 ; \t loss: 0.3547\n",
      "Total trained pairs (M):    1440.36 ; \t loss: 0.3530\n",
      "Total trained pairs (M):    1440.84 ; \t loss: 0.3515\n",
      "Total trained pairs (M):    1441.33 ; \t loss: 0.3489\n",
      "Total trained pairs (M):    1441.82 ; \t loss: 0.3461\n",
      "Total trained pairs (M):    1442.32 ; \t loss: 0.3443\n",
      "Total trained pairs (M):    1442.83 ; \t loss: 0.3478\n",
      "Total trained pairs (M):    1443.36 ; \t loss: 0.3481\n",
      "Total trained pairs (M):    1443.89 ; \t loss: 0.3475\n",
      "Total trained pairs (M):    1444.41 ; \t loss: 0.3458\n",
      "Total trained pairs (M):    1444.92 ; \t loss: 0.3424\n",
      "Total trained pairs (M):    1445.43 ; \t loss: 0.3424\n",
      "Total trained pairs (M):    1445.94 ; \t loss: 0.3408\n",
      "Total trained pairs (M):    1446.45 ; \t loss: 0.3383\n",
      "Total trained pairs (M):    1446.95 ; \t loss: 0.3349\n",
      "Total trained pairs (M):    1447.43 ; \t loss: 0.3336\n",
      "Total trained pairs (M):    1447.92 ; \t loss: 0.3310\n",
      "Total trained pairs (M):    1448.40 ; \t loss: 0.3297\n",
      "Total trained pairs (M):    1448.89 ; \t loss: 0.3287\n",
      "Total trained pairs (M):    1449.38 ; \t loss: 0.3295\n",
      "Total trained pairs (M):    1449.87 ; \t loss: 0.3270\n",
      "Total trained pairs (M):    1450.37 ; \t loss: 0.3304\n",
      "Total trained pairs (M):    1450.87 ; \t loss: 0.3286\n",
      "Total trained pairs (M):    1451.36 ; \t loss: 0.3274\n",
      "Total trained pairs (M):    1451.85 ; \t loss: 0.3250\n",
      "Total trained pairs (M):    1452.35 ; \t loss: 0.3267\n",
      "Total trained pairs (M):    1452.85 ; \t loss: 0.3286\n",
      "Total trained pairs (M):    1453.35 ; \t loss: 0.3279\n",
      "Total trained pairs (M):    1453.84 ; \t loss: 0.3286\n",
      "Total trained pairs (M):    1454.35 ; \t loss: 0.3315\n",
      "Total trained pairs (M):    1454.86 ; \t loss: 0.3295\n",
      "Total trained pairs (M):    1455.37 ; \t loss: 0.3297\n",
      "Total trained pairs (M):    1455.89 ; \t loss: 0.3300\n",
      "Total trained pairs (M):    1456.41 ; \t loss: 0.3284\n",
      "Total trained pairs (M):    1456.93 ; \t loss: 0.3279\n",
      "Total trained pairs (M):    1457.44 ; \t loss: 0.3259\n",
      "Total trained pairs (M):    1457.97 ; \t loss: 0.3319\n",
      "Total trained pairs (M):    1458.51 ; \t loss: 0.3312\n",
      "Total trained pairs (M):    1459.05 ; \t loss: 0.3306\n",
      "Total trained pairs (M):    1459.58 ; \t loss: 0.3296\n",
      "Total trained pairs (M):    1460.13 ; \t loss: 0.3295\n",
      "Total trained pairs (M):    1460.68 ; \t loss: 0.3306\n",
      "Total trained pairs (M):    1461.23 ; \t loss: 0.3287\n",
      "Total trained pairs (M):    1461.79 ; \t loss: 0.3285\n",
      "Total trained pairs (M):    1462.34 ; \t loss: 0.3269\n",
      "Total trained pairs (M):    1462.91 ; \t loss: 0.3275\n",
      "Total trained pairs (M):    1463.46 ; \t loss: 0.3271\n",
      "Total trained pairs (M):    1464.02 ; \t loss: 0.3245\n",
      "Total trained pairs (M):    1464.57 ; \t loss: 0.3239\n",
      "Total trained pairs (M):    1465.09 ; \t loss: 0.3272\n",
      "Total trained pairs (M):    1465.61 ; \t loss: 0.3256\n",
      "Total trained pairs (M):    1466.13 ; \t loss: 0.3268\n",
      "Total trained pairs (M):    1466.64 ; \t loss: 0.3277\n",
      "Total trained pairs (M):    1467.15 ; \t loss: 0.3265\n",
      "Total trained pairs (M):    1467.66 ; \t loss: 0.3243\n",
      "Total trained pairs (M):    1468.18 ; \t loss: 0.3233\n",
      "Total trained pairs (M):    1468.68 ; \t loss: 0.3233\n",
      "Total trained pairs (M):    1469.19 ; \t loss: 0.3229\n",
      "Total trained pairs (M):    1469.70 ; \t loss: 0.3228\n",
      "Total trained pairs (M):    1470.21 ; \t loss: 0.3226\n",
      "Total trained pairs (M):    1470.72 ; \t loss: 0.3240\n",
      "Total trained pairs (M):    1471.23 ; \t loss: 0.3215\n",
      "Total trained pairs (M):    1471.74 ; \t loss: 0.3210\n",
      "Total trained pairs (M):    1472.25 ; \t loss: 0.3229\n",
      "Total trained pairs (M):    1472.76 ; \t loss: 0.3207\n",
      "Total trained pairs (M):    1473.28 ; \t loss: 0.3217\n",
      "Total trained pairs (M):    1473.80 ; \t loss: 0.3221\n",
      "Total trained pairs (M):    1474.32 ; \t loss: 0.3217\n",
      "Total trained pairs (M):    1474.85 ; \t loss: 0.3208\n",
      "Total trained pairs (M):    1475.38 ; \t loss: 0.3216\n",
      "Total trained pairs (M):    1475.91 ; \t loss: 0.3206\n",
      "Total trained pairs (M):    1476.43 ; \t loss: 0.3192\n",
      "Total trained pairs (M):    1476.96 ; \t loss: 0.3182\n",
      "Total trained pairs (M):    1477.49 ; \t loss: 0.3177\n",
      "Total trained pairs (M):    1478.03 ; \t loss: 0.3164\n",
      "Total trained pairs (M):    1478.55 ; \t loss: 0.3182\n",
      "Total trained pairs (M):    1479.07 ; \t loss: 0.3183\n",
      "Total trained pairs (M):    1479.59 ; \t loss: 0.3161\n",
      "Total trained pairs (M):    1480.11 ; \t loss: 0.3166\n",
      "Total trained pairs (M):    1480.64 ; \t loss: 0.3172\n",
      "Total trained pairs (M):    1481.16 ; \t loss: 0.3173\n",
      "Total trained pairs (M):    1481.68 ; \t loss: 0.3160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trained pairs (M):    1482.20 ; \t loss: 0.3153\n",
      "Total trained pairs (M):    1482.72 ; \t loss: 0.3154\n",
      "Total trained pairs (M):    1483.23 ; \t loss: 0.3139\n",
      "Total trained pairs (M):    1483.74 ; \t loss: 0.3128\n",
      "Total trained pairs (M):    1484.25 ; \t loss: 0.3130\n",
      "Total trained pairs (M):    1484.78 ; \t loss: 0.3154\n",
      "Total trained pairs (M):    1485.31 ; \t loss: 0.3133\n",
      "Total trained pairs (M):    1485.83 ; \t loss: 0.3114\n",
      "Total trained pairs (M):    1486.36 ; \t loss: 0.3138\n",
      "Total trained pairs (M):    1486.88 ; \t loss: 0.3131\n",
      "Total trained pairs (M):    1487.42 ; \t loss: 0.3177\n",
      "Total trained pairs (M):    1487.95 ; \t loss: 0.3185\n",
      "Total trained pairs (M):    1488.48 ; \t loss: 0.3193\n",
      "Total trained pairs (M):    1489.02 ; \t loss: 0.3170\n",
      "Total trained pairs (M):    1489.56 ; \t loss: 0.3163\n",
      "Total trained pairs (M):    1490.09 ; \t loss: 0.3149\n",
      "Total trained pairs (M):    1490.63 ; \t loss: 0.3138\n",
      "Total trained pairs (M):    1491.16 ; \t loss: 0.3122\n",
      "Total trained pairs (M):    1491.69 ; \t loss: 0.3115\n",
      "Total trained pairs (M):    1492.22 ; \t loss: 0.3116\n",
      "Total trained pairs (M):    1492.75 ; \t loss: 0.3123\n",
      "Total trained pairs (M):    1493.28 ; \t loss: 0.3096\n",
      "Total trained pairs (M):    1493.81 ; \t loss: 0.3088\n",
      "Total trained pairs (M):    1494.33 ; \t loss: 0.3111\n",
      "Total trained pairs (M):    1494.83 ; \t loss: 0.3109\n",
      "Total trained pairs (M):    1495.32 ; \t loss: 0.3104\n",
      "Total trained pairs (M):    1495.81 ; \t loss: 0.3095\n",
      "Total trained pairs (M):    1496.30 ; \t loss: 0.3091\n",
      "Total trained pairs (M):    1496.79 ; \t loss: 0.3072\n",
      "Total trained pairs (M):    1497.27 ; \t loss: 0.3065\n",
      "Total trained pairs (M):    1497.76 ; \t loss: 0.3085\n",
      "Total trained pairs (M):    1498.28 ; \t loss: 0.3083\n",
      "Total trained pairs (M):    1498.79 ; \t loss: 0.3081\n",
      "Total trained pairs (M):    1499.30 ; \t loss: 0.3064\n",
      "Total trained pairs (M):    1499.82 ; \t loss: 0.3086\n",
      "Total trained pairs (M):    1500.34 ; \t loss: 0.3070\n",
      "Total trained pairs (M):    1500.86 ; \t loss: 0.3063\n",
      "Total trained pairs (M):    1501.37 ; \t loss: 0.3052\n",
      "Total trained pairs (M):    1501.89 ; \t loss: 0.3052\n",
      "Total trained pairs (M):    1502.39 ; \t loss: 0.3056\n",
      "Total trained pairs (M):    1502.90 ; \t loss: 0.3052\n",
      "Total trained pairs (M):    1503.40 ; \t loss: 0.3035\n",
      "Total trained pairs (M):    1503.91 ; \t loss: 0.3034\n",
      "Total trained pairs (M):    1504.40 ; \t loss: 0.3033\n",
      "Total trained pairs (M):    1504.88 ; \t loss: 0.3032\n",
      "Total trained pairs (M):    1505.36 ; \t loss: 0.3025\n",
      "Total trained pairs (M):    1505.83 ; \t loss: 0.3022\n",
      "Total trained pairs (M):    1506.29 ; \t loss: 0.3031\n",
      "Total trained pairs (M):    1506.75 ; \t loss: 0.3037\n",
      "Total trained pairs (M):    1507.21 ; \t loss: 0.3026\n",
      "Total trained pairs (M):    1507.65 ; \t loss: 0.3022\n",
      "Total trained pairs (M):    1508.09 ; \t loss: 0.3025\n",
      "Total trained pairs (M):    1508.53 ; \t loss: 0.3010\n",
      "Total trained pairs (M):    1508.98 ; \t loss: 0.3020\n",
      "Total trained pairs (M):    1509.40 ; \t loss: 0.3021\n",
      "Total trained pairs (M):    1509.83 ; \t loss: 0.3036\n",
      "Total trained pairs (M):    1510.25 ; \t loss: 0.3017\n",
      "Total trained pairs (M):    1510.67 ; \t loss: 0.3021\n",
      "Total trained pairs (M):    1511.10 ; \t loss: 0.3092\n",
      "Total trained pairs (M):    1511.51 ; \t loss: 0.3091\n",
      "Total trained pairs (M):    1511.94 ; \t loss: 0.3130\n",
      "Total trained pairs (M):    1512.35 ; \t loss: 0.3123\n",
      "Total trained pairs (M):    1512.77 ; \t loss: 0.3106\n",
      "Total trained pairs (M):    1513.17 ; \t loss: 0.3135\n",
      "Total trained pairs (M):    1513.59 ; \t loss: 0.3156\n",
      "Total trained pairs (M):    1514.01 ; \t loss: 0.3164\n",
      "Total trained pairs (M):    1514.45 ; \t loss: 0.3223\n",
      "Total trained pairs (M):    1514.91 ; \t loss: 0.3276\n",
      "Total trained pairs (M):    1515.37 ; \t loss: 0.3268\n",
      "Total trained pairs (M):    1515.81 ; \t loss: 0.3291\n",
      "Total trained pairs (M):    1516.26 ; \t loss: 0.3268\n",
      "Total trained pairs (M):    1516.71 ; \t loss: 0.3256\n",
      "Total trained pairs (M):    1517.15 ; \t loss: 0.3243\n",
      "Total trained pairs (M):    1517.60 ; \t loss: 0.3235\n",
      "Total trained pairs (M):    1518.04 ; \t loss: 0.3227\n",
      "Total trained pairs (M):    1518.49 ; \t loss: 0.3226\n",
      "Total trained pairs (M):    1518.93 ; \t loss: 0.3231\n",
      "Total trained pairs (M):    1519.38 ; \t loss: 0.3223\n",
      "Total trained pairs (M):    1519.83 ; \t loss: 0.3220\n",
      "Total trained pairs (M):    1520.28 ; \t loss: 0.3230\n",
      "Total trained pairs (M):    1520.73 ; \t loss: 0.3211\n",
      "Total trained pairs (M):    1521.18 ; \t loss: 0.3238\n",
      "Total trained pairs (M):    1521.63 ; \t loss: 0.3197\n",
      "Total trained pairs (M):    1522.07 ; \t loss: 0.3208\n",
      "Total trained pairs (M):    1522.51 ; \t loss: 0.3233\n",
      "Total trained pairs (M):    1522.95 ; \t loss: 0.3220\n",
      "Total trained pairs (M):    1523.40 ; \t loss: 0.3232\n",
      "Total trained pairs (M):    1523.83 ; \t loss: 0.3233\n",
      "Total trained pairs (M):    1524.27 ; \t loss: 0.3216\n",
      "Total trained pairs (M):    1524.71 ; \t loss: 0.3186\n",
      "Total trained pairs (M):    1525.15 ; \t loss: 0.3208\n",
      "Total trained pairs (M):    1525.59 ; \t loss: 0.3178\n",
      "Total trained pairs (M):    1526.04 ; \t loss: 0.3213\n",
      "Total trained pairs (M):    1526.48 ; \t loss: 0.3199\n",
      "Total trained pairs (M):    1526.92 ; \t loss: 0.3196\n",
      "Total trained pairs (M):    1527.37 ; \t loss: 0.3184\n",
      "Total trained pairs (M):    1527.81 ; \t loss: 0.3180\n",
      "Total trained pairs (M):    1528.26 ; \t loss: 0.3217\n",
      "Total trained pairs (M):    1528.70 ; \t loss: 0.3204\n",
      "Total trained pairs (M):    1529.15 ; \t loss: 0.3197\n",
      "Total trained pairs (M):    1529.60 ; \t loss: 0.3220\n",
      "Total trained pairs (M):    1530.05 ; \t loss: 0.3238\n",
      "Total trained pairs (M):    1530.51 ; \t loss: 0.3231\n",
      "Total trained pairs (M):    1530.97 ; \t loss: 0.3238\n",
      "Total trained pairs (M):    1531.44 ; \t loss: 0.3251\n",
      "Total trained pairs (M):    1531.89 ; \t loss: 0.3252\n",
      "Total trained pairs (M):    1532.35 ; \t loss: 0.3239\n",
      "Total trained pairs (M):    1532.81 ; \t loss: 0.3225\n",
      "Total trained pairs (M):    1533.26 ; \t loss: 0.3246\n",
      "Total trained pairs (M):    1533.72 ; \t loss: 0.3219\n",
      "Total trained pairs (M):    1534.15 ; \t loss: 0.3240\n",
      "Total trained pairs (M):    1534.59 ; \t loss: 0.3266\n",
      "Total trained pairs (M):    1535.02 ; \t loss: 0.3247\n",
      "Total trained pairs (M):    1535.46 ; \t loss: 0.3258\n",
      "Total trained pairs (M):    1535.89 ; \t loss: 0.3236\n",
      "Total trained pairs (M):    1536.33 ; \t loss: 0.3235\n",
      "Total trained pairs (M):    1536.77 ; \t loss: 0.3242\n",
      "Total trained pairs (M):    1537.25 ; \t loss: 0.3380\n",
      "Total trained pairs (M):    1537.72 ; \t loss: 0.3359\n",
      "Total trained pairs (M):    1538.19 ; \t loss: 0.3349\n",
      "Total trained pairs (M):    1538.66 ; \t loss: 0.3356\n",
      "Total trained pairs (M):    1539.14 ; \t loss: 0.3353\n",
      "Total trained pairs (M):    1539.62 ; \t loss: 0.3344\n",
      "Total trained pairs (M):    1540.10 ; \t loss: 0.3329\n",
      "Total trained pairs (M):    1540.57 ; \t loss: 0.3334\n",
      "Total trained pairs (M):    1541.05 ; \t loss: 0.3320\n",
      "Total trained pairs (M):    1541.52 ; \t loss: 0.3306\n",
      "Total trained pairs (M):    1541.99 ; \t loss: 0.3300\n",
      "Total trained pairs (M):    1542.47 ; \t loss: 0.3280\n",
      "Total trained pairs (M):    1542.95 ; \t loss: 0.3282\n",
      "Total trained pairs (M):    1543.42 ; \t loss: 0.3296\n",
      "Total trained pairs (M):    1543.88 ; \t loss: 0.3293\n",
      "Total trained pairs (M):    1544.33 ; \t loss: 0.3279\n",
      "Total trained pairs (M):    1544.80 ; \t loss: 0.3281\n",
      "Total trained pairs (M):    1545.27 ; \t loss: 0.3296\n",
      "Total trained pairs (M):    1545.73 ; \t loss: 0.3300\n",
      "Total trained pairs (M):    1546.20 ; \t loss: 0.3293\n",
      "Total trained pairs (M):    1546.67 ; \t loss: 0.3279\n",
      "Total trained pairs (M):    1547.15 ; \t loss: 0.3280\n",
      "Total trained pairs (M):    1547.63 ; \t loss: 0.3278\n",
      "Total trained pairs (M):    1548.12 ; \t loss: 0.3258\n",
      "Total trained pairs (M):    1548.61 ; \t loss: 0.3280\n",
      "Total trained pairs (M):    1549.12 ; \t loss: 0.3284\n",
      "Total trained pairs (M):    1549.62 ; \t loss: 0.3250\n",
      "Total trained pairs (M):    1550.13 ; \t loss: 0.3268\n",
      "Total trained pairs (M):    1550.65 ; \t loss: 0.3278\n",
      "Total trained pairs (M):    1551.16 ; \t loss: 0.3267\n",
      "Total trained pairs (M):    1551.67 ; \t loss: 0.3258\n",
      "Total trained pairs (M):    1552.18 ; \t loss: 0.3249\n",
      "Total trained pairs (M):    1552.69 ; \t loss: 0.3227\n",
      "Total trained pairs (M):    1553.20 ; \t loss: 0.3196\n",
      "Total trained pairs (M):    1553.72 ; \t loss: 0.3210\n",
      "Total trained pairs (M):    1554.23 ; \t loss: 0.3191\n",
      "Total trained pairs (M):    1554.75 ; \t loss: 0.3174\n",
      "Total trained pairs (M):    1555.27 ; \t loss: 0.3171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trained pairs (M):    1555.78 ; \t loss: 0.3166\n",
      "Total trained pairs (M):    1556.28 ; \t loss: 0.3155\n",
      "Total trained pairs (M):    1556.80 ; \t loss: 0.3171\n",
      "Total trained pairs (M):    1557.30 ; \t loss: 0.3187\n",
      "Total trained pairs (M):    1557.80 ; \t loss: 0.3162\n",
      "Total trained pairs (M):    1558.29 ; \t loss: 0.3166\n",
      "Total trained pairs (M):    1558.79 ; \t loss: 0.3167\n",
      "Total trained pairs (M):    1559.28 ; \t loss: 0.3151\n",
      "Total trained pairs (M):    1559.78 ; \t loss: 0.3147\n",
      "Total trained pairs (M):    1560.26 ; \t loss: 0.3173\n",
      "Total trained pairs (M):    1560.74 ; \t loss: 0.3219\n",
      "Total trained pairs (M):    1561.21 ; \t loss: 0.3218\n",
      "Total trained pairs (M):    1561.66 ; \t loss: 0.3211\n",
      "Total trained pairs (M):    1562.11 ; \t loss: 0.3211\n",
      "Total trained pairs (M):    1562.55 ; \t loss: 0.3196\n",
      "Total trained pairs (M):    1563.00 ; \t loss: 0.3186\n",
      "Total trained pairs (M):    1563.46 ; \t loss: 0.3193\n",
      "Total trained pairs (M):    1563.91 ; \t loss: 0.3177\n",
      "Total trained pairs (M):    1564.37 ; \t loss: 0.3174\n",
      "Total trained pairs (M):    1564.83 ; \t loss: 0.3181\n",
      "Total trained pairs (M):    1565.29 ; \t loss: 0.3182\n",
      "Total trained pairs (M):    1565.75 ; \t loss: 0.3177\n",
      "Total trained pairs (M):    1566.22 ; \t loss: 0.3200\n",
      "Total trained pairs (M):    1566.68 ; \t loss: 0.3176\n",
      "Total trained pairs (M):    1567.14 ; \t loss: 0.3164\n",
      "Total trained pairs (M):    1567.61 ; \t loss: 0.3173\n",
      "Total trained pairs (M):    1568.07 ; \t loss: 0.3181\n",
      "Total trained pairs (M):    1568.54 ; \t loss: 0.3218\n",
      "Total trained pairs (M):    1569.01 ; \t loss: 0.3205\n",
      "Total trained pairs (M):    1569.49 ; \t loss: 0.3193\n",
      "Total trained pairs (M):    1569.97 ; \t loss: 0.3196\n",
      "Total trained pairs (M):    1570.44 ; \t loss: 0.3175\n",
      "Total trained pairs (M):    1570.91 ; \t loss: 0.3188\n",
      "Total trained pairs (M):    1571.38 ; \t loss: 0.3185\n",
      "Total trained pairs (M):    1571.86 ; \t loss: 0.3181\n",
      "Total trained pairs (M):    1572.34 ; \t loss: 0.3145\n",
      "Total trained pairs (M):    1572.82 ; \t loss: 0.3145\n",
      "Total trained pairs (M):    1573.29 ; \t loss: 0.3191\n",
      "Total trained pairs (M):    1573.77 ; \t loss: 0.3164\n",
      "Total trained pairs (M):    1574.24 ; \t loss: 0.3167\n",
      "Total trained pairs (M):    1574.71 ; \t loss: 0.3160\n",
      "Total trained pairs (M):    1575.19 ; \t loss: 0.3150\n",
      "Total trained pairs (M):    1575.66 ; \t loss: 0.3151\n",
      "Total trained pairs (M):    1576.14 ; \t loss: 0.3156\n",
      "Total trained pairs (M):    1576.62 ; \t loss: 0.3180\n",
      "Total trained pairs (M):    1577.09 ; \t loss: 0.3155\n",
      "Total trained pairs (M):    1577.56 ; \t loss: 0.3153\n",
      "Total trained pairs (M):    1578.02 ; \t loss: 0.3151\n",
      "Total trained pairs (M):    1578.49 ; \t loss: 0.3230\n",
      "Total trained pairs (M):    1578.96 ; \t loss: 0.3237\n",
      "Total trained pairs (M):    1579.43 ; \t loss: 0.3230\n",
      "Total trained pairs (M):    1579.90 ; \t loss: 0.3212\n",
      "Total trained pairs (M):    1580.37 ; \t loss: 0.3229\n",
      "Total trained pairs (M):    1580.84 ; \t loss: 0.3230\n",
      "Total trained pairs (M):    1581.32 ; \t loss: 0.3217\n",
      "Total trained pairs (M):    1581.79 ; \t loss: 0.3227\n",
      "Total trained pairs (M):    1582.25 ; \t loss: 0.3219\n",
      "Total trained pairs (M):    1582.72 ; \t loss: 0.3217\n",
      "Total trained pairs (M):    1583.19 ; \t loss: 0.3244\n",
      "Total trained pairs (M):    1583.67 ; \t loss: 0.3251\n",
      "Total trained pairs (M):    1584.15 ; \t loss: 0.3253\n",
      "Total trained pairs (M):    1584.63 ; \t loss: 0.3261\n",
      "Total trained pairs (M):    1585.08 ; \t loss: 0.3266\n",
      "Total trained pairs (M):    1585.53 ; \t loss: 0.3299\n",
      "Total trained pairs (M):    1585.99 ; \t loss: 0.3351\n",
      "Total trained pairs (M):    1586.45 ; \t loss: 0.3332\n",
      "Total trained pairs (M):    1586.90 ; \t loss: 0.3335\n",
      "Total trained pairs (M):    1587.35 ; \t loss: 0.3337\n",
      "Total trained pairs (M):    1587.81 ; \t loss: 0.3330\n",
      "Total trained pairs (M):    1588.26 ; \t loss: 0.3306\n",
      "Total trained pairs (M):    1588.72 ; \t loss: 0.3287\n",
      "Total trained pairs (M):    1589.18 ; \t loss: 0.3280\n",
      "Total trained pairs (M):    1589.63 ; \t loss: 0.3281\n",
      "Total trained pairs (M):    1590.09 ; \t loss: 0.3256\n",
      "Total trained pairs (M):    1590.54 ; \t loss: 0.3303\n",
      "Total trained pairs (M):    1591.00 ; \t loss: 0.3309\n",
      "Total trained pairs (M):    1591.46 ; \t loss: 0.3318\n",
      "Total trained pairs (M):    1591.92 ; \t loss: 0.3290\n",
      "Total trained pairs (M):    1592.37 ; \t loss: 0.3319\n",
      "Total trained pairs (M):    1592.82 ; \t loss: 0.3331\n",
      "Total trained pairs (M):    1593.27 ; \t loss: 0.3322\n",
      "Total trained pairs (M):    1593.71 ; \t loss: 0.3348\n",
      "Total trained pairs (M):    1594.15 ; \t loss: 0.3358\n",
      "Total trained pairs (M):    1594.59 ; \t loss: 0.3355\n",
      "Total trained pairs (M):    1595.02 ; \t loss: 0.3359\n",
      "Total trained pairs (M):    1595.44 ; \t loss: 0.3389\n",
      "Total trained pairs (M):    1595.86 ; \t loss: 0.3386\n",
      "Total trained pairs (M):    1596.27 ; \t loss: 0.3416\n",
      "Total trained pairs (M):    1596.68 ; \t loss: 0.3406\n",
      "Total trained pairs (M):    1597.08 ; \t loss: 0.3399\n",
      "Total trained pairs (M):    1597.49 ; \t loss: 0.3445\n",
      "Total trained pairs (M):    1597.89 ; \t loss: 0.3422\n",
      "Total trained pairs (M):    1598.30 ; \t loss: 0.3402\n",
      "Total trained pairs (M):    1598.70 ; \t loss: 0.3386\n",
      "Total trained pairs (M):    1599.11 ; \t loss: 0.3406\n",
      "Total trained pairs (M):    1599.51 ; \t loss: 0.3384\n",
      "Total trained pairs (M):    1599.92 ; \t loss: 0.3414\n",
      "Total trained pairs (M):    1600.32 ; \t loss: 0.3423\n",
      "Total trained pairs (M):    1600.73 ; \t loss: 0.3424\n",
      "Total trained pairs (M):    1601.14 ; \t loss: 0.3421\n",
      "Total trained pairs (M):    1601.55 ; \t loss: 0.3420\n",
      "Total trained pairs (M):    1601.96 ; \t loss: 0.3412\n",
      "Total trained pairs (M):    1602.37 ; \t loss: 0.3482\n",
      "Total trained pairs (M):    1602.79 ; \t loss: 0.3477\n",
      "Total trained pairs (M):    1603.20 ; \t loss: 0.3555\n",
      "Total trained pairs (M):    1603.62 ; \t loss: 0.3544\n",
      "Total trained pairs (M):    1604.04 ; \t loss: 0.3556\n",
      "Total trained pairs (M):    1604.45 ; \t loss: 0.3543\n",
      "Total trained pairs (M):    1604.88 ; \t loss: 0.3578\n",
      "Total trained pairs (M):    1605.31 ; \t loss: 0.3615\n",
      "Total trained pairs (M):    1605.73 ; \t loss: 0.3630\n",
      "Total trained pairs (M):    1606.15 ; \t loss: 0.3661\n",
      "Total trained pairs (M):    1606.58 ; \t loss: 0.3635\n",
      "Total trained pairs (M):    1607.01 ; \t loss: 0.3642\n",
      "Total trained pairs (M):    1607.43 ; \t loss: 0.3654\n",
      "Total trained pairs (M):    1607.88 ; \t loss: 0.3678\n",
      "Total trained pairs (M):    1608.33 ; \t loss: 0.3741\n",
      "Total trained pairs (M):    1608.78 ; \t loss: 0.3738\n",
      "Total trained pairs (M):    1609.23 ; \t loss: 0.3727\n",
      "Total trained pairs (M):    1609.67 ; \t loss: 0.3732\n",
      "Total trained pairs (M):    1610.11 ; \t loss: 0.3726\n",
      "Total trained pairs (M):    1610.56 ; \t loss: 0.3850\n",
      "Total trained pairs (M):    1611.01 ; \t loss: 0.3909\n",
      "Total trained pairs (M):    1611.46 ; \t loss: 0.3924\n",
      "Total trained pairs (M):    1611.90 ; \t loss: 0.3907\n",
      "Total trained pairs (M):    1612.34 ; \t loss: 0.3915\n",
      "Total trained pairs (M):    1612.78 ; \t loss: 0.3904\n",
      "Total trained pairs (M):    1613.22 ; \t loss: 0.3952\n",
      "Total trained pairs (M):    1613.65 ; \t loss: 0.3951\n",
      "Total trained pairs (M):    1614.07 ; \t loss: 0.3958\n",
      "Total trained pairs (M):    1614.50 ; \t loss: 0.3966\n",
      "Total trained pairs (M):    1614.92 ; \t loss: 0.4003\n",
      "Total trained pairs (M):    1615.35 ; \t loss: 0.4015\n",
      "Total trained pairs (M):    1615.79 ; \t loss: 0.4118\n",
      "Total trained pairs (M):    1616.23 ; \t loss: 0.4124\n",
      "Total trained pairs (M):    1616.68 ; \t loss: 0.4134\n",
      "Total trained pairs (M):    1617.14 ; \t loss: 0.4124\n",
      "Epoch 8 ======\n",
      "Total trained pairs (M):    1617.65 ; \t loss: 0.5523\n",
      "Total trained pairs (M):    1618.18 ; \t loss: 0.5425\n",
      "Total trained pairs (M):    1618.70 ; \t loss: 0.5300\n",
      "Total trained pairs (M):    1619.22 ; \t loss: 0.5185\n",
      "Total trained pairs (M):    1619.73 ; \t loss: 0.5113\n",
      "Total trained pairs (M):    1620.24 ; \t loss: 0.5053\n",
      "Total trained pairs (M):    1620.76 ; \t loss: 0.4989\n",
      "Total trained pairs (M):    1621.26 ; \t loss: 0.4923\n",
      "Total trained pairs (M):    1621.76 ; \t loss: 0.4871\n",
      "Total trained pairs (M):    1622.26 ; \t loss: 0.4770\n",
      "Total trained pairs (M):    1622.76 ; \t loss: 0.4734\n",
      "Total trained pairs (M):    1623.26 ; \t loss: 0.4667\n",
      "Total trained pairs (M):    1623.76 ; \t loss: 0.4641\n",
      "Total trained pairs (M):    1624.26 ; \t loss: 0.4610\n",
      "Total trained pairs (M):    1624.75 ; \t loss: 0.4550\n",
      "Total trained pairs (M):    1625.24 ; \t loss: 0.4489\n",
      "Total trained pairs (M):    1625.72 ; \t loss: 0.4475\n",
      "Total trained pairs (M):    1626.20 ; \t loss: 0.4435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trained pairs (M):    1626.68 ; \t loss: 0.4396\n",
      "Total trained pairs (M):    1627.16 ; \t loss: 0.4358\n",
      "Total trained pairs (M):    1627.64 ; \t loss: 0.4321\n",
      "Total trained pairs (M):    1628.12 ; \t loss: 0.4292\n",
      "Total trained pairs (M):    1628.59 ; \t loss: 0.4251\n",
      "Total trained pairs (M):    1629.07 ; \t loss: 0.4240\n",
      "Total trained pairs (M):    1629.54 ; \t loss: 0.4217\n",
      "Total trained pairs (M):    1630.03 ; \t loss: 0.4144\n",
      "Total trained pairs (M):    1630.52 ; \t loss: 0.4118\n",
      "Total trained pairs (M):    1630.98 ; \t loss: 0.4010\n",
      "Total trained pairs (M):    1631.45 ; \t loss: 0.3996\n",
      "Total trained pairs (M):    1631.92 ; \t loss: 0.3963\n",
      "Total trained pairs (M):    1632.39 ; \t loss: 0.3932\n",
      "Total trained pairs (M):    1632.87 ; \t loss: 0.3923\n",
      "Total trained pairs (M):    1633.35 ; \t loss: 0.3894\n",
      "Total trained pairs (M):    1633.83 ; \t loss: 0.3879\n",
      "Total trained pairs (M):    1634.32 ; \t loss: 0.3860\n",
      "Total trained pairs (M):    1634.81 ; \t loss: 0.3837\n",
      "Total trained pairs (M):    1635.29 ; \t loss: 0.3806\n",
      "Total trained pairs (M):    1635.76 ; \t loss: 0.3790\n",
      "Total trained pairs (M):    1636.24 ; \t loss: 0.3765\n",
      "Total trained pairs (M):    1636.71 ; \t loss: 0.3744\n",
      "Total trained pairs (M):    1637.19 ; \t loss: 0.3715\n",
      "Total trained pairs (M):    1637.67 ; \t loss: 0.3621\n",
      "Total trained pairs (M):    1638.15 ; \t loss: 0.3612\n",
      "Total trained pairs (M):    1638.63 ; \t loss: 0.3591\n",
      "Total trained pairs (M):    1639.11 ; \t loss: 0.3570\n",
      "Total trained pairs (M):    1639.60 ; \t loss: 0.3554\n",
      "Total trained pairs (M):    1640.08 ; \t loss: 0.3528\n",
      "Total trained pairs (M):    1640.56 ; \t loss: 0.3513\n",
      "Total trained pairs (M):    1641.04 ; \t loss: 0.3498\n",
      "Total trained pairs (M):    1641.52 ; \t loss: 0.3476\n",
      "Total trained pairs (M):    1642.01 ; \t loss: 0.3475\n",
      "Total trained pairs (M):    1642.50 ; \t loss: 0.3439\n",
      "Total trained pairs (M):    1642.99 ; \t loss: 0.3420\n",
      "Total trained pairs (M):    1643.47 ; \t loss: 0.3408\n",
      "Total trained pairs (M):    1643.96 ; \t loss: 0.3390\n",
      "Total trained pairs (M):    1644.46 ; \t loss: 0.3370\n",
      "Total trained pairs (M):    1644.98 ; \t loss: 0.3386\n",
      "Total trained pairs (M):    1645.51 ; \t loss: 0.3391\n",
      "Total trained pairs (M):    1646.03 ; \t loss: 0.3388\n",
      "Total trained pairs (M):    1646.55 ; \t loss: 0.3371\n",
      "Total trained pairs (M):    1647.06 ; \t loss: 0.3344\n",
      "Total trained pairs (M):    1647.58 ; \t loss: 0.3359\n",
      "Total trained pairs (M):    1648.09 ; \t loss: 0.3324\n",
      "Total trained pairs (M):    1648.60 ; \t loss: 0.3313\n",
      "Total trained pairs (M):    1649.09 ; \t loss: 0.3291\n",
      "Total trained pairs (M):    1649.58 ; \t loss: 0.3243\n",
      "Total trained pairs (M):    1650.06 ; \t loss: 0.3238\n",
      "Total trained pairs (M):    1650.55 ; \t loss: 0.3224\n",
      "Total trained pairs (M):    1651.04 ; \t loss: 0.3207\n",
      "Total trained pairs (M):    1651.53 ; \t loss: 0.3216\n",
      "Total trained pairs (M):    1652.01 ; \t loss: 0.3189\n",
      "Total trained pairs (M):    1652.51 ; \t loss: 0.3229\n",
      "Total trained pairs (M):    1653.01 ; \t loss: 0.3206\n",
      "Total trained pairs (M):    1653.50 ; \t loss: 0.3183\n",
      "Total trained pairs (M):    1653.99 ; \t loss: 0.3181\n",
      "Total trained pairs (M):    1654.49 ; \t loss: 0.3196\n",
      "Total trained pairs (M):    1654.99 ; \t loss: 0.3216\n",
      "Total trained pairs (M):    1655.49 ; \t loss: 0.3203\n",
      "Total trained pairs (M):    1655.98 ; \t loss: 0.3202\n",
      "Total trained pairs (M):    1656.49 ; \t loss: 0.3232\n",
      "Total trained pairs (M):    1657.00 ; \t loss: 0.3215\n",
      "Total trained pairs (M):    1657.51 ; \t loss: 0.3206\n",
      "Total trained pairs (M):    1658.03 ; \t loss: 0.3222\n",
      "Total trained pairs (M):    1658.55 ; \t loss: 0.3219\n",
      "Total trained pairs (M):    1659.07 ; \t loss: 0.3215\n",
      "Total trained pairs (M):    1659.58 ; \t loss: 0.3179\n",
      "Total trained pairs (M):    1660.11 ; \t loss: 0.3233\n",
      "Total trained pairs (M):    1660.65 ; \t loss: 0.3237\n",
      "Total trained pairs (M):    1661.19 ; \t loss: 0.3238\n",
      "Total trained pairs (M):    1661.73 ; \t loss: 0.3215\n",
      "Total trained pairs (M):    1662.27 ; \t loss: 0.3198\n",
      "Total trained pairs (M):    1662.82 ; \t loss: 0.3212\n",
      "Total trained pairs (M):    1663.38 ; \t loss: 0.3206\n",
      "Total trained pairs (M):    1663.94 ; \t loss: 0.3204\n",
      "Total trained pairs (M):    1664.48 ; \t loss: 0.3195\n",
      "Total trained pairs (M):    1665.05 ; \t loss: 0.3194\n",
      "Total trained pairs (M):    1665.60 ; \t loss: 0.3186\n",
      "Total trained pairs (M):    1666.16 ; \t loss: 0.3173\n",
      "Total trained pairs (M):    1666.71 ; \t loss: 0.3176\n",
      "Total trained pairs (M):    1667.23 ; \t loss: 0.3192\n",
      "Total trained pairs (M):    1667.75 ; \t loss: 0.3163\n",
      "Total trained pairs (M):    1668.27 ; \t loss: 0.3202\n",
      "Total trained pairs (M):    1668.78 ; \t loss: 0.3187\n",
      "Total trained pairs (M):    1669.29 ; \t loss: 0.3186\n",
      "Total trained pairs (M):    1669.80 ; \t loss: 0.3189\n",
      "Total trained pairs (M):    1670.32 ; \t loss: 0.3166\n",
      "Total trained pairs (M):    1670.83 ; \t loss: 0.3164\n",
      "Total trained pairs (M):    1671.33 ; \t loss: 0.3143\n",
      "Total trained pairs (M):    1671.84 ; \t loss: 0.3163\n",
      "Total trained pairs (M):    1672.35 ; \t loss: 0.3160\n",
      "Total trained pairs (M):    1672.86 ; \t loss: 0.3163\n",
      "Total trained pairs (M):    1673.37 ; \t loss: 0.3149\n",
      "Total trained pairs (M):    1673.88 ; \t loss: 0.3141\n",
      "Total trained pairs (M):    1674.39 ; \t loss: 0.3133\n",
      "Total trained pairs (M):    1674.91 ; \t loss: 0.3140\n",
      "Total trained pairs (M):    1675.42 ; \t loss: 0.3147\n",
      "Total trained pairs (M):    1675.94 ; \t loss: 0.3154\n",
      "Total trained pairs (M):    1676.47 ; \t loss: 0.3153\n",
      "Total trained pairs (M):    1676.99 ; \t loss: 0.3146\n",
      "Total trained pairs (M):    1677.52 ; \t loss: 0.3138\n",
      "Total trained pairs (M):    1678.05 ; \t loss: 0.3130\n",
      "Total trained pairs (M):    1678.57 ; \t loss: 0.3118\n",
      "Total trained pairs (M):    1679.10 ; \t loss: 0.3096\n",
      "Total trained pairs (M):    1679.63 ; \t loss: 0.3107\n",
      "Total trained pairs (M):    1680.17 ; \t loss: 0.3086\n",
      "Total trained pairs (M):    1680.69 ; \t loss: 0.3117\n",
      "Total trained pairs (M):    1681.21 ; \t loss: 0.3108\n",
      "Total trained pairs (M):    1681.74 ; \t loss: 0.3087\n",
      "Total trained pairs (M):    1682.25 ; \t loss: 0.3090\n",
      "Total trained pairs (M):    1682.78 ; \t loss: 0.3113\n",
      "Total trained pairs (M):    1683.30 ; \t loss: 0.3101\n",
      "Total trained pairs (M):    1683.82 ; \t loss: 0.3099\n",
      "Total trained pairs (M):    1684.34 ; \t loss: 0.3090\n",
      "Total trained pairs (M):    1684.86 ; \t loss: 0.3083\n",
      "Total trained pairs (M):    1685.37 ; \t loss: 0.3057\n",
      "Total trained pairs (M):    1685.88 ; \t loss: 0.3054\n",
      "Total trained pairs (M):    1686.40 ; \t loss: 0.3056\n",
      "Total trained pairs (M):    1686.92 ; \t loss: 0.3065\n",
      "Total trained pairs (M):    1687.45 ; \t loss: 0.3060\n",
      "Total trained pairs (M):    1687.98 ; \t loss: 0.3042\n",
      "Total trained pairs (M):    1688.51 ; \t loss: 0.3067\n",
      "Total trained pairs (M):    1689.02 ; \t loss: 0.3055\n",
      "Total trained pairs (M):    1689.56 ; \t loss: 0.3106\n",
      "Total trained pairs (M):    1690.09 ; \t loss: 0.3096\n",
      "Total trained pairs (M):    1690.63 ; \t loss: 0.3119\n",
      "Total trained pairs (M):    1691.16 ; \t loss: 0.3099\n",
      "Total trained pairs (M):    1691.70 ; \t loss: 0.3095\n",
      "Total trained pairs (M):    1692.23 ; \t loss: 0.3067\n",
      "Total trained pairs (M):    1692.77 ; \t loss: 0.3064\n",
      "Total trained pairs (M):    1693.31 ; \t loss: 0.3044\n",
      "Total trained pairs (M):    1693.83 ; \t loss: 0.3040\n",
      "Total trained pairs (M):    1694.36 ; \t loss: 0.3035\n",
      "Total trained pairs (M):    1694.89 ; \t loss: 0.3051\n",
      "Total trained pairs (M):    1695.42 ; \t loss: 0.3020\n",
      "Total trained pairs (M):    1695.95 ; \t loss: 0.3013\n",
      "Total trained pairs (M):    1696.47 ; \t loss: 0.3029\n",
      "Total trained pairs (M):    1696.97 ; \t loss: 0.3038\n",
      "Total trained pairs (M):    1697.46 ; \t loss: 0.3047\n",
      "Total trained pairs (M):    1697.95 ; \t loss: 0.3030\n",
      "Total trained pairs (M):    1698.44 ; \t loss: 0.3024\n",
      "Total trained pairs (M):    1698.93 ; \t loss: 0.3002\n",
      "Total trained pairs (M):    1699.41 ; \t loss: 0.2988\n",
      "Total trained pairs (M):    1699.91 ; \t loss: 0.3013\n",
      "Total trained pairs (M):    1700.42 ; \t loss: 0.3015\n",
      "Total trained pairs (M):    1700.93 ; \t loss: 0.3011\n",
      "Total trained pairs (M):    1701.44 ; \t loss: 0.3011\n",
      "Total trained pairs (M):    1701.96 ; \t loss: 0.3007\n",
      "Total trained pairs (M):    1702.48 ; \t loss: 0.3017\n",
      "Total trained pairs (M):    1703.00 ; \t loss: 0.2989\n",
      "Total trained pairs (M):    1703.51 ; \t loss: 0.2983\n",
      "Total trained pairs (M):    1704.03 ; \t loss: 0.2988\n",
      "Total trained pairs (M):    1704.53 ; \t loss: 0.3012\n",
      "Total trained pairs (M):    1705.04 ; \t loss: 0.2989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trained pairs (M):    1705.54 ; \t loss: 0.2981\n",
      "Total trained pairs (M):    1706.05 ; \t loss: 0.2972\n",
      "Total trained pairs (M):    1706.54 ; \t loss: 0.2972\n",
      "Total trained pairs (M):    1707.02 ; \t loss: 0.2967\n",
      "Total trained pairs (M):    1707.50 ; \t loss: 0.2949\n",
      "Total trained pairs (M):    1707.97 ; \t loss: 0.2952\n",
      "Total trained pairs (M):    1708.43 ; \t loss: 0.2974\n",
      "Total trained pairs (M):    1708.89 ; \t loss: 0.2975\n",
      "Total trained pairs (M):    1709.35 ; \t loss: 0.2954\n",
      "Total trained pairs (M):    1709.79 ; \t loss: 0.2957\n",
      "Total trained pairs (M):    1710.23 ; \t loss: 0.2973\n",
      "Total trained pairs (M):    1710.68 ; \t loss: 0.2959\n",
      "Total trained pairs (M):    1711.12 ; \t loss: 0.2951\n",
      "Total trained pairs (M):    1711.54 ; \t loss: 0.2952\n",
      "Total trained pairs (M):    1711.97 ; \t loss: 0.2980\n",
      "Total trained pairs (M):    1712.39 ; \t loss: 0.2960\n",
      "Total trained pairs (M):    1712.81 ; \t loss: 0.2968\n",
      "Total trained pairs (M):    1713.24 ; \t loss: 0.3027\n",
      "Total trained pairs (M):    1713.66 ; \t loss: 0.3026\n",
      "Total trained pairs (M):    1714.08 ; \t loss: 0.3069\n",
      "Total trained pairs (M):    1714.50 ; \t loss: 0.3082\n",
      "Total trained pairs (M):    1714.91 ; \t loss: 0.3049\n",
      "Total trained pairs (M):    1715.31 ; \t loss: 0.3073\n",
      "Total trained pairs (M):    1715.73 ; \t loss: 0.3095\n",
      "Total trained pairs (M):    1716.15 ; \t loss: 0.3126\n",
      "Total trained pairs (M):    1716.59 ; \t loss: 0.3169\n",
      "Total trained pairs (M):    1717.05 ; \t loss: 0.3238\n",
      "Total trained pairs (M):    1717.51 ; \t loss: 0.3223\n",
      "Total trained pairs (M):    1717.96 ; \t loss: 0.3232\n",
      "Total trained pairs (M):    1718.40 ; \t loss: 0.3214\n",
      "Total trained pairs (M):    1718.85 ; \t loss: 0.3195\n",
      "Total trained pairs (M):    1719.30 ; \t loss: 0.3200\n",
      "Total trained pairs (M):    1719.74 ; \t loss: 0.3178\n",
      "Total trained pairs (M):    1720.18 ; \t loss: 0.3174\n",
      "Total trained pairs (M):    1720.63 ; \t loss: 0.3188\n",
      "Total trained pairs (M):    1721.08 ; \t loss: 0.3165\n",
      "Total trained pairs (M):    1721.53 ; \t loss: 0.3173\n",
      "Total trained pairs (M):    1721.97 ; \t loss: 0.3177\n",
      "Total trained pairs (M):    1722.42 ; \t loss: 0.3176\n",
      "Total trained pairs (M):    1722.87 ; \t loss: 0.3170\n",
      "Total trained pairs (M):    1723.32 ; \t loss: 0.3180\n",
      "Total trained pairs (M):    1723.77 ; \t loss: 0.3140\n",
      "Total trained pairs (M):    1724.22 ; \t loss: 0.3152\n",
      "Total trained pairs (M):    1724.65 ; \t loss: 0.3158\n",
      "Total trained pairs (M):    1725.09 ; \t loss: 0.3158\n",
      "Total trained pairs (M):    1725.54 ; \t loss: 0.3191\n",
      "Total trained pairs (M):    1725.98 ; \t loss: 0.3189\n",
      "Total trained pairs (M):    1726.42 ; \t loss: 0.3158\n",
      "Total trained pairs (M):    1726.86 ; \t loss: 0.3138\n",
      "Total trained pairs (M):    1727.30 ; \t loss: 0.3133\n",
      "Total trained pairs (M):    1727.73 ; \t loss: 0.3147\n",
      "Total trained pairs (M):    1728.18 ; \t loss: 0.3172\n",
      "Total trained pairs (M):    1728.62 ; \t loss: 0.3162\n",
      "Total trained pairs (M):    1729.06 ; \t loss: 0.3164\n",
      "Total trained pairs (M):    1729.51 ; \t loss: 0.3146\n",
      "Total trained pairs (M):    1729.96 ; \t loss: 0.3126\n",
      "Total trained pairs (M):    1730.40 ; \t loss: 0.3158\n",
      "Total trained pairs (M):    1730.84 ; \t loss: 0.3169\n",
      "Total trained pairs (M):    1731.29 ; \t loss: 0.3168\n",
      "Total trained pairs (M):    1731.74 ; \t loss: 0.3172\n",
      "Total trained pairs (M):    1732.19 ; \t loss: 0.3198\n",
      "Total trained pairs (M):    1732.65 ; \t loss: 0.3195\n",
      "Total trained pairs (M):    1733.11 ; \t loss: 0.3179\n",
      "Total trained pairs (M):    1733.58 ; \t loss: 0.3217\n",
      "Total trained pairs (M):    1734.04 ; \t loss: 0.3224\n",
      "Total trained pairs (M):    1734.49 ; \t loss: 0.3201\n",
      "Total trained pairs (M):    1734.95 ; \t loss: 0.3177\n",
      "Total trained pairs (M):    1735.41 ; \t loss: 0.3190\n",
      "Total trained pairs (M):    1735.86 ; \t loss: 0.3188\n",
      "Total trained pairs (M):    1736.29 ; \t loss: 0.3205\n",
      "Total trained pairs (M):    1736.73 ; \t loss: 0.3229\n",
      "Total trained pairs (M):    1737.17 ; \t loss: 0.3206\n",
      "Total trained pairs (M):    1737.60 ; \t loss: 0.3212\n",
      "Total trained pairs (M):    1738.04 ; \t loss: 0.3217\n",
      "Total trained pairs (M):    1738.47 ; \t loss: 0.3207\n",
      "Total trained pairs (M):    1738.91 ; \t loss: 0.3196\n",
      "Total trained pairs (M):    1739.39 ; \t loss: 0.3326\n",
      "Total trained pairs (M):    1739.86 ; \t loss: 0.3324\n",
      "Total trained pairs (M):    1740.33 ; \t loss: 0.3291\n",
      "Total trained pairs (M):    1740.81 ; \t loss: 0.3299\n",
      "Total trained pairs (M):    1741.29 ; \t loss: 0.3315\n",
      "Total trained pairs (M):    1741.77 ; \t loss: 0.3298\n",
      "Total trained pairs (M):    1742.24 ; \t loss: 0.3275\n",
      "Total trained pairs (M):    1742.72 ; \t loss: 0.3286\n",
      "Total trained pairs (M):    1743.19 ; \t loss: 0.3265\n",
      "Total trained pairs (M):    1743.66 ; \t loss: 0.3270\n",
      "Total trained pairs (M):    1744.13 ; \t loss: 0.3245\n",
      "Total trained pairs (M):    1744.61 ; \t loss: 0.3242\n",
      "Total trained pairs (M):    1745.09 ; \t loss: 0.3234\n",
      "Total trained pairs (M):    1745.56 ; \t loss: 0.3261\n",
      "Total trained pairs (M):    1746.02 ; \t loss: 0.3254\n",
      "Total trained pairs (M):    1746.48 ; \t loss: 0.3238\n",
      "Total trained pairs (M):    1746.94 ; \t loss: 0.3240\n",
      "Total trained pairs (M):    1747.41 ; \t loss: 0.3260\n",
      "Total trained pairs (M):    1747.87 ; \t loss: 0.3251\n",
      "Total trained pairs (M):    1748.34 ; \t loss: 0.3251\n",
      "Total trained pairs (M):    1748.81 ; \t loss: 0.3232\n",
      "Total trained pairs (M):    1749.29 ; \t loss: 0.3236\n",
      "Total trained pairs (M):    1749.77 ; \t loss: 0.3233\n",
      "Total trained pairs (M):    1750.26 ; \t loss: 0.3222\n",
      "Total trained pairs (M):    1750.76 ; \t loss: 0.3226\n",
      "Total trained pairs (M):    1751.26 ; \t loss: 0.3221\n",
      "Total trained pairs (M):    1751.77 ; \t loss: 0.3220\n",
      "Total trained pairs (M):    1752.28 ; \t loss: 0.3234\n",
      "Total trained pairs (M):    1752.79 ; \t loss: 0.3224\n",
      "Total trained pairs (M):    1753.30 ; \t loss: 0.3218\n",
      "Total trained pairs (M):    1753.81 ; \t loss: 0.3192\n",
      "Total trained pairs (M):    1754.32 ; \t loss: 0.3185\n",
      "Total trained pairs (M):    1754.83 ; \t loss: 0.3176\n",
      "Total trained pairs (M):    1755.34 ; \t loss: 0.3169\n",
      "Total trained pairs (M):    1755.86 ; \t loss: 0.3169\n",
      "Total trained pairs (M):    1756.38 ; \t loss: 0.3153\n",
      "Total trained pairs (M):    1756.89 ; \t loss: 0.3132\n",
      "Total trained pairs (M):    1757.41 ; \t loss: 0.3114\n",
      "Total trained pairs (M):    1757.92 ; \t loss: 0.3134\n",
      "Total trained pairs (M):    1758.42 ; \t loss: 0.3105\n",
      "Total trained pairs (M):    1758.94 ; \t loss: 0.3136\n",
      "Total trained pairs (M):    1759.44 ; \t loss: 0.3135\n",
      "Total trained pairs (M):    1759.94 ; \t loss: 0.3128\n",
      "Total trained pairs (M):    1760.44 ; \t loss: 0.3116\n",
      "Total trained pairs (M):    1760.93 ; \t loss: 0.3106\n",
      "Total trained pairs (M):    1761.43 ; \t loss: 0.3119\n",
      "Total trained pairs (M):    1761.92 ; \t loss: 0.3098\n",
      "Total trained pairs (M):    1762.40 ; \t loss: 0.3129\n",
      "Total trained pairs (M):    1762.88 ; \t loss: 0.3177\n",
      "Total trained pairs (M):    1763.35 ; \t loss: 0.3174\n",
      "Total trained pairs (M):    1763.80 ; \t loss: 0.3161\n",
      "Total trained pairs (M):    1764.25 ; \t loss: 0.3164\n",
      "Total trained pairs (M):    1764.70 ; \t loss: 0.3165\n",
      "Total trained pairs (M):    1765.15 ; \t loss: 0.3136\n",
      "Total trained pairs (M):    1765.60 ; \t loss: 0.3140\n",
      "Total trained pairs (M):    1766.05 ; \t loss: 0.3134\n",
      "Total trained pairs (M):    1766.51 ; \t loss: 0.3116\n",
      "Total trained pairs (M):    1766.97 ; \t loss: 0.3124\n",
      "Total trained pairs (M):    1767.43 ; \t loss: 0.3125\n",
      "Total trained pairs (M):    1767.89 ; \t loss: 0.3118\n",
      "Total trained pairs (M):    1768.36 ; \t loss: 0.3140\n",
      "Total trained pairs (M):    1768.82 ; \t loss: 0.3131\n",
      "Total trained pairs (M):    1769.28 ; \t loss: 0.3111\n",
      "Total trained pairs (M):    1769.75 ; \t loss: 0.3114\n",
      "Total trained pairs (M):    1770.21 ; \t loss: 0.3128\n",
      "Total trained pairs (M):    1770.68 ; \t loss: 0.3152\n",
      "Total trained pairs (M):    1771.16 ; \t loss: 0.3152\n",
      "Total trained pairs (M):    1771.63 ; \t loss: 0.3152\n",
      "Total trained pairs (M):    1772.11 ; \t loss: 0.3133\n",
      "Total trained pairs (M):    1772.58 ; \t loss: 0.3127\n",
      "Total trained pairs (M):    1773.05 ; \t loss: 0.3139\n",
      "Total trained pairs (M):    1773.53 ; \t loss: 0.3126\n",
      "Total trained pairs (M):    1774.00 ; \t loss: 0.3116\n",
      "Total trained pairs (M):    1774.48 ; \t loss: 0.3108\n",
      "Total trained pairs (M):    1774.96 ; \t loss: 0.3094\n",
      "Total trained pairs (M):    1775.43 ; \t loss: 0.3123\n",
      "Total trained pairs (M):    1775.91 ; \t loss: 0.3124\n",
      "Total trained pairs (M):    1776.38 ; \t loss: 0.3111\n",
      "Total trained pairs (M):    1776.85 ; \t loss: 0.3109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trained pairs (M):    1777.33 ; \t loss: 0.3091\n",
      "Total trained pairs (M):    1777.80 ; \t loss: 0.3105\n",
      "Total trained pairs (M):    1778.28 ; \t loss: 0.3096\n",
      "Total trained pairs (M):    1778.76 ; \t loss: 0.3106\n",
      "Total trained pairs (M):    1779.23 ; \t loss: 0.3100\n",
      "Total trained pairs (M):    1779.70 ; \t loss: 0.3103\n",
      "Total trained pairs (M):    1780.16 ; \t loss: 0.3088\n",
      "Total trained pairs (M):    1780.63 ; \t loss: 0.3161\n",
      "Total trained pairs (M):    1781.10 ; \t loss: 0.3183\n",
      "Total trained pairs (M):    1781.57 ; \t loss: 0.3170\n",
      "Total trained pairs (M):    1782.04 ; \t loss: 0.3149\n",
      "Total trained pairs (M):    1782.51 ; \t loss: 0.3163\n",
      "Total trained pairs (M):    1782.98 ; \t loss: 0.3167\n",
      "Total trained pairs (M):    1783.46 ; \t loss: 0.3175\n",
      "Total trained pairs (M):    1783.93 ; \t loss: 0.3169\n",
      "Total trained pairs (M):    1784.40 ; \t loss: 0.3172\n",
      "Total trained pairs (M):    1784.86 ; \t loss: 0.3184\n",
      "Total trained pairs (M):    1785.33 ; \t loss: 0.3184\n",
      "Total trained pairs (M):    1785.81 ; \t loss: 0.3178\n",
      "Total trained pairs (M):    1786.29 ; \t loss: 0.3208\n",
      "Total trained pairs (M):    1786.77 ; \t loss: 0.3189\n",
      "Total trained pairs (M):    1787.22 ; \t loss: 0.3221\n",
      "Total trained pairs (M):    1787.67 ; \t loss: 0.3232\n",
      "Total trained pairs (M):    1788.13 ; \t loss: 0.3295\n",
      "Total trained pairs (M):    1788.59 ; \t loss: 0.3278\n",
      "Total trained pairs (M):    1789.04 ; \t loss: 0.3293\n",
      "Total trained pairs (M):    1789.49 ; \t loss: 0.3270\n",
      "Total trained pairs (M):    1789.95 ; \t loss: 0.3263\n",
      "Total trained pairs (M):    1790.40 ; \t loss: 0.3238\n",
      "Total trained pairs (M):    1790.86 ; \t loss: 0.3231\n",
      "Total trained pairs (M):    1791.32 ; \t loss: 0.3221\n",
      "Total trained pairs (M):    1791.77 ; \t loss: 0.3235\n",
      "Total trained pairs (M):    1792.23 ; \t loss: 0.3224\n",
      "Total trained pairs (M):    1792.69 ; \t loss: 0.3240\n",
      "Total trained pairs (M):    1793.15 ; \t loss: 0.3240\n",
      "Total trained pairs (M):    1793.60 ; \t loss: 0.3247\n",
      "Total trained pairs (M):    1794.06 ; \t loss: 0.3227\n",
      "Total trained pairs (M):    1794.51 ; \t loss: 0.3256\n",
      "Total trained pairs (M):    1794.96 ; \t loss: 0.3269\n",
      "Total trained pairs (M):    1795.41 ; \t loss: 0.3267\n",
      "Total trained pairs (M):    1795.85 ; \t loss: 0.3291\n",
      "Total trained pairs (M):    1796.29 ; \t loss: 0.3307\n",
      "Total trained pairs (M):    1796.73 ; \t loss: 0.3301\n",
      "Total trained pairs (M):    1797.16 ; \t loss: 0.3306\n",
      "Total trained pairs (M):    1797.58 ; \t loss: 0.3325\n",
      "Total trained pairs (M):    1798.00 ; \t loss: 0.3341\n",
      "Total trained pairs (M):    1798.41 ; \t loss: 0.3348\n",
      "Total trained pairs (M):    1798.82 ; \t loss: 0.3341\n",
      "Total trained pairs (M):    1799.22 ; \t loss: 0.3332\n",
      "Total trained pairs (M):    1799.63 ; \t loss: 0.3383\n",
      "Total trained pairs (M):    1800.03 ; \t loss: 0.3368\n",
      "Total trained pairs (M):    1800.44 ; \t loss: 0.3358\n",
      "Total trained pairs (M):    1800.84 ; \t loss: 0.3354\n",
      "Total trained pairs (M):    1801.25 ; \t loss: 0.3338\n",
      "Total trained pairs (M):    1801.65 ; \t loss: 0.3316\n",
      "Total trained pairs (M):    1802.06 ; \t loss: 0.3369\n",
      "Total trained pairs (M):    1802.46 ; \t loss: 0.3377\n",
      "Total trained pairs (M):    1802.87 ; \t loss: 0.3378\n",
      "Total trained pairs (M):    1803.28 ; \t loss: 0.3360\n",
      "Total trained pairs (M):    1803.69 ; \t loss: 0.3377\n",
      "Total trained pairs (M):    1804.10 ; \t loss: 0.3361\n",
      "Total trained pairs (M):    1804.52 ; \t loss: 0.3430\n",
      "Total trained pairs (M):    1804.93 ; \t loss: 0.3419\n",
      "Total trained pairs (M):    1805.34 ; \t loss: 0.3496\n",
      "Total trained pairs (M):    1805.76 ; \t loss: 0.3502\n",
      "Total trained pairs (M):    1806.18 ; \t loss: 0.3506\n",
      "Total trained pairs (M):    1806.60 ; \t loss: 0.3501\n",
      "Total trained pairs (M):    1807.02 ; \t loss: 0.3539\n",
      "Total trained pairs (M):    1807.45 ; \t loss: 0.3563\n",
      "Total trained pairs (M):    1807.87 ; \t loss: 0.3604\n",
      "Total trained pairs (M):    1808.29 ; \t loss: 0.3594\n",
      "Total trained pairs (M):    1808.72 ; \t loss: 0.3575\n",
      "Total trained pairs (M):    1809.15 ; \t loss: 0.3607\n",
      "Total trained pairs (M):    1809.58 ; \t loss: 0.3611\n",
      "Total trained pairs (M):    1810.02 ; \t loss: 0.3641\n",
      "Total trained pairs (M):    1810.47 ; \t loss: 0.3714\n",
      "Total trained pairs (M):    1810.92 ; \t loss: 0.3691\n",
      "Total trained pairs (M):    1811.37 ; \t loss: 0.3673\n",
      "Total trained pairs (M):    1811.81 ; \t loss: 0.3694\n",
      "Total trained pairs (M):    1812.25 ; \t loss: 0.3701\n",
      "Total trained pairs (M):    1812.70 ; \t loss: 0.3820\n",
      "Total trained pairs (M):    1813.15 ; \t loss: 0.3873\n",
      "Total trained pairs (M):    1813.60 ; \t loss: 0.3881\n",
      "Total trained pairs (M):    1814.05 ; \t loss: 0.3871\n",
      "Total trained pairs (M):    1814.48 ; \t loss: 0.3867\n",
      "Total trained pairs (M):    1814.92 ; \t loss: 0.3863\n",
      "Total trained pairs (M):    1815.36 ; \t loss: 0.3913\n",
      "Total trained pairs (M):    1815.79 ; \t loss: 0.3922\n",
      "Total trained pairs (M):    1816.22 ; \t loss: 0.3904\n",
      "Total trained pairs (M):    1816.64 ; \t loss: 0.3921\n",
      "Total trained pairs (M):    1817.06 ; \t loss: 0.3958\n",
      "Total trained pairs (M):    1817.50 ; \t loss: 0.3979\n",
      "Total trained pairs (M):    1817.94 ; \t loss: 0.4078\n",
      "Total trained pairs (M):    1818.38 ; \t loss: 0.4089\n",
      "Total trained pairs (M):    1818.83 ; \t loss: 0.4115\n",
      "Total trained pairs (M):    1819.28 ; \t loss: 0.4110\n",
      "Epoch 9 ======\n",
      "Total trained pairs (M):    1819.79 ; \t loss: 0.5513\n",
      "Total trained pairs (M):    1820.32 ; \t loss: 0.5421\n",
      "Total trained pairs (M):    1820.84 ; \t loss: 0.5265\n",
      "Total trained pairs (M):    1821.36 ; \t loss: 0.5170\n",
      "Total trained pairs (M):    1821.87 ; \t loss: 0.5075\n",
      "Total trained pairs (M):    1822.38 ; \t loss: 0.4990\n",
      "Total trained pairs (M):    1822.90 ; \t loss: 0.4937\n",
      "Total trained pairs (M):    1823.40 ; \t loss: 0.4886\n",
      "Total trained pairs (M):    1823.90 ; \t loss: 0.4811\n",
      "Total trained pairs (M):    1824.40 ; \t loss: 0.4727\n",
      "Total trained pairs (M):    1824.90 ; \t loss: 0.4686\n",
      "Total trained pairs (M):    1825.40 ; \t loss: 0.4620\n",
      "Total trained pairs (M):    1825.90 ; \t loss: 0.4593\n",
      "Total trained pairs (M):    1826.40 ; \t loss: 0.4559\n",
      "Total trained pairs (M):    1826.89 ; \t loss: 0.4494\n",
      "Total trained pairs (M):    1827.38 ; \t loss: 0.4446\n",
      "Total trained pairs (M):    1827.86 ; \t loss: 0.4414\n",
      "Total trained pairs (M):    1828.34 ; \t loss: 0.4384\n",
      "Total trained pairs (M):    1828.82 ; \t loss: 0.4346\n",
      "Total trained pairs (M):    1829.30 ; \t loss: 0.4314\n",
      "Total trained pairs (M):    1829.78 ; \t loss: 0.4262\n",
      "Total trained pairs (M):    1830.26 ; \t loss: 0.4233\n",
      "Total trained pairs (M):    1830.73 ; \t loss: 0.4187\n",
      "Total trained pairs (M):    1831.21 ; \t loss: 0.4184\n",
      "Total trained pairs (M):    1831.69 ; \t loss: 0.4153\n",
      "Total trained pairs (M):    1832.17 ; \t loss: 0.4097\n",
      "Total trained pairs (M):    1832.66 ; \t loss: 0.4058\n",
      "Total trained pairs (M):    1833.12 ; \t loss: 0.3942\n",
      "Total trained pairs (M):    1833.59 ; \t loss: 0.3923\n",
      "Total trained pairs (M):    1834.06 ; \t loss: 0.3891\n",
      "Total trained pairs (M):    1834.54 ; \t loss: 0.3876\n",
      "Total trained pairs (M):    1835.01 ; \t loss: 0.3861\n",
      "Total trained pairs (M):    1835.49 ; \t loss: 0.3832\n",
      "Total trained pairs (M):    1835.98 ; \t loss: 0.3800\n",
      "Total trained pairs (M):    1836.47 ; \t loss: 0.3793\n",
      "Total trained pairs (M):    1836.95 ; \t loss: 0.3765\n",
      "Total trained pairs (M):    1837.43 ; \t loss: 0.3747\n",
      "Total trained pairs (M):    1837.91 ; \t loss: 0.3724\n",
      "Total trained pairs (M):    1838.38 ; \t loss: 0.3689\n",
      "Total trained pairs (M):    1838.85 ; \t loss: 0.3680\n",
      "Total trained pairs (M):    1839.33 ; \t loss: 0.3652\n",
      "Total trained pairs (M):    1839.81 ; \t loss: 0.3573\n",
      "Total trained pairs (M):    1840.29 ; \t loss: 0.3542\n",
      "Total trained pairs (M):    1840.77 ; \t loss: 0.3521\n",
      "Total trained pairs (M):    1841.26 ; \t loss: 0.3501\n",
      "Total trained pairs (M):    1841.74 ; \t loss: 0.3487\n",
      "Total trained pairs (M):    1842.22 ; \t loss: 0.3462\n",
      "Total trained pairs (M):    1842.70 ; \t loss: 0.3459\n",
      "Total trained pairs (M):    1843.18 ; \t loss: 0.3448\n",
      "Total trained pairs (M):    1843.66 ; \t loss: 0.3412\n",
      "Total trained pairs (M):    1844.15 ; \t loss: 0.3397\n",
      "Total trained pairs (M):    1844.64 ; \t loss: 0.3379\n",
      "Total trained pairs (M):    1845.13 ; \t loss: 0.3367\n",
      "Total trained pairs (M):    1845.62 ; \t loss: 0.3345\n",
      "Total trained pairs (M):    1846.10 ; \t loss: 0.3318\n",
      "Total trained pairs (M):    1846.60 ; \t loss: 0.3304\n",
      "Total trained pairs (M):    1847.12 ; \t loss: 0.3323\n",
      "Total trained pairs (M):    1847.65 ; \t loss: 0.3330\n",
      "Total trained pairs (M):    1848.17 ; \t loss: 0.3319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trained pairs (M):    1848.69 ; \t loss: 0.3301\n",
      "Total trained pairs (M):    1849.21 ; \t loss: 0.3277\n",
      "Total trained pairs (M):    1849.72 ; \t loss: 0.3276\n",
      "Total trained pairs (M):    1850.23 ; \t loss: 0.3255\n",
      "Total trained pairs (M):    1850.74 ; \t loss: 0.3233\n",
      "Total trained pairs (M):    1851.23 ; \t loss: 0.3207\n",
      "Total trained pairs (M):    1851.72 ; \t loss: 0.3190\n",
      "Total trained pairs (M):    1852.20 ; \t loss: 0.3176\n",
      "Total trained pairs (M):    1852.69 ; \t loss: 0.3159\n",
      "Total trained pairs (M):    1853.18 ; \t loss: 0.3156\n",
      "Total trained pairs (M):    1853.67 ; \t loss: 0.3145\n",
      "Total trained pairs (M):    1854.15 ; \t loss: 0.3129\n",
      "Total trained pairs (M):    1854.65 ; \t loss: 0.3163\n",
      "Total trained pairs (M):    1855.15 ; \t loss: 0.3141\n",
      "Total trained pairs (M):    1855.64 ; \t loss: 0.3131\n",
      "Total trained pairs (M):    1856.13 ; \t loss: 0.3110\n",
      "Total trained pairs (M):    1856.63 ; \t loss: 0.3118\n",
      "Total trained pairs (M):    1857.14 ; \t loss: 0.3145\n",
      "Total trained pairs (M):    1857.63 ; \t loss: 0.3143\n",
      "Total trained pairs (M):    1858.12 ; \t loss: 0.3139\n",
      "Total trained pairs (M):    1858.63 ; \t loss: 0.3161\n",
      "Total trained pairs (M):    1859.14 ; \t loss: 0.3147\n",
      "Total trained pairs (M):    1859.65 ; \t loss: 0.3154\n",
      "Total trained pairs (M):    1860.18 ; \t loss: 0.3148\n",
      "Total trained pairs (M):    1860.69 ; \t loss: 0.3143\n",
      "Total trained pairs (M):    1861.21 ; \t loss: 0.3137\n",
      "Total trained pairs (M):    1861.72 ; \t loss: 0.3117\n",
      "Total trained pairs (M):    1862.25 ; \t loss: 0.3164\n",
      "Total trained pairs (M):    1862.79 ; \t loss: 0.3176\n",
      "Total trained pairs (M):    1863.33 ; \t loss: 0.3154\n",
      "Total trained pairs (M):    1863.87 ; \t loss: 0.3166\n",
      "Total trained pairs (M):    1864.41 ; \t loss: 0.3149\n",
      "Total trained pairs (M):    1864.96 ; \t loss: 0.3152\n",
      "Total trained pairs (M):    1865.52 ; \t loss: 0.3148\n",
      "Total trained pairs (M):    1866.08 ; \t loss: 0.3142\n",
      "Total trained pairs (M):    1866.63 ; \t loss: 0.3127\n",
      "Total trained pairs (M):    1867.19 ; \t loss: 0.3133\n",
      "Total trained pairs (M):    1867.75 ; \t loss: 0.3130\n",
      "Total trained pairs (M):    1868.30 ; \t loss: 0.3118\n",
      "Total trained pairs (M):    1868.86 ; \t loss: 0.3112\n",
      "Total trained pairs (M):    1869.37 ; \t loss: 0.3133\n",
      "Total trained pairs (M):    1869.89 ; \t loss: 0.3100\n",
      "Total trained pairs (M):    1870.41 ; \t loss: 0.3142\n",
      "Total trained pairs (M):    1870.92 ; \t loss: 0.3131\n",
      "Total trained pairs (M):    1871.43 ; \t loss: 0.3111\n",
      "Total trained pairs (M):    1871.95 ; \t loss: 0.3113\n",
      "Total trained pairs (M):    1872.46 ; \t loss: 0.3100\n",
      "Total trained pairs (M):    1872.97 ; \t loss: 0.3095\n",
      "Total trained pairs (M):    1873.48 ; \t loss: 0.3088\n",
      "Total trained pairs (M):    1873.99 ; \t loss: 0.3089\n",
      "Total trained pairs (M):    1874.49 ; \t loss: 0.3110\n",
      "Total trained pairs (M):    1875.00 ; \t loss: 0.3105\n",
      "Total trained pairs (M):    1875.51 ; \t loss: 0.3090\n",
      "Total trained pairs (M):    1876.02 ; \t loss: 0.3052\n",
      "Total trained pairs (M):    1876.53 ; \t loss: 0.3085\n",
      "Total trained pairs (M):    1877.05 ; \t loss: 0.3079\n",
      "Total trained pairs (M):    1877.56 ; \t loss: 0.3083\n",
      "Total trained pairs (M):    1878.08 ; \t loss: 0.3087\n",
      "Total trained pairs (M):    1878.61 ; \t loss: 0.3065\n",
      "Total trained pairs (M):    1879.13 ; \t loss: 0.3071\n",
      "Total trained pairs (M):    1879.66 ; \t loss: 0.3078\n",
      "Total trained pairs (M):    1880.19 ; \t loss: 0.3067\n",
      "Total trained pairs (M):    1880.72 ; \t loss: 0.3062\n",
      "Total trained pairs (M):    1881.24 ; \t loss: 0.3022\n",
      "Total trained pairs (M):    1881.77 ; \t loss: 0.3027\n",
      "Total trained pairs (M):    1882.31 ; \t loss: 0.3029\n",
      "Total trained pairs (M):    1882.83 ; \t loss: 0.3049\n",
      "Total trained pairs (M):    1883.35 ; \t loss: 0.3043\n",
      "Total trained pairs (M):    1883.88 ; \t loss: 0.3033\n",
      "Total trained pairs (M):    1884.40 ; \t loss: 0.3034\n",
      "Total trained pairs (M):    1884.92 ; \t loss: 0.3045\n",
      "Total trained pairs (M):    1885.45 ; \t loss: 0.3034\n",
      "Total trained pairs (M):    1885.97 ; \t loss: 0.3020\n",
      "Total trained pairs (M):    1886.48 ; \t loss: 0.3028\n",
      "Total trained pairs (M):    1887.00 ; \t loss: 0.3011\n",
      "Total trained pairs (M):    1887.51 ; \t loss: 0.3015\n",
      "Total trained pairs (M):    1888.02 ; \t loss: 0.2991\n",
      "Total trained pairs (M):    1888.54 ; \t loss: 0.2986\n",
      "Total trained pairs (M):    1889.06 ; \t loss: 0.3011\n",
      "Total trained pairs (M):    1889.59 ; \t loss: 0.2996\n",
      "Total trained pairs (M):    1890.12 ; \t loss: 0.2990\n",
      "Total trained pairs (M):    1890.65 ; \t loss: 0.2996\n",
      "Total trained pairs (M):    1891.17 ; \t loss: 0.2997\n",
      "Total trained pairs (M):    1891.70 ; \t loss: 0.3037\n",
      "Total trained pairs (M):    1892.23 ; \t loss: 0.3049\n",
      "Total trained pairs (M):    1892.77 ; \t loss: 0.3046\n",
      "Total trained pairs (M):    1893.30 ; \t loss: 0.3033\n",
      "Total trained pairs (M):    1893.84 ; \t loss: 0.3024\n",
      "Total trained pairs (M):    1894.38 ; \t loss: 0.3008\n",
      "Total trained pairs (M):    1894.91 ; \t loss: 0.2996\n",
      "Total trained pairs (M):    1895.45 ; \t loss: 0.2993\n",
      "Total trained pairs (M):    1895.97 ; \t loss: 0.2992\n",
      "Total trained pairs (M):    1896.50 ; \t loss: 0.2983\n",
      "Total trained pairs (M):    1897.03 ; \t loss: 0.2981\n",
      "Total trained pairs (M):    1897.56 ; \t loss: 0.2976\n",
      "Total trained pairs (M):    1898.09 ; \t loss: 0.2964\n",
      "Total trained pairs (M):    1898.61 ; \t loss: 0.2986\n",
      "Total trained pairs (M):    1899.11 ; \t loss: 0.2992\n",
      "Total trained pairs (M):    1899.60 ; \t loss: 0.2980\n",
      "Total trained pairs (M):    1900.09 ; \t loss: 0.2968\n",
      "Total trained pairs (M):    1900.58 ; \t loss: 0.2971\n",
      "Total trained pairs (M):    1901.07 ; \t loss: 0.2948\n",
      "Total trained pairs (M):    1901.55 ; \t loss: 0.2935\n",
      "Total trained pairs (M):    1902.05 ; \t loss: 0.2953\n",
      "Total trained pairs (M):    1902.56 ; \t loss: 0.2975\n",
      "Total trained pairs (M):    1903.07 ; \t loss: 0.2958\n",
      "Total trained pairs (M):    1903.59 ; \t loss: 0.2949\n",
      "Total trained pairs (M):    1904.11 ; \t loss: 0.2958\n",
      "Total trained pairs (M):    1904.62 ; \t loss: 0.2956\n",
      "Total trained pairs (M):    1905.14 ; \t loss: 0.2948\n",
      "Total trained pairs (M):    1905.66 ; \t loss: 0.2933\n",
      "Total trained pairs (M):    1906.17 ; \t loss: 0.2930\n",
      "Total trained pairs (M):    1906.68 ; \t loss: 0.2944\n",
      "Total trained pairs (M):    1907.18 ; \t loss: 0.2933\n",
      "Total trained pairs (M):    1907.69 ; \t loss: 0.2939\n",
      "Total trained pairs (M):    1908.19 ; \t loss: 0.2918\n",
      "Total trained pairs (M):    1908.68 ; \t loss: 0.2925\n",
      "Total trained pairs (M):    1909.16 ; \t loss: 0.2913\n",
      "Total trained pairs (M):    1909.64 ; \t loss: 0.2920\n",
      "Total trained pairs (M):    1910.11 ; \t loss: 0.2910\n",
      "Total trained pairs (M):    1910.58 ; \t loss: 0.2921\n",
      "Total trained pairs (M):    1911.04 ; \t loss: 0.2920\n",
      "Total trained pairs (M):    1911.49 ; \t loss: 0.2918\n",
      "Total trained pairs (M):    1911.93 ; \t loss: 0.2922\n",
      "Total trained pairs (M):    1912.38 ; \t loss: 0.2915\n",
      "Total trained pairs (M):    1912.82 ; \t loss: 0.2895\n",
      "Total trained pairs (M):    1913.26 ; \t loss: 0.2885\n",
      "Total trained pairs (M):    1913.68 ; \t loss: 0.2908\n",
      "Total trained pairs (M):    1914.11 ; \t loss: 0.2941\n",
      "Total trained pairs (M):    1914.53 ; \t loss: 0.2906\n",
      "Total trained pairs (M):    1914.95 ; \t loss: 0.2921\n",
      "Total trained pairs (M):    1915.38 ; \t loss: 0.2986\n",
      "Total trained pairs (M):    1915.80 ; \t loss: 0.2991\n",
      "Total trained pairs (M):    1916.22 ; \t loss: 0.3015\n",
      "Total trained pairs (M):    1916.64 ; \t loss: 0.3023\n",
      "Total trained pairs (M):    1917.06 ; \t loss: 0.3003\n",
      "Total trained pairs (M):    1917.45 ; \t loss: 0.3038\n",
      "Total trained pairs (M):    1917.87 ; \t loss: 0.3043\n",
      "Total trained pairs (M):    1918.30 ; \t loss: 0.3073\n",
      "Total trained pairs (M):    1918.74 ; \t loss: 0.3138\n",
      "Total trained pairs (M):    1919.19 ; \t loss: 0.3187\n",
      "Total trained pairs (M):    1919.65 ; \t loss: 0.3179\n",
      "Total trained pairs (M):    1920.10 ; \t loss: 0.3176\n",
      "Total trained pairs (M):    1920.55 ; \t loss: 0.3169\n",
      "Total trained pairs (M):    1920.99 ; \t loss: 0.3158\n",
      "Total trained pairs (M):    1921.44 ; \t loss: 0.3165\n",
      "Total trained pairs (M):    1921.88 ; \t loss: 0.3124\n",
      "Total trained pairs (M):    1922.32 ; \t loss: 0.3128\n",
      "Total trained pairs (M):    1922.77 ; \t loss: 0.3123\n",
      "Total trained pairs (M):    1923.22 ; \t loss: 0.3125\n",
      "Total trained pairs (M):    1923.67 ; \t loss: 0.3127\n",
      "Total trained pairs (M):    1924.11 ; \t loss: 0.3133\n",
      "Total trained pairs (M):    1924.57 ; \t loss: 0.3134\n",
      "Total trained pairs (M):    1925.02 ; \t loss: 0.3104\n",
      "Total trained pairs (M):    1925.46 ; \t loss: 0.3131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trained pairs (M):    1925.91 ; \t loss: 0.3136\n",
      "Total trained pairs (M):    1926.36 ; \t loss: 0.3102\n",
      "Total trained pairs (M):    1926.80 ; \t loss: 0.3138\n",
      "Total trained pairs (M):    1927.23 ; \t loss: 0.3119\n",
      "Total trained pairs (M):    1927.68 ; \t loss: 0.3138\n",
      "Total trained pairs (M):    1928.12 ; \t loss: 0.3134\n",
      "Total trained pairs (M):    1928.56 ; \t loss: 0.3129\n",
      "Total trained pairs (M):    1929.00 ; \t loss: 0.3104\n",
      "Total trained pairs (M):    1929.44 ; \t loss: 0.3107\n",
      "Total trained pairs (M):    1929.88 ; \t loss: 0.3111\n",
      "Total trained pairs (M):    1930.32 ; \t loss: 0.3134\n",
      "Total trained pairs (M):    1930.76 ; \t loss: 0.3119\n",
      "Total trained pairs (M):    1931.21 ; \t loss: 0.3123\n",
      "Total trained pairs (M):    1931.65 ; \t loss: 0.3102\n",
      "Total trained pairs (M):    1932.10 ; \t loss: 0.3101\n",
      "Total trained pairs (M):    1932.54 ; \t loss: 0.3114\n",
      "Total trained pairs (M):    1932.98 ; \t loss: 0.3117\n",
      "Total trained pairs (M):    1933.43 ; \t loss: 0.3137\n",
      "Total trained pairs (M):    1933.88 ; \t loss: 0.3125\n",
      "Total trained pairs (M):    1934.34 ; \t loss: 0.3154\n",
      "Total trained pairs (M):    1934.79 ; \t loss: 0.3146\n",
      "Total trained pairs (M):    1935.26 ; \t loss: 0.3148\n",
      "Total trained pairs (M):    1935.72 ; \t loss: 0.3167\n",
      "Total trained pairs (M):    1936.18 ; \t loss: 0.3174\n",
      "Total trained pairs (M):    1936.64 ; \t loss: 0.3158\n",
      "Total trained pairs (M):    1937.10 ; \t loss: 0.3148\n",
      "Total trained pairs (M):    1937.55 ; \t loss: 0.3155\n",
      "Total trained pairs (M):    1938.00 ; \t loss: 0.3143\n",
      "Total trained pairs (M):    1938.43 ; \t loss: 0.3164\n",
      "Total trained pairs (M):    1938.87 ; \t loss: 0.3186\n",
      "Total trained pairs (M):    1939.31 ; \t loss: 0.3183\n",
      "Total trained pairs (M):    1939.74 ; \t loss: 0.3172\n",
      "Total trained pairs (M):    1940.18 ; \t loss: 0.3176\n",
      "Total trained pairs (M):    1940.62 ; \t loss: 0.3158\n",
      "Total trained pairs (M):    1941.06 ; \t loss: 0.3151\n",
      "Total trained pairs (M):    1941.53 ; \t loss: 0.3297\n",
      "Total trained pairs (M):    1942.00 ; \t loss: 0.3272\n",
      "Total trained pairs (M):    1942.48 ; \t loss: 0.3255\n",
      "Total trained pairs (M):    1942.95 ; \t loss: 0.3272\n",
      "Total trained pairs (M):    1943.43 ; \t loss: 0.3276\n",
      "Total trained pairs (M):    1943.91 ; \t loss: 0.3261\n",
      "Total trained pairs (M):    1944.38 ; \t loss: 0.3255\n",
      "Total trained pairs (M):    1944.86 ; \t loss: 0.3251\n",
      "Total trained pairs (M):    1945.33 ; \t loss: 0.3227\n",
      "Total trained pairs (M):    1945.80 ; \t loss: 0.3233\n",
      "Total trained pairs (M):    1946.28 ; \t loss: 0.3214\n",
      "Total trained pairs (M):    1946.75 ; \t loss: 0.3211\n",
      "Total trained pairs (M):    1947.23 ; \t loss: 0.3192\n",
      "Total trained pairs (M):    1947.71 ; \t loss: 0.3211\n",
      "Total trained pairs (M):    1948.16 ; \t loss: 0.3229\n",
      "Total trained pairs (M):    1948.62 ; \t loss: 0.3207\n",
      "Total trained pairs (M):    1949.08 ; \t loss: 0.3190\n",
      "Total trained pairs (M):    1949.55 ; \t loss: 0.3211\n",
      "Total trained pairs (M):    1950.01 ; \t loss: 0.3231\n",
      "Total trained pairs (M):    1950.48 ; \t loss: 0.3194\n",
      "Total trained pairs (M):    1950.95 ; \t loss: 0.3211\n",
      "Total trained pairs (M):    1951.43 ; \t loss: 0.3222\n",
      "Total trained pairs (M):    1951.91 ; \t loss: 0.3194\n",
      "Total trained pairs (M):    1952.40 ; \t loss: 0.3187\n",
      "Total trained pairs (M):    1952.90 ; \t loss: 0.3189\n",
      "Total trained pairs (M):    1953.40 ; \t loss: 0.3191\n",
      "Total trained pairs (M):    1953.91 ; \t loss: 0.3181\n",
      "Total trained pairs (M):    1954.42 ; \t loss: 0.3202\n",
      "Total trained pairs (M):    1954.93 ; \t loss: 0.3191\n",
      "Total trained pairs (M):    1955.44 ; \t loss: 0.3182\n",
      "Total trained pairs (M):    1955.95 ; \t loss: 0.3165\n",
      "Total trained pairs (M):    1956.47 ; \t loss: 0.3162\n",
      "Total trained pairs (M):    1956.98 ; \t loss: 0.3141\n",
      "Total trained pairs (M):    1957.49 ; \t loss: 0.3136\n",
      "Total trained pairs (M):    1958.00 ; \t loss: 0.3132\n",
      "Total trained pairs (M):    1958.52 ; \t loss: 0.3090\n",
      "Total trained pairs (M):    1959.04 ; \t loss: 0.3095\n",
      "Total trained pairs (M):    1959.55 ; \t loss: 0.3105\n",
      "Total trained pairs (M):    1960.06 ; \t loss: 0.3094\n",
      "Total trained pairs (M):    1960.57 ; \t loss: 0.3078\n",
      "Total trained pairs (M):    1961.08 ; \t loss: 0.3101\n",
      "Total trained pairs (M):    1961.58 ; \t loss: 0.3108\n",
      "Total trained pairs (M):    1962.08 ; \t loss: 0.3090\n",
      "Total trained pairs (M):    1962.58 ; \t loss: 0.3095\n",
      "Total trained pairs (M):    1963.07 ; \t loss: 0.3083\n",
      "Total trained pairs (M):    1963.57 ; \t loss: 0.3087\n",
      "Total trained pairs (M):    1964.07 ; \t loss: 0.3070\n",
      "Total trained pairs (M):    1964.54 ; \t loss: 0.3096\n",
      "Total trained pairs (M):    1965.03 ; \t loss: 0.3135\n",
      "Total trained pairs (M):    1965.49 ; \t loss: 0.3147\n",
      "Total trained pairs (M):    1965.94 ; \t loss: 0.3131\n",
      "Total trained pairs (M):    1966.39 ; \t loss: 0.3132\n",
      "Total trained pairs (M):    1966.84 ; \t loss: 0.3123\n",
      "Total trained pairs (M):    1967.29 ; \t loss: 0.3094\n",
      "Total trained pairs (M):    1967.74 ; \t loss: 0.3104\n",
      "Total trained pairs (M):    1968.20 ; \t loss: 0.3080\n",
      "Total trained pairs (M):    1968.65 ; \t loss: 0.3084\n",
      "Total trained pairs (M):    1969.11 ; \t loss: 0.3087\n",
      "Total trained pairs (M):    1969.57 ; \t loss: 0.3092\n",
      "Total trained pairs (M):    1970.03 ; \t loss: 0.3076\n",
      "Total trained pairs (M):    1970.50 ; \t loss: 0.3115\n",
      "Total trained pairs (M):    1970.96 ; \t loss: 0.3098\n",
      "Total trained pairs (M):    1971.42 ; \t loss: 0.3071\n",
      "Total trained pairs (M):    1971.89 ; \t loss: 0.3099\n",
      "Total trained pairs (M):    1972.35 ; \t loss: 0.3082\n",
      "Total trained pairs (M):    1972.82 ; \t loss: 0.3102\n",
      "Total trained pairs (M):    1973.30 ; \t loss: 0.3108\n",
      "Total trained pairs (M):    1973.77 ; \t loss: 0.3113\n",
      "Total trained pairs (M):    1974.25 ; \t loss: 0.3115\n",
      "Total trained pairs (M):    1974.72 ; \t loss: 0.3096\n",
      "Total trained pairs (M):    1975.19 ; \t loss: 0.3102\n",
      "Total trained pairs (M):    1975.67 ; \t loss: 0.3099\n",
      "Total trained pairs (M):    1976.15 ; \t loss: 0.3082\n",
      "Total trained pairs (M):    1976.62 ; \t loss: 0.3072\n",
      "Total trained pairs (M):    1977.10 ; \t loss: 0.3049\n",
      "Total trained pairs (M):    1977.58 ; \t loss: 0.3092\n",
      "Total trained pairs (M):    1978.05 ; \t loss: 0.3080\n",
      "Total trained pairs (M):    1978.52 ; \t loss: 0.3069\n",
      "Total trained pairs (M):    1979.00 ; \t loss: 0.3069\n",
      "Total trained pairs (M):    1979.47 ; \t loss: 0.3066\n",
      "Total trained pairs (M):    1979.94 ; \t loss: 0.3056\n",
      "Total trained pairs (M):    1980.43 ; \t loss: 0.3051\n",
      "Total trained pairs (M):    1980.90 ; \t loss: 0.3057\n",
      "Total trained pairs (M):    1981.38 ; \t loss: 0.3054\n",
      "Total trained pairs (M):    1981.84 ; \t loss: 0.3059\n",
      "Total trained pairs (M):    1982.30 ; \t loss: 0.3057\n",
      "Total trained pairs (M):    1982.77 ; \t loss: 0.3132\n",
      "Total trained pairs (M):    1983.24 ; \t loss: 0.3133\n",
      "Total trained pairs (M):    1983.72 ; \t loss: 0.3139\n",
      "Total trained pairs (M):    1984.19 ; \t loss: 0.3128\n",
      "Total trained pairs (M):    1984.66 ; \t loss: 0.3126\n",
      "Total trained pairs (M):    1985.13 ; \t loss: 0.3137\n",
      "Total trained pairs (M):    1985.60 ; \t loss: 0.3134\n",
      "Total trained pairs (M):    1986.07 ; \t loss: 0.3120\n",
      "Total trained pairs (M):    1986.54 ; \t loss: 0.3124\n",
      "Total trained pairs (M):    1987.01 ; \t loss: 0.3120\n",
      "Total trained pairs (M):    1987.48 ; \t loss: 0.3144\n",
      "Total trained pairs (M):    1987.95 ; \t loss: 0.3149\n",
      "Total trained pairs (M):    1988.44 ; \t loss: 0.3152\n",
      "Total trained pairs (M):    1988.92 ; \t loss: 0.3168\n",
      "Total trained pairs (M):    1989.36 ; \t loss: 0.3189\n",
      "Total trained pairs (M):    1989.81 ; \t loss: 0.3197\n",
      "Total trained pairs (M):    1990.28 ; \t loss: 0.3241\n",
      "Total trained pairs (M):    1990.73 ; \t loss: 0.3255\n",
      "Total trained pairs (M):    1991.19 ; \t loss: 0.3243\n",
      "Total trained pairs (M):    1991.64 ; \t loss: 0.3230\n",
      "Total trained pairs (M):    1992.09 ; \t loss: 0.3226\n",
      "Total trained pairs (M):    1992.55 ; \t loss: 0.3205\n",
      "Total trained pairs (M):    1993.00 ; \t loss: 0.3194\n",
      "Total trained pairs (M):    1993.46 ; \t loss: 0.3195\n",
      "Total trained pairs (M):    1993.92 ; \t loss: 0.3198\n",
      "Total trained pairs (M):    1994.37 ; \t loss: 0.3182\n",
      "Total trained pairs (M):    1994.83 ; \t loss: 0.3214\n",
      "Total trained pairs (M):    1995.29 ; \t loss: 0.3206\n",
      "Total trained pairs (M):    1995.75 ; \t loss: 0.3214\n",
      "Total trained pairs (M):    1996.20 ; \t loss: 0.3194\n",
      "Total trained pairs (M):    1996.65 ; \t loss: 0.3207\n",
      "Total trained pairs (M):    1997.10 ; \t loss: 0.3237\n",
      "Total trained pairs (M):    1997.55 ; \t loss: 0.3234\n",
      "Total trained pairs (M):    1997.99 ; \t loss: 0.3263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trained pairs (M):    1998.44 ; \t loss: 0.3262\n",
      "Total trained pairs (M):    1998.87 ; \t loss: 0.3250\n",
      "Total trained pairs (M):    1999.30 ; \t loss: 0.3258\n",
      "Total trained pairs (M):    1999.72 ; \t loss: 0.3293\n",
      "Total trained pairs (M):    2000.14 ; \t loss: 0.3302\n",
      "Total trained pairs (M):    2000.55 ; \t loss: 0.3292\n",
      "Total trained pairs (M):    2000.96 ; \t loss: 0.3303\n",
      "Total trained pairs (M):    2001.36 ; \t loss: 0.3291\n",
      "Total trained pairs (M):    2001.77 ; \t loss: 0.3338\n",
      "Total trained pairs (M):    2002.18 ; \t loss: 0.3331\n",
      "Total trained pairs (M):    2002.58 ; \t loss: 0.3330\n",
      "Total trained pairs (M):    2002.98 ; \t loss: 0.3305\n",
      "Total trained pairs (M):    2003.39 ; \t loss: 0.3289\n",
      "Total trained pairs (M):    2003.79 ; \t loss: 0.3279\n",
      "Total trained pairs (M):    2004.20 ; \t loss: 0.3332\n",
      "Total trained pairs (M):    2004.60 ; \t loss: 0.3329\n",
      "Total trained pairs (M):    2005.01 ; \t loss: 0.3356\n",
      "Total trained pairs (M):    2005.42 ; \t loss: 0.3320\n",
      "Total trained pairs (M):    2005.84 ; \t loss: 0.3325\n",
      "Total trained pairs (M):    2006.25 ; \t loss: 0.3345\n",
      "Total trained pairs (M):    2006.66 ; \t loss: 0.3398\n",
      "Total trained pairs (M):    2007.07 ; \t loss: 0.3397\n",
      "Total trained pairs (M):    2007.48 ; \t loss: 0.3473\n",
      "Total trained pairs (M):    2007.90 ; \t loss: 0.3466\n",
      "Total trained pairs (M):    2008.32 ; \t loss: 0.3466\n",
      "Total trained pairs (M):    2008.74 ; \t loss: 0.3464\n",
      "Total trained pairs (M):    2009.16 ; \t loss: 0.3499\n",
      "Total trained pairs (M):    2009.59 ; \t loss: 0.3526\n",
      "Total trained pairs (M):    2010.01 ; \t loss: 0.3546\n",
      "Total trained pairs (M):    2010.44 ; \t loss: 0.3575\n",
      "Total trained pairs (M):    2010.86 ; \t loss: 0.3543\n",
      "Total trained pairs (M):    2011.29 ; \t loss: 0.3567\n",
      "Total trained pairs (M):    2011.72 ; \t loss: 0.3562\n",
      "Total trained pairs (M):    2012.16 ; \t loss: 0.3618\n",
      "Total trained pairs (M):    2012.61 ; \t loss: 0.3677\n",
      "Total trained pairs (M):    2013.06 ; \t loss: 0.3664\n",
      "Total trained pairs (M):    2013.51 ; \t loss: 0.3642\n",
      "Total trained pairs (M):    2013.95 ; \t loss: 0.3676\n",
      "Total trained pairs (M):    2014.39 ; \t loss: 0.3648\n",
      "Total trained pairs (M):    2014.84 ; \t loss: 0.3802\n",
      "Total trained pairs (M):    2015.29 ; \t loss: 0.3857\n",
      "Total trained pairs (M):    2015.74 ; \t loss: 0.3846\n",
      "Total trained pairs (M):    2016.19 ; \t loss: 0.3849\n",
      "Total trained pairs (M):    2016.63 ; \t loss: 0.3838\n",
      "Total trained pairs (M):    2017.06 ; \t loss: 0.3843\n",
      "Total trained pairs (M):    2017.50 ; \t loss: 0.3903\n",
      "Total trained pairs (M):    2017.93 ; \t loss: 0.3903\n",
      "Total trained pairs (M):    2018.36 ; \t loss: 0.3893\n",
      "Total trained pairs (M):    2018.78 ; \t loss: 0.3895\n",
      "Total trained pairs (M):    2019.21 ; \t loss: 0.3934\n",
      "Total trained pairs (M):    2019.64 ; \t loss: 0.3964\n",
      "Total trained pairs (M):    2020.08 ; \t loss: 0.4058\n",
      "Total trained pairs (M):    2020.52 ; \t loss: 0.4072\n",
      "Total trained pairs (M):    2020.97 ; \t loss: 0.4100\n",
      "Total trained pairs (M):    2021.42 ; \t loss: 0.4097\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "\n",
    "epochs = 10\n",
    "ntot = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch %d ======\" % epoch)\n",
    "    for words, contexts, labels in training_data_generator(text_encoded, window_size=4,batch_docs=100):\n",
    "        loss = model.train_on_batch(x=[words, contexts], y=labels)\n",
    "        ntot += len(words)\n",
    "        print(\"Total trained pairs (M): %10.2f ; \\t loss: %.4f\" % (ntot/1e6, loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the embedding to a table\n",
    "\n",
    "Ready to translate the model you trained into the embedding DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T00:54:01.369150Z",
     "start_time": "2019-03-30T00:54:01.317001Z"
    }
   },
   "outputs": [],
   "source": [
    "def embedding2df(embedding_layer, tokenmap_reverse):\n",
    "    return pd.DataFrame(embedding_layer.get_weights()[0], \n",
    "                        tokenmap_reverse.values()).drop(\"_unknown_\", errors='ignore')  \n",
    "\n",
    "skip = embedding2df(model.layers[2], tokenmap_reverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your trained embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T01:10:10.760048Z",
     "start_time": "2019-03-30T01:10:08.741980Z"
    }
   },
   "source": [
    "Use the embedding you just trained, repeat the exploration you did for Section 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital      2.888160\n",
      "existing     3.315533\n",
      "native       3.351735\n",
      "dish         3.353260\n",
      "somewhere    3.368453\n",
      "dtype: float64\n",
      "female     5.069273\n",
      "strikes    5.422312\n",
      "episode    5.497815\n",
      "enter      5.530743\n",
      "escaped    5.545122\n",
      "dtype: float64\n",
      "a                  2.889378\n",
      "dish               2.975097\n",
      "existing           2.995639\n",
      "representatives    3.009952\n",
      "native             3.044108\n",
      "dtype: float64\n",
      "9    0.943478\n",
      "7    0.943649\n",
      "6    1.037347\n",
      "5    1.111455\n",
      "4    1.339804\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "find_nearest_with_vector(skip.T,skip.T['king']-skip.T['male']+skip.T['female'])\n",
    "words = ['china', 'beijing', 'russia', 'moscow', 'poland', 'warsaw', 'japan', 'tokyo',\n",
    "        'france', 'paris', 'germany', 'berlin', 'italy', 'rome', 'spain', 'madrid']\n",
    "\n",
    "plot_words_embedding(skip.T, words)\n",
    "print(find_nearest_with_vector(skip.T, skip.T['china']+skip.T['capital']))\n",
    "print(find_nearest_with_vector(skip.T, skip.T['king']-skip.T['male']+skip.T['female']))\n",
    "print(find_nearest(skip.T, 'china'))\n",
    "print(find_nearest(glove, '8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T20:31:01.815021Z",
     "start_time": "2020-03-07T20:31:01.809481Z"
    }
   },
   "outputs": [],
   "source": [
    "#HW1, HW2\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from utils.hw1 import load_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data = load_data(\"../HW01-BAG-OF-WORDS/train.txt\")\n",
    "X, y = data.text, data.target\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH=200\n",
    "# Yes! it is a good practice to do data processing outside the ML model\n",
    "wnet = WordNetLemmatizer()\n",
    "\n",
    "X_train1 = [' '.join([wnet.lemmatize(w) for w in word_tokenize(sent)]) for sent in X_train]\n",
    "vec = CountVectorizer()\n",
    "vec.fit_transform(X_train1)\n",
    "vocab = vec.vocabulary_\n",
    "vocab['__unknown__'] = len(vocab) + 1\n",
    "# Numerical encode all the words\n",
    "unknown = vocab['__unknown__']\n",
    "X_train2 = [[vocab.get(wnet.lemmatize(w), unknown) for w in word_tokenize(sent)] for sent in X_train]\n",
    "X_dev2 = [[vocab.get(wnet.lemmatize(w), unknown)for w in word_tokenize(sent)] for sent in X_dev]\n",
    "\n",
    "# Tensorflow does not handle variable length input well, let's unify all input to the same length\n",
    "def trim_X(X, max_length=100, default=vocab['__unknown__']):\n",
    "    for i in range(len(X)):\n",
    "        if len(X[i]) > max_length:\n",
    "            X[i] = X[i][:max_length]\n",
    "        elif len(X[i]) < max_length:\n",
    "            X[i] = X[i] + [default] * (max_length - len(X[i]))\n",
    "            \n",
    "    return np.array(X)\n",
    "            \n",
    "X_train2 = trim_X(X_train2, MAX_LENGTH)\n",
    "X_dev2 = trim_X(X_dev2, MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 200, 100)          10000000  \n",
      "_________________________________________________________________\n",
      "Conv1D-1 (Conv1D)            (None, 197, 400)          160400    \n",
      "_________________________________________________________________\n",
      "MaxPooling1D-1 (GlobalMaxPoo (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "Dense-1 (Dense)              (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 10,160,801\n",
      "Trainable params: 10,160,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, AveragePooling1D, GlobalAveragePooling1D, MaxPooling1D, Dense, GlobalMaxPooling1D, LSTM\n",
    "from keras.regularizers import l2\n",
    "\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(input_dim=100000,\n",
    "                            output_dim=100,\n",
    "                            weights=[glove.T],\n",
    "                            input_length=MAX_LENGTH,\n",
    "                            trainable=True)\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=400, kernel_size=4, activation=\"tanh\", name=\"Conv1D-1\"))\n",
    "model.add(GlobalMaxPooling1D(name=\"MaxPooling1D-1\"))\n",
    "#model.add(LSTM(32))\n",
    "model.add(Dense(1, activation=\"sigmoid\", name=\"Dense-1\"))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/20\n",
      "7000/7000 [==============================] - 42s 6ms/step - loss: 0.6732 - acc: 0.5749 - val_loss: 0.6546 - val_acc: 0.5823\n",
      "Epoch 2/20\n",
      "7000/7000 [==============================] - 41s 6ms/step - loss: 0.5060 - acc: 0.7839 - val_loss: 0.6044 - val_acc: 0.6807\n",
      "Epoch 3/20\n",
      "7000/7000 [==============================] - 42s 6ms/step - loss: 0.3235 - acc: 0.9057 - val_loss: 0.6069 - val_acc: 0.6977\n",
      "Epoch 4/20\n",
      "7000/7000 [==============================] - 39s 6ms/step - loss: 0.1588 - acc: 0.9736 - val_loss: 0.6459 - val_acc: 0.7053\n",
      "Epoch 5/20\n",
      "7000/7000 [==============================] - 44s 6ms/step - loss: 0.0591 - acc: 0.9971 - val_loss: 0.6949 - val_acc: 0.7137\n",
      "Epoch 6/20\n",
      "7000/7000 [==============================] - 44s 6ms/step - loss: 0.0218 - acc: 0.9997 - val_loss: 0.7290 - val_acc: 0.7217\n",
      "Epoch 7/20\n",
      "7000/7000 [==============================] - 41s 6ms/step - loss: 0.0107 - acc: 0.9999 - val_loss: 0.7809 - val_acc: 0.7193\n",
      "Epoch 8/20\n",
      "7000/7000 [==============================] - 39s 6ms/step - loss: 0.0068 - acc: 0.9999 - val_loss: 0.8051 - val_acc: 0.7203\n",
      "Epoch 9/20\n",
      "7000/7000 [==============================] - 41s 6ms/step - loss: 0.0047 - acc: 0.9999 - val_loss: 0.8371 - val_acc: 0.7240\n",
      "Epoch 10/20\n",
      "7000/7000 [==============================] - 41s 6ms/step - loss: 0.0036 - acc: 0.9999 - val_loss: 0.8573 - val_acc: 0.7273\n",
      "Epoch 11/20\n",
      "7000/7000 [==============================] - 44s 6ms/step - loss: 0.0028 - acc: 0.9999 - val_loss: 0.8820 - val_acc: 0.7283\n",
      "Epoch 12/20\n",
      "7000/7000 [==============================] - 43s 6ms/step - loss: 0.0023 - acc: 0.9999 - val_loss: 0.9115 - val_acc: 0.7247\n",
      "Epoch 13/20\n",
      "7000/7000 [==============================] - 40s 6ms/step - loss: 0.0019 - acc: 0.9999 - val_loss: 0.9278 - val_acc: 0.7280\n",
      "Epoch 14/20\n",
      "7000/7000 [==============================] - 38s 5ms/step - loss: 0.0016 - acc: 0.9999 - val_loss: 0.9504 - val_acc: 0.7250\n",
      "Epoch 15/20\n",
      "7000/7000 [==============================] - 44s 6ms/step - loss: 0.0014 - acc: 0.9997 - val_loss: 0.9634 - val_acc: 0.7273\n",
      "Epoch 16/20\n",
      "7000/7000 [==============================] - 44s 6ms/step - loss: 0.0012 - acc: 0.9999 - val_loss: 0.9884 - val_acc: 0.7260\n",
      "Epoch 17/20\n",
      "7000/7000 [==============================] - 46s 7ms/step - loss: 0.0010 - acc: 0.9999 - val_loss: 1.0062 - val_acc: 0.7273\n",
      "Epoch 18/20\n",
      "7000/7000 [==============================] - 44s 6ms/step - loss: 9.4910e-04 - acc: 0.9999 - val_loss: 1.0352 - val_acc: 0.7233\n",
      "Epoch 19/20\n",
      "7000/7000 [==============================] - 39s 6ms/step - loss: 9.0509e-04 - acc: 0.9999 - val_loss: 1.0538 - val_acc: 0.7273\n",
      "Epoch 20/20\n",
      "7000/7000 [==============================] - 42s 6ms/step - loss: 7.6321e-04 - acc: 0.9999 - val_loss: 1.0581 - val_acc: 0.7277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b91067f0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train2, y_train, epochs=20, validation_data=[X_dev2, y_dev])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final project (Not due this week)\n",
    "\n",
    "**Work with your teammates and start working on your final project proposal, think about these questions:**\n",
    "- The problem you try to solve and the value of this problem\n",
    "- Some current solution to this problem, reference citation if needed\n",
    "- Outline your approach and the goal you want to achieve\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "170px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
